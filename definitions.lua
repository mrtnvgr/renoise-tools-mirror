--[[
Instructions for definitions
----------------------------

Name the preset within the square brackets and in quotation marks after dsp[] e.g. dsp["Preset name"] 
The name can be anything but try and keep it the same or similar to the CDP reference documentation.


cmds
----
  exe - The name of the CDP executable (exclude the .exe extension)
  mode - The first part of the command argument that comes after the exe and before infile, outfile
  channels - Specify if the process can handle stereo files or has other i/o channel specifications, values can be: 
             "mono" - The default if nothing is specified (stereo sounds are split into two sounds, processed and then stitched back together)
             "any" - Can handle mono and stereo files
             "1in2out" - Takes a mono input and returns a stereo output (stereo inputs will be mixed to mono)
             "2in2out" - Only works on stereo inputs (mono inputs will be converted to stereo)                
      TODO   "1in1out" - Stereo inputs are mixed into mono and output is always mono - To use for getpitch repitch and similar 
  (Note:Audio conversions handled automatically for pvoc)

  url - The URL to the online document for this process, if left blank the tool will go to the default A-Z process index
  tip - The text displayed in the tool info window
             
  
  To do 
  -----
  multichannel outputs


arg1 to arg..n
--------------
  name - The name displayed in the GUI (STRING)

  input - Specify the input type for this parameter (if applicable) supported types are:
    
      wav - This will create a instrument/sample selector
      ana - This will create a instrument/sample selector - for PVOC processes, conversion is done in background
      brk - Creates a button to open a breakpoint editor for parameters that 'vary over time'
      txt - This will create a text editor that allows text files to be created, loaded and saved
      frq - Opens a file selector for this type of file
      trn - ""
      for - ""
      env - ""
      data - This will create a file open selector that does not filter for an extension, use this when not sure of filetype
      string - Creates a string field input
        
  output - Specify the output file type for this parameter (if applicable) supported types are:

      wav - This will create a instrument/sample selector
      ana - This will create a instrument/sample selector- for PVOC processes, conversion is done in background
      brk - This will create a file save selector with the given file extension
      txt - ""
      frq - ""
      trn - ""
      for - ""
      env - ""

  switch - The switch flag, use only if it is a command switch e.g "-s" (STRING)
  min - The minimum value of the range (NUMBER)
  max - The maximum value of the range (NUMBER)
  def - The default value the preset loads with (OPTIONAL NUMBER or STRING)
  tip - The tool tip for the GUI control (OPTIONAL STRING)


Additional variables are available that give info about the sample
------------------------------------------------------------------
  srate - Gives the sample rate
  cycles - Gives the number of wavecycles (same as number of zero crossings/2)
  length - Gives the sample length in milliseconds, divide this by 1000 to get it in seconds
--]]

-- Do not edit
srate = get_srate() -- Sample rate in Hz
cycles = get_cycles() -- Number of wave cycles in sample (zero crossings/2)
length = get_length() -- Sample length in milliseconds        
dsp = {}
-- Do not edit


------------------------
-- PRESET DEFINITIONS --
------------------------


------------------------------------------------
-- blur
------------------------------------------------
 
dsp["Blur Avrg - average spectral energy over N adjacent channels"] = {
  cmds = { exe = "blur", mode = "avrg", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#AVRG", tip = "The averaging of engergy information causes high energy data to cross channels. The result is the intrusion of timbral data into channels which previously didn't have that data. Test results led to a 'roughening' of the sound, though without going as far as distorting it. Note that this process relates to channels, not to windows (as does the BLUR BLUR process). This means that the averaging directly affects the frequency components of the sound. The energy (amplitude) in adjacent channels is averaged out over Average-span adjacent channels, without affecting the frequencies of those channels. The result is to broaden, or defocus, any energy peaks in the spectrum. This reduces the frequency definition of the sound, which is why it is in the BLUR category and is, in effect, a type of filtering. Musical applications; BLUR AVRG appears to be useful to enrich the timbre of a sound and make its texture rougher. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 0, max = 4096, def = 512, tip = "must be ODD and <= half the -N param used in the original analysis" },
}
 
dsp["Blur Blur - time-average the spectrum"] = {
  cmds = { exe = "blur", mode = "blur", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#BLUR", tip = " BLUR BLUR time-averages the spectrum. It 'blurs' detail in the time dimension by interpolating between the spectral envelope values of the start and end windows blurring windows. Note that it is not interpolating continuously over all the windows inbetween, just between the data in the start and end windows. The overall result is somewhat affected by just how different the data is in these two windows. The interpolation process produces a 'straight line' (linear) scale of values between the start and end points. BLUR BLUR differs from HILITE BLTR in the absence of the 'trace' parameter. This means that the blurring occurs without any reduction in the number of partials. Musical applications; As with all of these programs, the result is greatly affected by the nature of the input sound. The more windows blurred, the more blurring or smoothing of the sound you might expect to happen. However, you might not notice much difference if the sound is already constant (or similar at the start and end points). You will probably need a sound with a great deal of internal change for the blurring to have a perceptible effect. The musical results of this process begin with a softening of the attack transients, so is highly effective with, for example, plucked or percussive sounds. A time-varying transition from the original to a very blurred effect can be achieved by a simple breakpoint file which sets a low value for blurring at the beginning of the sound and a high value at the end. It is possible for the sound to disappear gradually into its own ambience. This powerful technique can therefore useful in the creation of 'ambient' music, as well as any degree of softening of the original sound." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "blurring", min = 2, max = 32768, input = "brk", def = 512, tip = "blurring is the number of windows over which to average the spectrum" },
}
 
dsp["Blur Chorus 1 - Randomise partial amplitudes - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 1", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial amplitudes - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "aspread", min = 1, max = 1028, input = "brk", def = 1, tip = "is maximum random scatter of partial-amps (Range 1-1028)" },
}
 
dsp["Blur Chorus 2 - Randomise partial frequencies - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 2", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial frequencies - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Chorus 3 - Randomise partial frequencies upwards only 2 - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 3", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = " Randomise partial frequencies upwards only 2 - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Chorus 4 - Randomise partial frequencies downwards only 3 - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 4", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial frequencies downwards only 3 - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Chorus 5 - Randomise partial amplitudes AND frequencies - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 5", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial amplitudes AND frequencies - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "aspread", min = 1, max = 1028, input = "brk", def = 1, tip = "is maximum random scatter of partial-amps (Range 1-1028)" },
  arg4 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Chorus 6 - Randomise partial amplitudes, and frequencies upwards only - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 6", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial amplitudes, and frequencies upwards only - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "aspread", min = 1, max = 1028, input = "brk", def = 1, tip = "is maximum random scatter of partial-amps (Range 1-1028)" },
  arg4 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Chorus 7 - Randomise partial amplitudes, and frequencies downwards only - chorusing by randomising amplitudes and/or frequencies of partials "] = {
  cmds = { exe = "blur", mode = "chorus 7", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#CHORUS", tip = "Randomise partial amplitudes, and frequencies downwards only - this process attempts to achieve a chorusing effect by randomising the amplitude and frequency values of the partials. It is based on an idea of Stephen McAdams. See his Spectral Fusion and the Creation of Auditory Images (1981). If very large amplitude aspread values are used, the sound will turn to noise. The ranges shown for aspread and fspread appear to be rather extreme, but they are rescaled within the program. There is in fact no mathematical limit to the values which can be entered. In practice, the 'chorusing' effect itself is achieved by values just a little above 1!. Values of 2 or 3 begin to create a granular effect, and values of 10, 100 amd 1000, for example, create more and more noise. Musical applications; 'Chorusing' is one of the standard effects used to enliven and enrich a sound. It becomes available here in an open-ended context, where the effect can be driven beyond the usual bounds into granulation and noise effects, useful when a certain textural or gritty quality is needed. In Mode 5, a burbly but comprehensible vocal transformation may occur with aspread = 30 and fspread = 4." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "aspread", min = 1, max = 1028, input = "brk", def = 1, tip = "is maximum random scatter of partial-amps (Range 1-1028)" },
  arg4 = { name = "fspread", min = 1, max = 4, input = "brk", def = 1, tip = "is maximum random scatter of partial-frqs (Range 1-4)" },
}
 
dsp["Blur Drunk"] = {
  cmds = { exe = "blur", mode = "drunk", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#DRUNK", tip = "This process can be used to lurch about in a soundfile, producing a jumbled version of infile to varying degrees. The salient parameter is range. If range is small, the output will tend to linger around your start-point in the file, progressing very slowly away from it in an arbitrary direction. This results in a mix of time-stretching and slow wandering through the source. If range is large, the drunken walk tends to leap about wildly in the file, scrambling the source sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "Range", min = 0, max = 64, tip = "the maximum step (in windows) for the drunken walk: <= 64" },
  arg4 = { name = "Start Time", min = 0, max = length/1000, tip = "the time (in seconds) in the file at which the walk should begin" },
  arg5 = { name = "Duration", min = 0, max = (length/1000)*4, tip = "the required duration of the outfile after re-synthesis ­ it may be longer than infile" },
  arg6 = { name = "-z", switch = "-z", tip = "eliminates zero steps (window-repeats) in the drunken walk" },
}
 
dsp["Blur Noise - put noise in the spectrum in file  "] = {
  cmds = { exe = "blur", mode = "noise", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#NOISE", tip = "It enables one to move a sound source towards pure noise, by making the data in every channel ­ most of which is actually low level noise ­ equally loud. Total saturation will reduce all sounds to a very similar noise signal. Partial or gradual saturation is possible, through the use of values less than 1 or by using a time-varying breakpoint file. These work in the usual way, with gradual change between different time points to which are assigned different noise values. Musical applications; This technique can be used to cause sound material to emerge from obscurity to clarity, or v.vs.. Also, a carefully chosen noise factor can be used to colour a sound or soften its edges." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "noise", min = 0, max = 1, input = "brk", def = 0, tip = "Range 0 (no noise in spectrum) to 1 (spectrum saturated with noise)" },
}
 
dsp["Blur Scatter - randomly thin out the spectrum"] = {
  cmds = { exe = "blur", mode = "scatter", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#SCATTER", tip = "This function throws away a specified proportion of the analysis data in each window. Unlike HILITE TRACE, the channels chosen to be suppressed are selected entirely at random. The spectrum is thus thinned out in an unpredictable fashion. Remember that the number of channels in an analysis file is determined by the value for the -N flag of the Phase Vocoder (now referred to as -cpoints). For example, if it was -c1024, there will be 512 + 1 channels, each containing frequency and amplitude data. Before scattering, the channels are gathered together into blocks of adjacent channels, and these blocks-of-channels are then either retained or discarded (at random). Blocksize sets how many of these channels form one block. Keep sets how many of these blocks (i.e., groups of channels) will be randomly chosen and retained from the total of 513. The values for keep and blocksize therefore need to be worked out in relation to the value given for -c. For example, retaining keep = 10 and blocksize = 6 will retain a total of 60 (randomly selected) channels from the analysis data. Musical applications; The musical result will be more variable than HILITE TRACE. The latter retains the N loudest channels, thus always keeping the most audibly prominent data of the original sound. BLUR SCATTER may or may not pick up audibly prominent data as it makes its random selections. The original sound material will therefore be more variably retained or cast aside, and this variability can be increased by using a time keep and/or a time blocksize breakpoint file. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "keep", min = 0, max = 1000, def = 2, tip = "keep number of (randomly chosen) blocks to keep in each spectral window" },
  arg4 = { name = "blocksize", switch = "-b", min = 1, max = srate/2, tip = "frequency range of each block (default is width of 1 analysis channel. (Rounded internally to a multiple of channel width.)" },
  arg5 = { name = "-r", switch = "-r", tip = "number of blocks actually selected is randomised between 1 and keep" },
  arg6 = { name = "-n", switch = "-n", tip = "turn OFF normalisation of resulting sound" },
}
 
dsp["Blur Shuffle - shuffle order of analysis windows in file"] = {
  cmds = { exe = "blur", mode = "shuffle", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#SHUFFLE", tip = " SHUFFLE shuffles windows in a PVOC analysis data file. This is a spectral version of 'brassage' ­ see SAUSAGE and BRASSAGE in the Groucho Signal Processing Suite. As with most of these programs, the degree of audible difference is going to depend on how much the sound changes in the first place. Shuffling spectral windows in a fairly uniform sound may not have much effect. Experimenting with this program ought to lead to the unexpected. To illustrate how this works, suppose there is a domain of three letters: A B C. This means that there will be three consecutive windows, numbered 1 2 3 in the order in which they occur in the analysis file. If the image is ordered C B A, then these three windows will be rearranged in the output into the order 3 2 1. grpsize has not been set in this example, so the default of 1 is operating. If we then set grpsize to, say 3, the A will stand for 3 windows, starting with 1 2 3, B for windows 4 5 6, and C for windows 7 8 9. When the shuffling takes place according to the Image CBA, the result will be 7 8 9, 4 5 6, 1 2 3. The process continues in this way throughout the analysis file. Larger values for grpsize (e.g., 20) can lead to the repetition of fragments of sound material.  Musical applications; The domain-image parameter is what makes SHUFFLE particularly useful. The domain identifies a series of consecutive windows, and the image defines the order of those windows (some may be omitted). Then this whole pattern repeats as a block, affecting each consecutive set of windows in the source file. The control over re-ordering makes it possible to jumble, retrograde, mix forwards and backwards etc. the successive windows. An increasing grpsize makes more of the source audible at any given time, which means that one can play with the degree to which the source can be identified. Thus one is stepping through the source in chunks of varying size and re-ordering the contents of those chunks. The re-orderings play with familiarity, create repeatable sub-patterns, and will lead to additional timbral changes depending on the degree to which differing spectral data get mixed together.   " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "domain-image", input = "string", def = "abcd-dcba", tip = "two strings of letters, separated by a hyphen ('-'); the first string is the domain and the second string is the image, e.g., 'abc-abbabcc'." },
  arg4 = { name = "grpsize", min = 2, max = 32768, def = 10, tip = "the number of analysis windows corresponding to each letter of domain. " },
}
 
dsp["Blur Spread - spread peaks of spectrum, introducing controlled noisiness"] = {
  cmds = { exe = "blur", mode = "spread", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#SPREAD", tip = "For this one to work you need to enable one of the N toggles first. This process introduces noise into the spectrum in a way which is coherent with (i.e., related to) the spectral envelope. The spectral envelope (formants) in each window is retained, and the level in every channel is made to approximate this average spectral contour to a greater or lesser extent, depending on spread. This process tends to exaggerate the less prominent (noise) constituents of the spectrum. Musical applications; This is a distortion technique which can be handled in a very sensitive manner because spread can be so finely adjusted, or altered in a time-varying manner. If applied to speech, the noise introduces a certain edginess, so if an effect such as unvoiced speech is sought, FORMANTS VOCODE is recommended, which allows some low or high frequency filtering.  " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 0, max = 256, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency channels" },
  arg4 = { name = "N", switch = "-p", min = 2, max = 12, def = 12, tip = "extract formant envelope linear pitch-wise, using N equally-spaced pitch bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "-spread", switch = "-s", min = 0, max = 1, input = "brk", tip = "spread degree of spreading of spectrum (Range 0-1; Default is 1)." },
}
 
dsp["Blur Suppress - suppress the N loudest partials (on a window by window basis)"] = {
  cmds = { exe = "blur", mode = "suppress", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#SUPPRESS", tip = "This process is the opposite of HILITE TRACE, which retains the N loudest partials. Whereas with TRACE many components have to be removed before the sound starts to be seriously affected, here the change to infile will be more immediate when a relatively small value for N is used. Musical applications; The loudest partials will not necessarily relate to either the higher or lower parts of the frequency spectrum. They will relate simply to whatever is most audible ­ prominent ­ in the sound, such as possibly the fundamental of a pitched sound. This will have the effect of revealing the more strictly timbral aspects of the sound. This process can also be useful in preparing a sound for another process which will benefit from suppressing its more audible features. As an example of this, ... (I did a sound in which I couldn't get rid of the fundamental, which was reinforced by the process, when I really wanted to bring out other things.) " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 1, max = 513, input = "brk", def = 2, tip = "N is the number of spectral components to reject" },
}
 
dsp["Blur Weave - modify sound by weaving amongst analysis windows specified in weave text file "] = {
  cmds = { exe = "blur", mode = "weave", url = "http://www.ensemble-software.net/CDPDocs/html/cblur.htm#WEAVE", tip = "BLUR WEAVE is similar to SHUFFLE but is a smoother process which plots a regular course through the analysis windows, weaving backwards and forwards as instructed by the weavfile. Note that the weave process enables one to jump to points forwards or backwards in the soundfile, at which point it takes a single window. Thus one of the keys to understanding WEAVE is to realise that it only takes one window at a time after it has moved somewhere. It doesn't fill in with the windows inbetween. The weavfile therefore creates a sequence of single windows taken from various points relative to the start window. This is what makes it a form of brassage in the spectral dimension. The pattern in the weavfile then repeats, starting from the window it has now reached.  Musical applications; WEAVE can be used to shorten or extend a sound and/or to blur its characteristics: by taking windows out of sequence, earlier (backwards) or later (forwards) in the source. Windows taken from relatively distant locations of course mix up the sound quite a bit. A very different effect is achieved by having the windows hover around a certain area. The fact that the pattern repeats leads to additional possibilities to create pulsations in the sound, though the timbral effect is likely to vary due to changing spectral content as the weave moves through the source.  " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "weavfile", input = "txt", tip = "weavfile contains a list of integers which define successive steps (in windows) through the input file." },
}


------------------------------------------------
-- brktopi
------------------------------------------------
 
dsp["Brktopi - Convert a breakpoint pitch data file to a binary pitch data file"] = {
  cmds = { exe = "brktopi", mode = "brktopi", url = "http://www.ensemble-software.net/CDPDocs/html/crepitch.htm#BRKTOPI", tip = "BRKTOPI converts a breakpoint pitch data file (text) to a binary pitch data file (.for). The function is the opposite of REPITCH PCHTOTEXT. Musical applications: While a great variety of manipulation is possible with binary pitch data files (.for), there is unlimited scope for altering the pitch data in breakpoint (text) form. As only binary pitch data can be converted into sound (via MAKE), this function provides the necessary conversion to the binary format. You need to be aware, however, that altering pitch data is not altering the formant data with which it will be combined in MAKE. The pitches you specify will sound only to the extent of the level that their equivalent frequencies (fundamental and harmonics) have in the formant file." },
  arg1 = { name = "intextpitchfile", input = "data", tip = "breakpoint pitch data file (.txt, .brk or .pch) extracted by REPITCH GETPITCH or created / manipulated as text" },
  arg2 = { name = "outbinarypitchfile", output = "for", tip = "binary pitch data file (.for)" },
}

------------------------------------------------
-- cantor
------------------------------------------------
 
--[[ Doesn't work? Check doc's, it tries to do something with the output filename, maybe that is the culprit?
  
dsp["Cantor - Cut holes in a sound in the manner of a cantor set (holes within holes within holes (hole-size = %))"] = {
  cmds = { exe = "cantor", mode = "set 1", tip = "CANTOR gradually cuts a hole in the central third of the input sound, which must be mono, a 'hole' being a reduction in level (see SNDREPORT FINDHOLE). It then cuts holes in the central third of the remaining segments, and so on. The output soundfile consists of a sequence of sounds with more and more holes cut in it. Note that Mode 3 uses superimposed vibrato envelopes as well. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "holesize", min = 0.0001, max = 100, tip = "the percentage of current segment-time taken up by a hole." },
  arg4 = { name = "holedig", min = 0, max = 1, tip = "the depth of each cut as the hole is gradually created. Range: >0 to 1" },
  arg5 = { name = "depth-trig", min = 0, max = 1, tip = " the level depth of the hole triggering the next hole-cuttin" },
  arg6 = { name = "splicelen", min = 3, max = 50, def = 4, tip = "splicelength in milliseconds" },
  arg7 = { name = "maxdur", min = 0, max = 32767, def = 10, tip = "the maximum output duration of all the output sound" },
  arg8 = { name = "-e", switch = "-e", tip = "extend the sound beyond the splicelen limits" },
}
 
dsp["Cantor - Cut holes in a sound in the manner of a cantor set (holes within holes within holes (hole-size = fixed))"] = {
  cmds = { exe = "cantor", mode = "set 2", tip = "CANTOR gradually cuts a hole in the central third of the input sound, which must be mono, a 'hole' being a reduction in level (see SNDREPORT FINDHOLE). It then cuts holes in the central third of the remaining segments, and so on. The output soundfile consists of a sequence of sounds with more and more holes cut in it. Note that Mode 3 uses superimposed vibrato envelopes as well. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "holesize", min = 0.0001, max = length/1000, tip = "the (fixed) duration of the holes." },
  arg4 = { name = "holedig", min = 0, max = 1, tip = "the depth of each cut as the hole is gradually created. Range: >0 to 1" },
  arg5 = { name = "depth-trig", min = 0, max = 1, tip = " the level depth of the hole triggering the next hole-cuttin" },
  arg6 = { name = "splicelen", min = 3, max = 50, def = 5, tip = "splicelength in milliseconds" },
  arg7 = { name = "maxdur", min = 0, max = 32767, def = 10, tip = "the maximum output duration of all the output sound" },
  arg8 = { name = "-e", switch = "-e", tip = "extend the sound beyond the splicelen limits" },
}
 
dsp["Cantor - Cut holes in a sound in the manner of a cantor set (holes within holes within holes (hole-size = vibrato))"] = {
  cmds = { exe = "cantor", mode = "set 3", tip = "CANTOR gradually cuts a hole in the central third of the input sound, which must be mono, a 'hole' being a reduction in level (see SNDREPORT FINDHOLE). It then cuts holes in the central third of the remaining segments, and so on. The output soundfile consists of a sequence of sounds with more and more holes cut in it. Note that Mode 3 uses superimposed vibrato envelopes as well. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "holelev", min = 0.0001, max = 1, tip = "the level of signal at the base of the holes" },
  arg4 = { name = "holedig", min = 2, max = 256, tip = "the depth of each cut as the hole is gradually created. Range: >0 to 1" },
  arg5 = { name = "layercnt", min = 1, max = 100, tip = "the number of vibrato layers used" },
  arg6 = { name = "layerdec", min = 0.01, max = 1, tip = "the depth of the next vibrato in relation to the previous one" },
  arg7 = { name = "maxdur", min = 0, max = 32767, def = 10, tip = "the maximum output duration of all the output sound" },
}
--]]
 
------------------------------------------------
-- ceracu
------------------------------------------------
 
dsp["Ceracu - Repeat the source sound in several cycles that synchronise after specified counts - needs datafile"] = {
  cmds = { exe = "ceracu", mode = "ceracu", tip = "This process needs a data.text file as input for the 'cyclecnts' parameter. CERACU creates polyrhythms (such as the familiar 2-against-3 pattern). In each of several sound streams, the source sound (which must be mono) is re-triggered at a regular time-interval, which is normally different for each stream. After a certain number of repetitions, the streams re-synchronise, completing a full 'resync cycle'. One complete pass is a 'resync-cycle', e.g., specified as 10, 12 and 15 in the cyclecnts textfile. The source repeats 12, 12 and 15 times before the cyclestreams resynchronise.      The time-span of the cycle is determined by mincycdur, the shortest division (e.g. 3, in 2 against 3: more repeats within the time make them closer together). This division may mean that the source is not played in full within the cycle. Typically, the number of streams might equal the number of output channels; if not, some channels will be silent or contain a rhythm that is the resultant of two or more streams. The process always outputs a whole number of complete resync-cycles, equal to or greater than outdur, the overall minimum output length. The final playing of the source in each channel always runs to its end and an extra tail of silence also seems to be added. If the duration would be greater than 1 hour, the sound is curtailed, unless the 'override' [-o] flag is set. Musical Applications; CERACU provides a means to explore rhythmic overlays of the same soundfile, overlays that are not only more complex, but also precisely defined. You can get overlays with the TEXTURE via the packing parameter – expecially interesting with TEXTURE MOTIFS – but without the degree of rhythmic timing that you can get with CERACU. The possibility of spreading the outputs across a multi-channel rig is another benefit. In this regard, you might also want to look at TEXMCHAN." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnts", input = "txt", tip = "datafile consisting of a list of integers, being the number of repeats in each cyclestream before the streams resynchronise" },
  arg4 = { name = "mincycdur", min = 0, max = length/1000, def = 0, tip = "the time before the first repeat in the fastest cyclestream. If set to ZERO, it is assumed to be the duration of the input sound." },
  arg5 = { name = "chans", min = 1, max = 2, def = 1, tip = "number of channels in the output – NB: it is not necessarily the same as the number of cyclestreams" },
  arg6 = { name = "outdur", min = 0, max = 60, def = 10, tip = "duration of the output. (If set to ZERO, it outputs a single resync-cycle.) The process always outputs a whole number of complete resync-cycles, equal to or greater than the specified output duration." },
  arg7 = { name = "echo", min = 0, max = length/1000, def = 0, tip = "single-echo-delay of entire output, in seconds. (Set to ZERO for no echo.)" },
  arg8 = { name = "echshift", min = -2, max = 2, def = 0, tip = "Spatial offset of echo-delay (an integer value) – ignored if no echo" },
  arg9 = { name = "-o", switch = "-o", tip = "override the duration restriction, to produce all resync-cycles (CARE!)." },
  arg10 = { name = "-l", switch = "-l", tip = "output channels are arranged linearly (Default: arranged in a circle.)" },
}
 
------------------------------------------------
-- chanphase
------------------------------------------------
 
dsp["Chanphase - Invert the phase of a specified channel of an input sound "] = {
  cmds = { exe = "chanphase", mode = "chanphase", channels = "any", tip = "This program seeks to rectify a problem encountered by a couple of students at Durham University. They had bought what seemed to be stereo digital recorders, but the output display on Sound Loom's SOUND VIEW showed no display. The SOUND VIEW display is a mono display. To display stereo signals it mixes the 2 channels to mono. The display was empty because the recorders were generating 2 identical mono, phase-inverted, signals. This may have been a manufacturing fault or a misrepresentation on the part of the manufacturer. Who knows?! Musical Applications; CHANPHASE allows you to phase-invert one of the channels so that is no longer identical with the other one and will therefore display in SOUND VIEW. It is therefore a utility program with a specific purpose. At the time of writing, it is not available in Sound Loom on the MAC. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "channel", min = 1, max = 2, def = 1, tip = "channel to invert" },
}

------------------------------------------------
-- chorder
------------------------------------------------
 
--[[ guarenteed Renoise crash if you input aaa instead of the default aa, multichannel files not supported 
dsp["Chorder - Reorder soundfile channels in a multi-channel soundfile"] = {
cmds = { exe = "chorder", mode = "", tip = "Format of the orderstring: Infile channels are mapped in order as a = 1, b = 2 ... z = 26. For example, the channels in a 4-channel file are represented by the characters abcd. Any other character is an error. Characters may be lower case, and may be used more than once, duplicate characters duplicating the corresponding input channel. The zero character ('0') may be used to set a silent channel. A maximum of 26 channels is supported for both input and output. Note that the infile (WAVE_EX) speaker positions are discarded. The .amb extension is supported for the outfile. CHORDER expedites many of the tasks previously requiring a combination of CHANNELX (extract channels) and INTERLX (interleave multi-channel soundfiles). Channels may be reordered while retaining the number of them. The program also supports both extracting a subset of input channels (e.g., 1st-order channels from a 2nd or 3rd order file), and augmenting the channel count by duplicating input channels and/or defining a number of empty (silent) channels. All .wav output files are in WAVE_EX format – the speaker positions (and/or the GUID) can be changed using CHXFORMAT." },
arg1 = { name = "orderstring", input = "string", def = "aa", tip = "the order string is made up of any combination of characters, a to z inclusive." },
}
--]] 
 
------------------------------------------------
-- combine
------------------------------------------------
 
dsp["Combine Cross - Replace spectral amplitudes of 1st file with those of 2nd"] = {
  cmds = { exe = "combine", mode = "cross", tip = "Formerly SPECCROS, this function makes a weighted substitution of the amplitude of one spectral envelope for that of the other. It is therefore a fairly straightforward way to achieve a crossover between the timbral characteristics of two different sounds. Musical applications; The spectral envelope contains the data for spectral peaks/formants. Thus the transfer of timbral characteristics. However, it is not a full substitution of the whole envelope, but only a (weighted) crossover. It can therefore be used, for example, to prepare two sounds for morphing, especially when the timbral characteristics differ to a considerable extent. By moving one sound gradually into the timbral field of the other prior to morphing, a smoother morph transition can be achieved." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "interp", switch = "-i", min = 0, max = 1, input = "brk", def = 0, tip = "interp is the degree of replacement." },
}
 
dsp["Combine Diff - Find difference of two spectra "] = {
  cmds = { exe = "combine", mode = "diff", tip = "In COMBINE DIFF one spectrum is subtracted from another on a channel-by- channel basis (weighted by crossover). If amplitude in any channel goes negative, you can opt whether or not to retain the negative amplitudes. By default, the setting of any amplitudes < 0 to 0 protects against negative overflow, but you can use the -a flag and see what happens. Musical applications; This is a way to change a sound on a very experimental basis.Also see COMBINE SUM." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "crossover", switch = "-c", min = 0, max = 1, input = "brk", def = 0, tip = "is the amount of 2nd spectrum which is subtracted from the 1st (Range 0 to 1) " },
}
 
dsp["Combine Interleave - Interleave windows from infiles, leafsize windows per leaf  "] = {
  cmds = { exe = "combine", mode = "interleave", tip = "INTERLEAVE is a new version of the former SPECLIFN. It interleaves N spectra from N analysis files, thereby constructing a new analysis file as output. The spectral characteristics of several sounds are thus amalgamated into one new sound. The degree to which the original sounds are retained depends on the number of analysis windows used. A leafsize is simply a specified group of analysis windows. If leafsize is 1, then the windows from each file are interleaved one at a time. If leafsize is 2, then the windows from each file are interleaved two at a time, etc. Larger leafsize retains more recognisable chunks of the original sounds. Musical applications. With very small values for leafsize, the various sounds are mingled quite thoroughly and relatively smoothly – relative to the nature of the input sounds, though a somewhat grainy effect may result. With larger values for leafsize, say 8, 10, 20 or more, more of each component sound is heard in each leaf and the result is more like a churning or pulsating effect as the new soundfile oscillates among the sound material in the various source soundfiles. To churn means to shake, stir or agitate violently, as when making butter from milk, which is nicely descriptive of what happens when several sound sources, each fairly strong, are interleaved in this way. [It's one of my favourites! – Ed.]" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "leafsize", min = 1, max = 40000, def = 1, tip = "number of analysis windows in each leaf" },
}
 
dsp["Combine Make - Generate spectrum from pitch & formant data only"] = {
  cmds = { exe = "combine", mode = "make", tip = ".frq + .for = .ana. This is an interesting way to combine the pitch data and the timbral (formant) data from any two arbitrarily chosen sounds." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select the first data input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the second data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}
 
dsp["Combine Make2 - Generate a spectrum from only pitch, formant and envelope data "] = {
  cmds = { exe = "combine", mode = "make2", tip = ".frq + .for + .evl = .ana. This function can be used to combine the pitch contour, formant envelope, and loudness envelope of different sounds, thereby transferring the characteristics of one sound onto another. MAKE2 extends the functionality of MAKE by enabling (spectral) envelope data to be used as well. The source sound could also be recreated from the data in the pitch, formant and envelope files extracted from its own spectrum. This provides a way to check on the accuracy of the extraction – small details, especially of noise sounds, often get changed. Musical applications; New sounds can be engineered with this procedure by combining three different features of up to three different sounds. The pitch data file contains an extracted pitch contour, the formant data file contains the strongest, most resonant, frequency bands of the spectrum, and the envelope file contains the contour of the spectral envelope, the (changing) amplitude pattern made by the frequencies. Given that you are alert to the distinctive characteristics of your sounds, this function provides a tremendously powerful tool with which to construct new sonic material. This is important not just from a purely sonic point of view. It is also an important tool for creating structural relationships between sounds, that is, the duplication of identical or similar shapes within a composition. This can create obvious relationships or it can be used to create subtle, hardly perceptible connections. Overall, the result is to achieve a sense of unity within the composition, even when diverse materials are being used. We are familiar with this in note-based music, where motifs and themes can have similar components, contours etc. COMBINE MAKE and COMBINE MAKE2 therefore provide a vital compositional tools in the spectral realm. Recreating the original sound from its own pitch, formant and envelope data provides a diagnostic tool for assessing how the analysis is coping with the frequency contents of a sound." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select the first data input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the second data input sound to the process" },
  arg3 = { name = "Input.env", input = "env", tip = "Select the third data input sound to the process" },
  arg4 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}
 
dsp["Combine Max - In each analysis channel, in each window, take the maximum value amongst the input files "] = {
  cmds = { exe = "combine", mode = "max", tip = "COMBINE MAX studies the data in all the input analysis files. For each channel it retains only the single loudest spectral component among all the input files. In spite of the loss of the other components, the sound often remains the same to a remarkable degree. But how several sounds will merge in this way will be a little unpredictable: because data from the files which DO NOT have the loudest spectral component will simply be omitted.The spectral characteristics of the input sounds are just merged, producing an ultra-seamless mix. The input sounds remain recognisable, just like an ordinary mix. Note that this is different from the programs which transfer the spectrum or spectral envelope of a sound. With the latter, the audible characteristics of one of the sounds are noticeably changed. Rob Waring's wonderful hornwhivoc sound, which we often use for demos, was made with this function. It merges a horn, a whistle and a voice (note that these are timbrally very different sounds). You can find it on the Future Music cover CD for December 1993." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}
 
dsp["Combine Mean 1 - Generate the spectral 'mean' of 2 sounds - mean channel amp of 2 files : mean of two pitches"] = {
  cmds = { exe = "combine", mode = "mean 1", tip = "1 mean channel amp of 2 files : mean of two pitches - (set points to anything other then 1024 or the exe will crash in windows vista!) This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 2 - Generate the spectral 'mean' of 2 sounds - mean channel amp of 2 files : mean of two frequencies"] = {
  cmds = { exe = "combine", mode = "mean 2", tip = "2 mean channel amp of 2 files : mean of two frequencies - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 3 - Generate the spectral 'mean' of 2 sounds - channel amp from file 1 : mean of two pitches"] = {
  cmds = { exe = "combine", mode = "mean 3", tip = "3 channel amp from file 1 : mean of two pitches - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 4 - Generate the spectral 'mean' of 2 sounds - channel amp from file 1 : mean of two frequencies"] = {
  cmds = { exe = "combine", mode = "mean 4", tip = "4 channel amp from file 1 : mean of two frequencies -(set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 5 - Generate the spectral 'mean' of 2 sounds - channel amp from file 1 : mean of two pitches"] = {
  cmds = { exe = "combine", mode = "mean 5", tip = "5 channel amp from file 1 : mean of two pitches - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 6 - Generate the spectral 'mean' of 2 sounds - channel amp from file 1 : mean of two frequencies"] = {
  cmds = { exe = "combine", mode = "mean 6", tip = "6 channel amp from file 1 : mean of two frequencies - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 7 - Generate the spectral 'mean' of 2 sounds - max channel amp of 2 files : mean of two pitches"] = {
  cmds = { exe = "combine", mode = "mean 7", tip = "7 max channel amp of 2 files : mean of two pitches - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Mean 8 - Generate the spectral 'mean' of 2 sounds - max channel amp of 2 files : mean of two frequencies"] = {
  cmds = { exe = "combine", mode = "mean 8", tip = "8 max channel amp of 2 files : mean of two frequencies - (set points to anything other then 1024 or the exe will crash in windows vista!). This program takes 2 input sounds and calculates the average of the 2 signals, spectral window by spectral window. The average in loudness between, for example, two speech sounds, should reach some vague mean between the formants of the speech components being used. The frequency average is altogether more strange. It may, for example, convert harmonic into inharmonic spectra. Seriously wierd. Musical applications; It is not really possible to predict the results of this way to combine sounds, as they are so dependent on the nature of the sources. It is best to try it with the type of source file you favour and try to build up some experimental data about what happens." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lofrq", switch = "-l", min = 0, max = 22050, def = 350, tip = "low frequency limit of channels to look at" },
  arg5 = { name = "hifrq", switch = "-h", min = 0, max = 22050, def = 12000, tip = "high frequency limit of channels to look at" },
  arg6 = { name = "chans", switch = "-c", min = 2, max = 40000, def = 512, tip = "number of significant channels to compare. Default: ALL within range." },
  arg7 = { name = "-z", switch = "-z", tip = "zeroes channels OUTSIDE the frequency range specified" },
}
 
dsp["Combine Sum - Find the sum of two spectra  "] = {
  cmds = { exe = "combine", mode = "sum", tip = "COMBINE SUM examines the amplitude levels in each channel of each of the two input analysis files. Where they differ, it adds this difference to the 1st file, as weighted by crossover. In other words, the result is infile plus the difference between it and infile2. When resynthesising, watch out for clipping. Musical applications; This is a way to adjust the timbre of a sound by adding to it some part of of the timbral features which belong to another sound. Very often, the change is both major and relatively unpredictable. Also see COMBINE DIFF." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "crossover", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "is the amount of 2nd spectrum which is added to the 1st (Range 0 to 1)" },
}
 
------------------------------------------------
-- constrict
------------------------------------------------
 
dsp["Constrict - Shorten the durations of any zero-level sections in a sound "] = {
  cmds = { exe = "constrict", mode = "constrict", tip = "This is a form of time-contraction which does not time-stretch the sonic-substance of the source. Note that the constriction is a percentage. Thus: 50 will halve the silent gaps, 75 will take away three quarters of the silent gaps, 20 will reduce them to one-fifth their original length, 100 will eliminate the silences, 150 will cause the non-silent portions to overlap by half the duration of the original silences. It works only with soundfiles which contain areas of zero-level signal which can be contracted by the process.CONSTRICT can be used to take a sequence of rapidfile events separated by silences and make them even tighter." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "constriction", min = 0, max = 200, tip = "percentage deletion of zero-level areas." },
}
 
------------------------------------------------
-- copysfx
------------------------------------------------
 
dsp["Copysfx - Copy/convert soundfiles  "] = {
  cmds = { exe = "copysfx", mode = "", tip = "By default COPYSFX writes a PEAK chunk to the output soundfile. This stores the position and value of the first absolute maximum sample in each channel, together with a timestamp. Almost all the Toolkit programs print the PEAK data to the screen on completion. It can also be shown via SFPROPS. For integer formats, the value will of necessity be clipped to 1.0. For floating-point formats the value reflects the true peak values in the file. However, it is still common for many applications to assume a fixed length header, and be broken by any file with additional chunks such as the PEAK data. The -h flag enables you to write a simple header with no extra chunks, should you need to use the soundfile with one of these header-challenged applications. Also see further explanations in the System Utility Manual. Note that while the new program CHXFORMAT can save considerable disk space and time by changing the header of a file directly, it is inherently risky. COPYSFX is non-destructive, and if space and time permit, it is still the recommended tool for converting files from one format to another. CHXFORMAT may however be required where it is desired to set an unusual or experimental speaker mask not supported by COPYSFX." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "-d", switch = "-d", tip = "apply TPDF (2-LSB triangular) dither to a 16-bit outfile" },
  arg4 = { name = "N", switch = "-h", min = 0, max = 1, def = 0, tip = "write minimum header: 0 = minimum (no extra data), 1 = PEAK data only" },
  arg5 = { name = "N", switch = "-s", min = 1, max = 4, tip = "force output sample type to type N; 16-bit integers (shorts), 32-bit integer (longs), 32-bit floating-point, 24-bit integer" },
  arg6 = { name = "N", switch = "-t", min = 0, max = 1, def = 0, tip = "write outfile format as type N; 0 = standard soundfile (.wav, .aif, .afc, .aifc), 1 = generic WAVE_EX (no speaker assignments)" },
}
 
------------------------------------------------
-- distort
------------------------------------------------
 
dsp["Distort Average - average the waveshape over N wavecycles"] = {
  cmds = { exe = "distort", mode = "average", tip = "DISTORT AVERAGE performs a mathematical averaging of the data in cyclecnt pseudo-wavecycles. The effect is more akin to a loss of resolution than the blurring which might be expected. Values below 10 retain some semblance of the original, while values of, for example, 100 seem to create a kind of 'sample hold' effect. For modest distortion, values 5 or less are recommended." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 2, max = 6000, def = 5, tip = "number of cycles over which to average (Range: > 1)" },
  arg4 = { name = "maxwavelen", switch = "-m", min = 0, max = length/1000, tip = "maximum permissible wavelength in seconds (Default: 0.50)" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = 1000, def = 0, tip = "(integer) number of wavecycles to skip at start of file" },
}

dsp["Distort Cyclecnt - Count 'wavecycles' in soundfile"] = {
  cmds = { exe = "distort", mode = "cyclecnt", tip = "DISTORT CYCLECNT checks the waveform of the infile for zero crossings, determining how many segments lie between these crossings. It then displays this figure on screen. Musical applications; This information ú the number of 'waveycles' in a file – can help predict the level of distortion a given process might produce." },
  arg1 = { name = "infile", input = "wav", tip = "soundfile to examine" },
}
 
dsp["Distort Delete - 1 Time-contract soundfile by deleting 'wavecycles' "] = {
  cmds = { exe = "distort", mode = "delete 1", tip = "One 'wavecycle' in every cyclecnt 'wavecycles' is retained. Mode 1 dramatically removes data from the infile, leaving very little behind. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "groups of 'wavecycles': really the level of resolution at which the process will work " },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file." },
}
 
dsp["Distort Delete - 2 Time-contract soundfile by deleting 'wavecycles' "] = {
  cmds = { exe = "distort", mode = "delete 2", tip = "because it sets out to retain the strongest (i.e., highest amplitude) 'wavecycle' in each set, retains more recognisable features from the original. This process achieves a time-compression and textural roughening of the source. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "groups of 'wavecycles': really the level of resolution at which the process will work " },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file." },
}
 
dsp["Distort Delete - 3 Time-contract soundfile by deleting 'wavecycles' "] = {
  cmds = { exe = "distort", mode = "delete 3", tip = "The weakest (single) 'wavecycle' in every cyclecnt 'wavecycles' is deleted. This process achieves a time-compression and textural roughening of the source. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "groups of 'wavecycles': really the level of resolution at which the process will work " },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file." },
}
 
dsp["Distort Divide - Distortion by dividing wavecycle frequency"] = {
  cmds = { exe = "distort", mode = "divide", tip = "Without altering duration, this process effectively lowers the sound while adding a rough texture.This is a useful form of distortion because it roughens the sound without being too violent about it" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 2, max = 16, tip = "divider (Range: integer only, 2 to 16)" },
  arg4 = { name = "-i", switch = "-i", tip = "use waveform interpolation: slower but cleaner" },
}
 
dsp["Distort Envel - 1 Rising envelope - Impose envelope over each group of cyclecnt 'wavecycles'  "] = {
  cmds = { exe = "distort", mode = "envel 1", tip = "Rising envelope - The process takes the amplitude envelope data for each group of cyclecnt 'wavecycles' and adjusts this data to form a single envelope shape (for that group) according to the mode selected. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "number of 'wavecycles' under a single envelope " },
  arg4 = { name = "throughing", switch = "-t", min = 0, max = 1, input = "brk", def = 0, tip = "the trough depth of the envelope (Range: 0 [most troughed] to 1 [least troughed], Default = 0)" },
  arg5 = { name = "exponent", switch = "-e", min = 0, max = 50, input = "brk", def = 0, tip = "exponent to shape envelope rise or decay" },
}
 
dsp["Distort Envel - 2 Falling envelope - Impose envelope over each group of cyclecnt 'wavecycles'  "] = {
  cmds = { exe = "distort", mode = "envel 2", tip = "Falling envelope - The process takes the amplitude envelope data for each group of cyclecnt 'wavecycles' and adjusts this data to form a single envelope shape (for that group) according to the mode selected. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "number of 'wavecycles' under a single envelope " },
  arg4 = { name = "throughing", switch = "-t", min = 0, max = 1, input = "brk", def = 0, tip = "the trough depth of the envelope (Range: 0 [most troughed] to 1 [least troughed], Default = 0)" },
  arg5 = { name = "exponent", switch = "-e", min = 0, max = 50, input = "brk", def = 0, tip = "exponent to shape envelope rise or decay" },
}
 
dsp["Distort Envel - 3 Troughed envelope - Impose envelope over each group of cyclecnt 'wavecycles'  "] = {
  cmds = { exe = "distort", mode = "envel 3", tip = "Troughed envelope - The process takes the amplitude envelope data for each group of cyclecnt 'wavecycles' and adjusts this data to form a single envelope shape (for that group) according to the mode selected. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", tip = "number of 'wavecycles' under a single envelope " },
  arg4 = { name = "throughing", min = 0, max = 1, input = "brk", def = 0, tip = "the trough depth of the envelope (Range: 0 [most troughed] to 1 [least troughed], Default = 0)" },
  arg5 = { name = "exponent", switch = "-e", min = 0, max = 50, input = "brk", def = 0, tip = "exponent to shape envelope rise or decay" },
}

dsp["Distort Envel - 4 User-defined envelopee - Impose envelope over each group of cyclecnt 'wavecycles'  "] = {
  cmds = { exe = "distort", mode = "envel 4", channels = "any", tip = "4 User-defined envelope - The process takes the amplitude envelope data for each group of cyclecnt 'wavecycles' and adjusts this data to form a single envelope shape (for that group) according to the mode selected. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "envfile", input = "txt", tip = "defines user envelope as time value pairs (Range of value is 0 to 1)" },
  arg4 = { name = "cyclecnt", min = 0, max = cycles, input = "brk", def = 0, tip = "number of 'wavecycles' under a single envelope" },
}
 
dsp["Distort Filter - Time-contract a sound by filtering out wavecycles above freq"] = {
  cmds = { exe = "distort", mode = "filter 2", tip = "Period and frequency are inverse functions. Therefore it is possible to relate the length of a 'wavecycle' to the frequency it would have were it to recur regularly. This program therefore filters by removing 'wavecycles' shorter or longer than those relating to a specific, user-defined, frequency. The duration of the outfile is affected by this process: because 'wavecycles' are being removed, the outfile will be shorter, by varying degrees. The aural effect of the DISTORT FILTER process is actually like gating. In gating, you can imagine a horizontal line drawn through the time/amplitude display of a soundfile. If the peaks above the line are retained (by filtering out lower frequencies), you just hear them (joined up). Similarly, if you filter out above the line, the peaks are gone, leaving the lower sound material (joined up). The difference here is that the process is operating on 'pseudo-wavecycles' according to their length, so here the results are more unpredictable and also distort the sound to some degree, depending on where the horizontal line – the frequency variable(s) – is drawn. Technically, the process is akin to low-, high- and band-pass filters, but aurally it is more like gating. You can therefore use this procedure to cut out some and distort other material in a sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 10, max = 22050, input = "brk", def = 11000, tip = "frequency in Hz (Range: 10.0 to 22050.0)" },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file" },
}
 
dsp["Distort Filter - Time-contract a sound by filtering out wavecycles below & above freq"] = {
  cmds = { exe = "distort", mode = "filter 3", tip = "Period and frequency are inverse functions. Therefore it is possible to relate the length of a 'wavecycle' to the frequency it would have were it to recur regularly. This program therefore filters by removing 'wavecycles' shorter or longer than those relating to a specific, user-defined, frequency. The duration of the outfile is affected by this process: because 'wavecycles' are being removed, the outfile will be shorter, by varying degrees. The aural effect of the DISTORT FILTER process is actually like gating. In gating, you can imagine a horizontal line drawn through the time/amplitude display of a soundfile. If the peaks above the line are retained (by filtering out lower frequencies), you just hear them (joined up). Similarly, if you filter out above the line, the peaks are gone, leaving the lower sound material (joined up). The difference here is that the process is operating on 'pseudo-wavecycles' according to their length, so here the results are more unpredictable and also distort the sound to some degree, depending on where the horizontal line – the frequency variable(s) – is drawn. Technically, the process is akin to low-, high- and band-pass filters, but aurally it is more like gating. You can therefore use this procedure to cut out some and distort other material in a sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq1", min = 10, max = 22050, input = "brk", def = 80,  tip = "frequency in Hz to delete below" },
  arg4 = { name = "freq2", min = 10, max = 22050, input = "brk", def = 12000, tip = "frequency in Hz to delete above" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file" },
}
 
dsp["Distort Filter - Time-contract a sound by filtering out wavecycles below freq"] = {
  cmds = { exe = "distort", mode = "filter 1", tip = "Period and frequency are inverse functions. Therefore it is possible to relate the length of a 'wavecycle' to the frequency it would have were it to recur regularly. This program therefore filters by removing 'wavecycles' shorter or longer than those relating to a specific, user-defined, frequency. The duration of the outfile is affected by this process: because 'wavecycles' are being removed, the outfile will be shorter, by varying degrees. The aural effect of the DISTORT FILTER process is actually like gating. In gating, you can imagine a horizontal line drawn through the time/amplitude display of a soundfile. If the peaks above the line are retained (by filtering out lower frequencies), you just hear them (joined up). Similarly, if you filter out above the line, the peaks are gone, leaving the lower sound material (joined up). The difference here is that the process is operating on 'pseudo-wavecycles' according to their length, so here the results are more unpredictable and also distort the sound to some degree, depending on where the horizontal line – the frequency variable(s) – is drawn. Technically, the process is akin to low-, high- and band-pass filters, but aurally it is more like gating. You can therefore use this procedure to cut out some and distort other material in a sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 10, max = 22050, input = "brk", def = 250, tip = "frequency in Hz (Range: 10.0 to 22050.0)" },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file" },
}
 
dsp["Distort Fractal - Superimpose miniature copies of source wavecycles onto themselves"] = {
  cmds = { exe = "distort", mode = "fractal", tip = "Note the very wide range of scaling. Because it is a divisor, the larger the value of scale the shorter will be the miniature copies to be superimposed. These superimposed copies can be made to increase (be careful!) or decrease in amplitude with the loudness parameter. Using a value of 1.0 maintains the original amplitude of the infile, which will be heard as pretty much as the original, but with the superimpositions on top of it. Loudness is therefore a means of balancing the input and the processed sound components.This is a powerful and somewhat wild tool for producing distortion effects. The higher the value of scaling, the more the superimposed copies appear as a sheen of distortion above the original sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "scaling", min = 2, max = srate/2, input = "brk", def = 2, tip = "(integer) division of scale of source wave (Range: 2 to sample_rate/2)" },
  arg4 = { name = "loudness", min = 0, max = 10, input = "brk", def = 1, tip = "loudness of scaled component relative to source (Loudness of source is reckoned to be 1.0)" },
  arg5 = { name = "pre_attenuation", switch = "-p", tip = "apply attenuation to infile before processing" },
}
 
dsp["Distort Harmonic - Harmonic distortion by superimposing 'harmonics' onto 'wavecycles' (needs harmonic text file)"] = {
  cmds = { exe = "distort", mode = "harmonic", tip = "Harmonic distortion multiplies and adds within a single 'wavecycle' – possibly several times. For each harmonic_number in the harmonics-file, DISTORT HARMONIC scales and copies the shape of the 'wavecycle' harmonic_number times and adds the result to the original at the given amplitude (relative to that of the infile). This is by direct analogy to harmonic additive synthesis, in which a complex pitched sound is created by adding sinusoidal partials. Indeed, DISTORT HARMONIC can be used for just this purpose by using a sine wave as input. There is no internal scaling of harmonic amplitude values. It will be necessary in many cases to scale the infile with the prescale parameter to avoid overflow. Prescale is a multiplier, like an ordinary gain factor. (See the Gain – dB Chart). Musical applications. The higher 'harmonics' of the 'wavecycles' are heard as faster versions superimposed on the original 'wavecycle' and on the lower 'harmonics'. Therefore, the application is to add these higher and denser levels of distortion to the sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "harmonics-file ", min = 0, max = 0, input = "txt", def = 0, tip = "this process needs a text harmonics-file – contains harmonic_number amplitude pairs - see docs for syntax" },
  arg4 = { name = "pre_attenuation", switch = "-p", min = 0, max = 100, def = 0.9, tip = "apply attenuation to infile before processing" },
}
 
dsp["Distort Interact 1 - Time-domain interaction of two sounds - Interleave 'wavecycles' from the two infiles"] = {
  cmds = { exe = "distort", mode = "interact 1", tip = "In Mode 1 material from both soundfiles is audibly apparent due to the interleaving process.  Musical applications; DISTORT INTERACT can be used to achieve distortion which combines data from two different sounds or distortion which totally alters a sound." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Interact 2 - Time-domain interaction of two sounds - Impose 'wavecycle' lengths of 1st file on 'wavecycles' of 2nd"] = {
  cmds = { exe = "distort", mode = "interact 2", tip = "Impose 'wavecycle' lengths of 1st file on 'wavecycles' of 2nd. Musical applications; DISTORT INTERACT can be used to achieve distortion which combines data from two different sounds or distortion which totally alters a sound." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Interpolate - Time-stretch file by repeating wavecycles and interpolating between them "] = {
  cmds = { exe = "distort", mode = "interpolate", tip = "With this process, the shape of a 'wavecycle' is transformed into that of the next over multiplier repetitions. Note that this is waveshape-based interpolation, not a spectral interpolation, and that the length of the 'wavecycle' is also transformed by the process. The effect of the transformation is drastic, leading to a strongly granular outfile. The length of the outfile increases in step with the value of multiplier, as does the apparent pitchiness.The interpolation process adds a modulatory quality to the output, so that the successive wavecycles gliss and bend as they flow into one another. Even so, as multiplier increases, the perception of separate 'grains', i.e., 'wavecycles' increases. A value of 32, for example, changes the sound to a strange stream of modulating tones." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multiplier", min = 0, max = 50, input = "brk", def = 10, tip = "(integer) number of times each 'wavecycle' repeats" },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = 1000, def = 0, tip = "(integer) number of 'wavecycles' to skip at start of file" },
}
 
dsp["Distort Multiply - Distortion by multiplying wavecycle frequency"] = {
  cmds = { exe = "distort", mode = "multiply", tip = "The duration of the sound is not changed, only the frequency of the 'wavecycles', with the result that the pitch rises.The distortion is relatively mild, in that the original sound remains recognisable. However, the surface is textured and the pitch rises with each increase in the value of N. DISTORT MULTIPLY can be used, for example, to create high, modulating, grainy vocal sounds." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 2, max = 16, def = 2, tip = "multiplier (Range: 2 to 16, integer only)" },
  arg4 = { name = "-s", switch = "-s", tip = "smoothing (try this if glitches appear)" },
}
 
dsp["Distort Omit – Omit A out of every B wavecycles, replacing them with silence"] = {
  cmds = { exe = "distort", mode = "omit", tip = "Because the omitted 'wavecyles' are replaced by silence, the overall duration of the sound does not change. The larger the proportion of 'wavecycles' omitted from B, of course, the more distorted the the sound becomes. This distortion is like a rough texturing, rather than the highly modulatory results of some of the other processes.This process can be used, therefore, to achieve a rough texturing with no loss of duration." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "A", min = 0, max = length, input = "brk", tip = "number of wavecycles to omit, A may vary over time, but must always be less than B." },
  arg4 = { name = "B", min = 0, max = length, tip = "size of group of 'wavecycles' out of which to omit A wavecycles" },
}
 
dsp["Distort Overload 1 - Clip the signal with noise or a (possibly timevarying) waveform"] = {
  cmds = { exe = "distort", mode = "overload 1", tip = "The clip-level parameter is rather like a 'gate' level. If the signal level is already high, anything over, for example, 0.1 is likely to push it into distortion, and values considerably higher than this will make it heavily distorted. However, if it only distorts, Trevor advises me, when it reaches a level of, for example, 0.99, it is not going to be distorted very often. The sound doesn't actually have amplitude overload, because it is distorted by 'slicing off' the top (clipping) where it would have overloaded. The sound becomes loud and 'strained', like a voice which is shouting too loudly.Given the trials made so far, this can be a fairly subtle effect, but the words 'straining', 'loud', 'uncompromising' seem appropriate as the amplitude gets pushed towards the top of the range." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "clip-level", min = 0, max = 1, input = "brk", tip = "level at which the signal is to be clipped (Range: 0 to 1) The signal level is renormalised after clipping." },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", tip = "depth of the pattern of distortion imposed on clipped stretches of the signal. (Range: 0 to 1)" },
}
 
dsp["Distort Overload 2 - Clip the signal with noise or a (possibly timevarying) waveform"] = {
  cmds = { exe = "distort", mode = "overload 2", tip = "The clip-level parameter is rather like a 'gate' level. If the signal level is already high, anything over, for example, 0.1 is likely to push it into distortion, and values considerably higher than this will make it heavily distorted. However, if it only distorts, Trevor advises me, when it reaches a level of, for example, 0.99, it is not going to be distorted very often. The sound doesn't actually have amplitude overload, because it is distorted by 'slicing off' the top (clipping) where it would have overloaded. The sound becomes loud and 'strained', like a voice which is shouting too loudly. Mode 2 can add an extra ringing sound as the value for freq gets higher, e.g., 2000Hz and beyond. Given the trials made so far, this can be a fairly subtle effect, but the words 'straining', 'loud', 'uncompromising' seem appropriate as the amplitude gets pushed towards the top of the range." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "clip-level", min = 0, max = 1, input = "brk", def = 0.1, tip = "level at which the signal is to be clipped (Range: 0 to 1) The signal level is renormalised after clipping." },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", def = 1, tip = "depth of the pattern of distortion imposed on clipped stretches of the signal. (Range: 0 to 1)" },
  arg5 = { name = "freq", min = 0, max = 22500, input = "brk", def = 880, tip = "frequency of the waveform imposed on clipped stretches of the signal" },
}
 
dsp["Distort Pitch - Pitchwarp wavecycles of sound"] = {
  cmds = { exe = "distort", mode = "pitch", tip = "The random up/down movement of the 'wavecycles' within the total octvary range produces a great deal of bending of the sound, especially if the original alters its pitch a good deal. It is better, therefore, to start with relatively small values for octvary – e.g., less than 1 – so that you start to use this function with some degree of control over the results.The full power of DISTORT PITCH doesn't really come into its own until time-varying parameters are used, especially for cyclecnt. Large values for the latter will serve to slow down the rate of change. DISTORT PITCH is useful for creating 'flexitones' (to coin a term) – with distortion, of course." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "octvary", min = 0, max = 8, input = "brk", tip = "maximum possible transposition up or down in (fractions of) octaves (Range > 0.0 to 8.0)" },
  arg4 = { name = "cyclelen", switch = "-c", min = 1, max = cycles, input = "brk", tip = "mamimum number of 'wavecycles' between the generation of transposition values (Range: > 1, Default: 64)" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "(integer) number of 'wavecycles' to skip at start of file" },
}
 
dsp["Distort Pulsed 1- Impose impulse-train on a sound through a text file"] = {
  cmds = { exe = "distort", mode = "pulsed 1", tip = "Needs 'stime' textfile input, see CDP docs for specifics. Distort a sound by imposing a series of impulses on the source, or on a specific waveset segment of the source. An impulse is like a brief event created by a sharp envelope on the sound. The sound inside the impulse might glissando slightly, as if whatever is causing the impulsion has warped the sound by its impact." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "stime", min = 0, max = 0, input = "txt", def = 0, tip = "time in the source sound where the impulses begin" },
  arg4 = { name = "dur", min = 0.001, max = length/1000, tip = "length of time that the impulses continue" },
  arg5 = { name = "frq", min = 0, max = 1000, def = 240, tip = "number of impulses per second" },
  arg6 = { name = "frand", min = 0, max = 12, tip = "number of semitones by which to randomise the frequency of the impulses" },
  arg7 = { name = "trand", min = 0, max = 10, tip = "amount of time in seconds by which to randomise the relative time positions of amplitude peaks and troughs from impulse to impulse" },
  arg8 = { name = "arand", min = 0, max = 1, tip = "randomisation of the amplitude shape created by the peaks and troughs from impulse to impulse" },
  arg9 = { name = "transp", min = 0, max = 1, tip = "transposition contour of sound inside each impulse" },
  arg10 = { name = "tranrand", min = 0, max = 1, tip = "randomisation of transposition contour from impulse to impulse" },
  arg11 = { name = "-s", switch = "-s", tip = "keep start of source sound, before impulses begin (if any)" },
  arg12 = { name = "-e", switch = "-e", tip = "keep end of source sound, after impulses end (if any)" },
}
 
dsp["Distort Pulsed 2 - Imposed regular pulsations on a sound through a text file"] = {
  cmds = { exe = "distort", mode = "pulsed 2", tip = "Needs 'stime' textfile input, see CDP docs for specifics. Distort a sound by imposing a series of impulses on the source, or on a specific waveset segment of the source. An impulse is like a brief event created by a sharp envelope on the sound. The sound inside the impulse might glissando slightly, as if whatever is causing the impulsion has warped the sound by its impact." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "stime", min = 0, max = 0, input = "txt", def = 0, tip = "time in the source sound where the impulses begin" },
  arg4 = { name = "dur", min = 0.001, max = length/1000, tip = "length of time that the impulses continue" },
  arg5 = { name = "frq", min = 0, max = 1000, def = 240, tip = "number of impulses per second" },
  arg6 = { name = "frand", min = 0, max = 12, tip = "number of semitones by which to randomise the frequency of the impulses" },
  arg7 = { name = "trand", min = 0, max = 10, def = 0.5, tip = "amount of time in seconds by which to randomise the relative time positions of amplitude peaks and troughs from impulse to impulse" },
  arg8 = { name = "arand", min = 0, max = 1, tip = "randomisation of the amplitude shape created by the peaks and troughs from impulse to impulse" },
  arg9 = { name = "cycletime", min = 0, max = 1, tip = "duration in seconds of wavecycles to grab as sound substance inside the impulses" },
  arg10 = { name = "transp", min = 0.001, max = 4, tip = "transposition contour of sound inside each impulse" },
  arg11 = { name = "tranrand", min = 0, max = 1, tip = "randomisation of transposition contour from impulse to impulse" },
  arg12 = { name = "-s", switch = "-s", tip = "keep start of source sound, before impulses begin (if any)" },
  arg13 = { name = "-e", switch = "-e", tip = "keep end of source sound, after impulses end (if any)" },
}

dsp["Distort Pulsed 3 - Imposed regular pulsations on a sound through a text file"] = {
  cmds = { exe = "distort", mode = "pulsed 3", tip = "number of wavecycles to grab as sound substance inside the impulses. Needs 'stime' textfile input, see CDP docs for specifics. Distort a sound by imposing a series of impulses on the source, or on a specific waveset segment of the source. An impulse is like a brief event created by a sharp envelope on the sound. The sound inside the impulse might glissando slightly, as if whatever is causing the impulsion has warped the sound by its impact." },
  arg1 = { name = "infile", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "outfile", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "stime", input = "txt", tip = "stime is given as samplecnt, i.e., number of samples" },
  arg4 = { name = "dur", min = 0.001, max = length/1000, tip = "length of time that the impulses continue" },
  arg5 = { name = "frq", min = 0, max = 1000, def = 240, tip = "number of impulses per second" },
  arg6 = { name = "frand", min = 0, max = 12, tip = "number of semitones by which to randomise the frequency of the impulses" },
  arg7 = { name = "trand", min = 0, max = 10, def = 0.5, tip = "amount of time in seconds by which to randomise the relative time positions of amplitude peaks and troughs from impulse to impulse" },
  arg8 = { name = "arand", min = 0, max = 1, tip = "randomisation of the amplitude shape created by the peaks and troughs from impulse to impulse" },
  arg9 = { name = "cycletime", min = 0, max = 1, tip = "number of wavecycles to grab as sound substance inside the impulses" },
  arg10 = { name = "transp", min = 1, max = 256, tip = "transposition contour of sound inside each impulse" },
  arg11 = { name = "tranrand", min = 0, max = 1, tip = "randomisation of transposition contour from impulse to impulse" },
  arg12 = { name = "-s", switch = "-s", tip = "keep start of source sound, before impulses begin (if any)" },
  arg13 = { name = "-e", switch = "-e", tip = "keep end of source sound, after impulses end (if any)" },
}
 
dsp["Distort Reform - Convert to click streams"] = {
  cmds = { exe = "distort", mode = "reform 6", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to fixed level square wave"] = {
  cmds = { exe = "distort", mode = "reform 1", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to fixed level triangular wave"] = {
  cmds = { exe = "distort", mode = "reform 3", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to inverted half-cycles"] = {
  cmds = { exe = "distort", mode = "reform 5", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to sinusoid"] = {
  cmds = { exe = "distort", mode = "reform 7", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to square wave"] = {
  cmds = { exe = "distort", mode = "reform 2", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Convert to triangular wave"] = {
  cmds = { exe = "distort", mode = "reform 4", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Distort Reform - Exaggerate waveform contour"] = {
  cmds = { exe = "distort", mode = "reform 8", tip = "This process reads each 'wavecycle' (sound inbetween zero crossings) and replaces it with a different waveform of the same length. Several waveform options are provided. Those which do not fix the amplitude level respond to the varying amplitude levels of each successive wavecycle, thus producing an additional (and arbitrary) distortion feature.The 'fixed level' options produce consistently loud output.The 'click' option replaces each 'wavecycle' with a mishmash of square pulses several samples long (random sizes), which sounds a bit like a rattle.The 'sinusoid' option, as might be expected, is relatively smooth. It is actually a subtle form of filtering. The sine waves vary in length and amplitude because they are based on 'wavecycles' and because only some of the 'wavecycles' are replaced.The 'exaggeration' option just seems to add a surface buzz." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "exaggeration", min = 0, max = 40, input = "brk", def = 10, tip = "exaggeration factor (Range: 0.000002 to 40.0)" },
}
 
dsp["Distort Repeat - Timestretch soundfile by repeating wavecycles"] = {
  cmds = { exe = "distort", mode = "repeat", tip = "The repetition of the 'wavecycles' stretches out the sound, making it both longer and more granular in texture. This granularity is increased if (increasingly larger) groups of cyclecnt 'wavecycles' are used: then the whole group repeats multiplier times. DISTORT REPEAT produces long, grainy (distorted) sounds. The sense of stretching out the original is very apparent.A significant application of DISTORT REPEAT is that, by increasing the cyclecnt factor, one crosses the pitch-perception boundary: that is, starting with a noisy sound in which all the wavecycles are randomly different, one ends up with, for example, 7 repetitions of the same wavecycle, followed by 7 of another and so on – and each of these comprise sufficient repetitions for us to hear pitch. Thus the noise source becomes a string of pitch beads, each of arbitrary timbre. With a cyclecnt of, for example, 128, one can even get a slowish random melody." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multiplier", min = 0, max = 50, input = "brk", def = 3, tip = "number of times (integer) each 'wavecycle' (group) repeats" },
  arg4 = { name = "cyclelen", switch = "-c", min = 1, max = 1000, input = "brk", tip = "number of 'wavecycles' (integer) in repeated groups" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "number of 'wavecyles' (integer) to skip at start of file " },
}
 
dsp["Distort Repeat 2 - Repeat wavecycles without time-stretching"] = {
  cmds = { exe = "distort", mode = "repeat2", tip = "Repeating the 'wavecycles' without time-stretching (as in DISTORT REPEAT) enables you to increase the strength of the distortion with the multiplier parameter without making the output file any longer than the original. Larger values for cyclecnt increases the length of infile that is affected. Higher values for multiplier increase the distortion, while higher values for cyclecnt increase the length of infile that is processed as one unit. Thus we could have: a low value for multiplier coupled with a high value for cyclecnt – this will produce a bit of distortion while the source remains recognisable, a high value for multiplier coupled with a low value for cyclecnt – this will produces a great deal of distortion, but the effect is limited because only a few cycles are affected as a unit. High values for both parameters – this appears to create the most distortion" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multiplier", min = 0, max = 50, input = "brk", def = 3, tip = "number of times (integer) each 'wavecycle' (group) repeats" },
  arg4 = { name = "cyclelen", switch = "-c", min = 1, max = 1000, input = "brk", tip = "number of 'wavecycles' (integer) in repeated groups" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "number of 'wavecyles' (integer) to skip at start of file " },
}
 
dsp["Distort Replace – The strongest wavecycle in a cyclecnt group replaces the others"] = {
  cmds = { exe = "distort", mode = "replace", tip = "The replacing action serves to simplify the sound. Note that the single strong 'wavecycle' in the group will take the place of several others, which will be deleted. This simplification becomes extreme when the cyclecnt is high, leading to a 'sample-hold' kind of stepped effect. Time-varying cyclecnt makes it possible to introduce gradual change. With DISTORT REPLACE we can achieve a simplification of the sound, up to very clear 'sample-hold' type stepped tones" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = 500, input = "brk", def = 20, tip = "(integer) size of group of 'wavecycles'" },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "number of 'wavecyles' (integer) to skip at start of file" },
}
 
dsp["Distort Replim – Timestretch by repeating wavecycles (below a specified frequency) "] = {
  cmds = { exe = "distort", mode = "replim", tip = "This function is like DISTORT REPEAT, but with a slight change. Here the length of wavecycle to be affected can be set. Thus, if you set a mid-range frequency, only those below that frequency will repeat, and the others (above the frequency) will be discarded (filtered out). Hence the name 'REP-LIM', meaning 'repeat (with a) limit'. DISTORT REPLIM is therefore like a filtering program that also repeats wavecycles. In the DISTORT set, the wavecycles are wavelengths that occur between zero crossings, so distortion also occurs. It is helpful to remember that wavelength is inversely proportional to frequency. Wavelength is the actual physical length of the oscillation, and frequency is the number of cycles that occur in one second (i.e., Hertz). These two aspects of sound are inversely proportional to one another: P = 1/f. For example, a sound oscillating at 100 Hz will have a period, i.e., a wavelength of 1/100 meters = 0.01 meters (0.39 inch). A sound oscillating at 1000Hz will have a wavelength of 1/1000 meters = 0.001 meters (0.039 inch). Short wavecycles are therefore higher in pitch and long ones are lower in pitch. When the frequency setting for DISTORT REPLIM is high, the filter point is set higher and more of the sound will be retained. Here we are dealing with 'pseudo-wavecycles' (portions of soundfile between zero crossings), which is what introduces distortion into the equation.The net result of the function is to create repetition distortion while filtering out a user-definable amount of the higher frequencies. Remember that the relative amounts of high and low frequencies in the infile will affect the results." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multiplier", min = 0.001, max = 100, input = "brk", def = 5, tip = "the number of times each wavecycle (group) repeats (Integer)" },
  arg4 = { name = "cyclecnt", switch = "-c", min = 0, max = 500, input = "brk", def = 20, tip = "the number of wavecycles in repeated groups" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = " the number of wavecycles to skip at the beginning of the soundfile" },
  arg6 = { name = "hilim", switch = "-f", min = 0, max = 22500, tip = "the frequency below which cycles are counted" },
}
 
dsp["Distort Reverse - Cycle-reversal distortion in which the wavecycles are reversed in groups"] = {
  cmds = { exe = "distort", mode = "reverse", tip = "Here the original soundfile is grouped into a series of 'wavecycles' with cyclecnt 'wavecycles' in each group. Then each of these groups of 'wavecycles' is reversed. The term 'distortion' here is something of a misnomer, because no distortion process is applied to the 'wvecycles' themselves. Instead, cyclecnt sets the number of 'wavecycles' which are to be copied in reverse as a group to the outfile. For example, if cyclecnt = 3, 15 'wavecycles' reversed in groups of 3 will assume the order: 3-2-1, 6-5-4, 9-8-7, 12-11-10, 15-14-13. Thus, not only is the sound material backwards, but the reversed 1st 'wavecycle' is now adjacent to the reversed 6th 'wavecycle'. This mimics the classical tape studio technique of cutting up a length of tape into segments (of varying lengths), reversing the segments, and joining up the reversed pieces. The result will be similar to a random brassage because of the differing lengths of the 'wavecycles'. The process moves steadily through the infile from beginning to end, so the normal order of the (reversed) events is preserved. It is surprising how normal the output can be. With mid-range values for cyclecnt (say, 30 to 100), one hears the original breaking up, but only with very large values for cyclecnt does one hear the sound sweeping backwards in large swathes. Again, it is a question of 'resolution': the size of the units being manipulated. A small value for cyclecnt will produce a grainy result, mid-values a 'broken up' result, and large values swathes of reversed sound. If the value for cyclecnt exceeds the number of 'wavecycles' in the infile, you will be told that the 'sound source is too short...'. DISTORT CYCLECNT returns the number of 'wavecycles' in a sound, should you want to provide a value for cyclecnt which is right up to the limit. Reversing the output of DISTORT REVERSE turns the cyclecnt groups back the other way while reading the whole soundfile from back to front, producing an interesting mixture of forwards and backwards! Using the time-varying option for cyclecnt provides an opportunity for dramatic or gradual changes in the output." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = 10000, input = "brk", def = 10, tip = "number of 'wavecycles' in a reversed group (Range: > 0)" },
}
 
dsp["Distort Shuffle - Distortion by shuffling 'wavecycles"] = {
  cmds = { exe = "distort", mode = "shuffle", tip = "A simple reordering in which domain and image have the same number of characters will suitably roughen up the sound. As the image duplicates characters, some time-stretching will occur. Introducing higher cyclecnt values will mean that the infile is processed in larger units, increasing the recognisability of the original. Note that, in spite of the higher cyclecnt, time-stretching does not result – unless domain characters repeat in the image. Thus, a domain-image of abc-cba would be: 3-2-1, 6-5-4 etc. but with a cyclecnt of 5, a would comprise 1-2-3-4-5 (in the domain) b would comprise 6-7-8-9-10 (in the domain) c would comprise 11-12-13-14-15 (in the domain) and the image c-b-a would now be: 11-12-13-14-15, 6-7-8-9-10, 1-2-3-4-5 – 26-27-28-29-30, 21-22-23-24-25, 16-17-18-19-20. Note how the cyclecnt 'wavecycles' proceed sequentially forward in the file, even though the image involves a reversal of the domain. This is what produces the increased recognisability. Musical applications; The possibilities focus here on sculpting the roughness of the distortion along with time-stretch factors. Lots of room for playing with the image shapes." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "domain-image", input = "string", def = "abc-cba", tip = "domain – set of letters representing 'wavecycles'. Image – set of letters which forms some permutation of the domain set" },
  arg4 = { name = "cyclecnt", switch = "-c", min = 1, max = 32768, input = "brk", def = 1, tip = "the size of 'wavecycle' groups to process: each character in domain-image represents cyclecnt groups of 'wavecycles' (Default: 1)" },
  arg5 = { name = "skipcycles", switch = "-s", min = 0, max = length/1000, def = 0, tip = "number of 'wavecyles' (integer) to skip at start of file" },
}
 
dsp["Distort Telescope - Time-contract sound by telescoping N wavecycles into 1"] = {
  cmds = { exe = "distort", mode = "telescope", tip = "Although at first rather like DISTORT OMIT, here the 'wavecycles' are not deleted as such. Instead, they are superimposed (i.e., mixed) onto each other, with shorter 'wavecycles' being stretched to fit the longest one in each group of cyclecnt 'wavecycles'. The outfile will usually be much shorter than the infile and can be reduced to a mere blip with this process. The -a flag tells the program to telescope to the average 'wavecycle' length, rather than to the longest. Since the longest 'wavecycle' in each group is compressed by this method, the outfile will be even shorter. Interesting results can be achieved with small values for cyclecnt, the output tending to have a 'mushy' quality. It responds well to pitched material, producing a singing, if mushy, tone." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cyclecnt", min = 0, max = 1000, input = "brk", def = 20, tip = "the number of 'wavecycles' in a group" },
  arg4 = { name = "skipcycles", switch = "-s", min = 0, max = cycles, tip = "number of 'wavecyles' (integer) to skip at start of file" },
  arg5 = { name = "-a", switch = "-a", tip = "telescope to an average 'wavecycle' length (Default: telescope to the longest 'wavecycle' length)" },
}
 
------------------------------------------------
-- envel
------------------------------------------------
 
dsp["Convert a binary envelope file to a (text) breakpoint envelope file with dB values"] = {
  cmds = { exe = "envel", mode = "envtodb", tip = "This function converts a binary envelope file to a text breakpoint envelope file with its values expressed in dB. Bear in mind that 0.0dB represents full volume, so the numbers in the dB file ought to be negative. A positive value indicates a boosting of the original level (care!). See the Gain - dB Chart for orientation regarding the aural significance of dB values. The datareduce parameter enables you to reduce the amount of data contained in the file. A low value, such as 0.2 will keep most of the data, while a high value such as 0.8 will lose most of the data – while trying to retain the basic shape of the envelope. Musical applications; If you prefer to enter mixfile amplitude data in dB, using ENVEL ENVTODB can provide experience in relating dB values to what you hear in various sounds." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "datareduce", switch = "-d", min = 0, max = 1, def = 1, tip = "weights in favour of the quantity or the accuracy of the data written to the breakpoint file (Range: 0 [quantity] to 1 [accuracy])" },
}
 
dsp["Envel Cyclic - 1 Rising - Create a sequence of repeated envelopes, in a binary envelope file"] = {
  cmds = { exe = "envel", mode = "cyclic 1", tip = "1 Rising. ENVEL CYCLIC creates a binary envelope file that cyclically repeats the shape that you give to it, at whatever repetition rate you want. It is therefore a way to rhythmicise a continuous sound by imposing a regularly pulsating envelope. The program is very flexible, allowing you to set the output duration, the duration of the repeating contour shapes (time-variable), phase (start point in the shape), type and depth of shape. The window-size (wsize) determines how accurately the output envelope is defined: a small window-size gives a very precisely defined envelope, a larger window-size gives a less well-defined envelope. Mode 4 accepts a (special) text breakpoint file with which you can define an envelope shape. What is special about this user-defined breakpoint file is that its time-units are relative. You the user define the pulse shape between times 0 and 1, or between times 0 and 1000, but the actual duration will be scaled up or down to fit the duration of (each) cell-dur. Cell-dur may be a constant, or it may itself be defined in a time-varying breakpoint file. The shape will then be stretched or compressed to to fit each (changing) cell-dur. In this way, you can create a pulsing envelope that accelerates, gets slower, or is randomised. Musical applications; ENVEL CYCLIC can be used to create an amplitude tremolo with a contour shape designed specifically for the task in hand." },
  arg1 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg2 = { name = "wsize", min = 0, max = 10000, def = 100, tip = "envelope window size in milliseconds, i.e., the resolution of the envelope generated" },
  arg3 = { name = "total-dur", min = 0, max = 30, def = 6, tip = "duration of the output soundfile file" },
  arg4 = { name = "cell-dur", min = 0, max = 30, input = "brk", def = 2, tip = "duration of (repeating) envelope-cell" },
  arg5 = { name = "phase", min = 0, max = 1, tip = "where in the cell-envelope to begin the output: 0 = start, 1 = end of cell" },
  arg6 = { name = "trough", min = 0, max = 1, input = "brk", tip = "the lowest point of the envelope (Range: 0 to 1)" },
  arg7 = { name = "env-exp", min = 0, max = 12, def = 0, tip = "shapes are linear (0) or exponential (< 1 = start slow and speed up: steep at top; > 1 = start quickly and slow down: steep at bottom)" },
}
 
dsp["Envel Cyclic - 2 Falling - Create a sequence of repeated envelopes, in a binary envelope file"] = {
  cmds = { exe = "envel", mode = "cyclic 2", tip = "2 Falling. ENVEL CYCLIC creates a binary envelope file that cyclically repeats the shape that you give to it, at whatever repetition rate you want. It is therefore a way to rhythmicise a continuous sound by imposing a regularly pulsating envelope. The program is very flexible, allowing you to set the output duration, the duration of the repeating contour shapes (time-variable), phase (start point in the shape), type and depth of shape. The window-size (wsize) determines how accurately the output envelope is defined: a small window-size gives a very precisely defined envelope, a larger window-size gives a less well-defined envelope. Mode 4 accepts a (special) text breakpoint file with which you can define an envelope shape. What is special about this user-defined breakpoint file is that its time-units are relative. You the user define the pulse shape between times 0 and 1, or between times 0 and 1000, but the actual duration will be scaled up or down to fit the duration of (each) cell-dur. Cell-dur may be a constant, or it may itself be defined in a time-varying breakpoint file. The shape will then be stretched or compressed to to fit each (changing) cell-dur. In this way, you can create a pulsing envelope that accelerates, gets slower, or is randomised. Musical applications; ENVEL CYCLIC can be used to create an amplitude tremolo with a contour shape designed specifically for the task in hand." },
  arg1 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg2 = { name = "wsize", min = 0, max = 10000, def = 100, tip = "envelope window size in milliseconds, i.e., the resolution of the envelope generated" },
  arg3 = { name = "total-dur", min = 0, max = 30, def = 6, tip = "duration of the output soundfile file" },
  arg4 = { name = "cell-dur", min = 0, max = 30, input = "brk", def = 2, tip = "duration of (repeating) envelope-cell" },
  arg5 = { name = "phase", min = 0, max = 1, tip = "where in the cell-envelope to begin the output: 0 = start, 1 = end of cell" },
  arg6 = { name = "trough", min = 0, max = 1, input = "brk", tip = "the lowest point of the envelope (Range: 0 to 1)" },
  arg7 = { name = "env-exp", min = 0, max = 12, def = 0, tip = "shapes are linear (0) or exponential (< 1 = start slow and speed up: steep at top; > 1 = start quickly and slow down: steep at bottom)" },
}
 
dsp["Envel Cyclic - 3 Troughed (falls then rises) - Create a sequence of repeated envelopes, in a binary envelope file"] = {
  cmds = { exe = "envel", mode = "cyclic 3", tip = "3 Troughed (falls then rises). ENVEL CYCLIC creates a binary envelope file that cyclically repeats the shape that you give to it, at whatever repetition rate you want. It is therefore a way to rhythmicise a continuous sound by imposing a regularly pulsating envelope. The program is very flexible, allowing you to set the output duration, the duration of the repeating contour shapes (time-variable), phase (start point in the shape), type and depth of shape. The window-size (wsize) determines how accurately the output envelope is defined: a small window-size gives a very precisely defined envelope, a larger window-size gives a less well-defined envelope. Mode 4 accepts a (special) text breakpoint file with which you can define an envelope shape. What is special about this user-defined breakpoint file is that its time-units are relative. You the user define the pulse shape between times 0 and 1, or between times 0 and 1000, but the actual duration will be scaled up or down to fit the duration of (each) cell-dur. Cell-dur may be a constant, or it may itself be defined in a time-varying breakpoint file. The shape will then be stretched or compressed to to fit each (changing) cell-dur. In this way, you can create a pulsing envelope that accelerates, gets slower, or is randomised. Musical applications; ENVEL CYCLIC can be used to create an amplitude tremolo with a contour shape designed specifically for the task in hand." },
  arg1 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg2 = { name = "wsize", min = 0, max = 10000, def = 100, tip = "envelope window size in milliseconds, i.e., the resolution of the envelope generated" },
  arg3 = { name = "total-dur", min = 0, max = 30, def = 6, tip = "duration of the output soundfile file" },
  arg4 = { name = "cell-dur", min = 0, max = 30, input = "brk", def = 2, tip = "duration of (repeating) envelope-cell" },
  arg5 = { name = "phase", min = 0, max = 1, tip = "where in the cell-envelope to begin the output: 0 = start, 1 = end of cell" },
  arg6 = { name = "trough", min = 0, max = 1, input = "brk", tip = "the lowest point of the envelope (Range: 0 to 1)" },
  arg7 = { name = "env-exp", min = 0, max = 12, def = 0, tip = "shapes are linear (0) or exponential (< 1 = start slow and speed up: steep at top; > 1 = start quickly and slow down: steep at bottom)" },
}
 
dsp["Envel Cyclic - 4 User-defined (in a text breakpoint file) - Create a sequence of repeated envelopes, in a binary envelope file"] = {
  cmds = { exe = "envel", mode = "cyclic 4", tip = "4 User-defined (in a text breakpoint file). ENVEL CYCLIC creates a binary envelope file that cyclically repeats the shape that you give to it, at whatever repetition rate you want. It is therefore a way to rhythmicise a continuous sound by imposing a regularly pulsating envelope. The program is very flexible, allowing you to set the output duration, the duration of the repeating contour shapes (time-variable), phase (start point in the shape), type and depth of shape. The window-size (wsize) determines how accurately the output envelope is defined: a small window-size gives a very precisely defined envelope, a larger window-size gives a less well-defined envelope. Mode 4 accepts a (special) text breakpoint file with which you can define an envelope shape. What is special about this user-defined breakpoint file is that its time-units are relative. You the user define the pulse shape between times 0 and 1, or between times 0 and 1000, but the actual duration will be scaled up or down to fit the duration of (each) cell-dur. Cell-dur may be a constant, or it may itself be defined in a time-varying breakpoint file. The shape will then be stretched or compressed to to fit each (changing) cell-dur. In this way, you can create a pulsing envelope that accelerates, gets slower, or is randomised. Musical applications; ENVEL CYCLIC can be used to create an amplitude tremolo with a contour shape designed specifically for the task in hand." },
  arg1 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg2 = { name = "userenv", input = "txt", tip = "a user-defined textfile of time value pairs defining the envelope, in which the time units are arbitrary: the envelope is stretched to fit into each of the envelope cells." },
  arg3 = { name = "wsize", min = 0, max = 10000, def = 100, tip = "envelope window size in milliseconds, i.e., the resolution of the envelope generated" },
  arg4 = { name = "total-dur", min = 0, max = 30, def = 6, tip = "duration of the output soundfile file" },
  arg5 = { name = "cell-dur", min = 0, max = 30, input = "brk", def = 2, tip = "duration of (repeating) envelope-cell" },
  arg6 = { name = "phase", min = 0, max = 1, tip = "where in the cell-envelope to begin the output: 0 = start, 1 = end of cell" },
}
 
dsp["Envel Attack - 1 Set attack point where sound level first exceeds gate level. Emphasize the attack of a sound "] = {
  cmds = { exe = "envel", mode = "attack 1", channels = "any", tip = "1 Set attack point where sound level first exceeds gate level. Set attack point where sound level first exceeds gate level." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gate", min = 0, max = 1, tip = "level (threshold) to be reached for the attack point to be recognised (Range: 0 to 1). " },
  arg4 = { name = "Gain", min = 1, max = 32767, def = 1, tip = "floating point multiplier by which to amplify the signal at the attack point (< 1 reduces the signal; > 1 amplifies the signal)." },
  arg5 = { name = "onset", min = 5, max = 32767, tip = "attack onset duration in milliseconds, i.e., the 'sharpness' of the attack (Range:5ms to 32767ms). " },
  arg6 = { name = "decay", min = 5, max = length, tip = "attack decay duration in milliseconds (Range: 5 to < duration of infile)." },
  arg7 = { name = "envtype", switch = "-t", min = 0, max = 1, tip = " type of envelope: 0 linear, 1 exponential (Default)" },
}
 
dsp["Envel Attack - 2 Attack point at maximum level at your 'approx-time' (+/- 200 ms) Emphasize the attack of a sound 2"] = {
  cmds = { exe = "envel", mode = "attack 2", channels = "any", tip = "2 Attack point at maximum level at your 'approx-time' (+/- 200 ms). Attack point at maximum level at your 'approx-time' (+/- 200 ms)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000, tip = "time (approximate or exact) of the attack point, in seconds" },
  arg4 = { name = "Gain", min = 0.001, max = 2, },
  arg5 = { name = "onset", min = 5, max = 32767, tip = "attack onset duration in milliseconds, i.e., the 'sharpness' of the attack (Range:5ms to 32767ms)." },
  arg6 = { name = "decay", min = 5, max = length, tip = "attack decay duration in milliseconds (Range: 5 to < duration of infile)." },
  arg7 = { name = "envtype", switch = "-t", min = 0, max = 1, tip = " type of envelope: 0 linear, 1 exponential (Default)" },
}
 
dsp["Envel Attack - 3 Attack point at your 'exact-time' Emphasize the attack of a sound 3"] = {
  cmds = { exe = "envel", mode = "attack 3", channels = "any", tip = "3 Attack point at your 'exact-time'. Attack point at your 'exact-time'." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000, tip = "time (approximate or exact) of the attack point, in seconds" },
  arg4 = { name = "Gain", min = 1, max = 32767, def = 1, tip = "floating point multiplier by which to amplify the signal at the attack point (< 1 reduces the signal; > 1 amplifies the signal)." },
  arg5 = { name = "onset", min = 5, max = 32767, tip = "attack onset duration in milliseconds, i.e., the 'sharpness' of the attack (Range:5ms to 32767ms). " },
  arg6 = { name = "decay", min = 5, max = length, tip = "attack decay duration in milliseconds (Range: 5 to < duration of infile).The decay portion of the attack is also set in milliseconds. " },
  arg7 = { name = "envtype", switch = "-t", min = 0, max = 1, tip = " type of envelope: 0 linear, 1 exponential (Default)" },
}
 
dsp["Envel Attack - 4 Attack point at maximum level in the soundfile Emphasize the attack of a sound 4"] = {
  cmds = { exe = "envel", mode = "attack 4", channels = "any", tip = "4 Attack point at maximum level in the soundfile. Attack point at maximum level in the soundfile " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Gain", min = 1, max = 32767, def = 1, tip = "floating point multiplier by which to amplify the signal at the attack point (< 1 reduces the signal; > 1 amplifies the signal). " },
  arg4 = { name = "onset", min = 5, max = 32767, tip = "attack onset duration in milliseconds, i.e., the 'sharpness' of the attack (Range:5ms to 32767ms). " },
  arg5 = { name = "decay", min = 5, max = length, tip = "attack decay duration in milliseconds (Range: 5 to < duration of infile)." },
  arg6 = { name = "envtype", switch = "-t", min = 0, max = 1, tip = " type of envelope: 0 linear, 1 exponential (Default)" },
}
 
dsp["Envel Brktoenv - Convert a (text) breakpoint envelope to a binary envelope file  "] = {
  cmds = { exe = "envel", mode = "brktoenv", tip = "The text format is an editable time value text breakpoint file. The binary format is used by some processes of the system and cannot be edited. Musical applications; This is a utility to convert text format to binary format." },
  arg1 = { name = "Input.brk", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "wsize", min = 5, max = 1000, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of file)" },
}
 
dsp["Envel Create - 1 the output is a binary envelope file - Create an envelope "] = {
  cmds = { exe = "envel", mode = "create 1", tip = "1 the output is a binary envelope file. ENVEL CREATE provides a facility for hand-crafting an envelope shape. There can be as many 'points' as you like, and specific segments may be given an exponential curve, rather than a straight line shape, by preceding the value with an e. It may be useful to study the existing envelope of a soundfile in a soundfile viewer before creating your own envelope file, because it will need to relate to the existing values in order to produce predictable results. Virtually simultaneous jumps can be accomplished by making the next time increment the minimum 0.010 sec allowed (10ms). Windows 95 users may also use the graphic BRKEDIT program. Mode 2 may at first seem redundant because both the input and the output are text breakpoint files. The reason it is here is that the createfile may contain -e's for exponential curves. These are appropriately converted in the output textfile. Musical applications; Here are just a few ideas among the myriad possibilities: create very long, gradual changes in amplitude, create many ragged, jagged changes in amplitude, introduce regular, slow amplitude pulsations, pull down the amplitude after the attack transient, tailor the attack and/or decay transients " },
  arg1 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg2 = { name = "createfile", input = "txt", tip = "input text envelope data file, for the format see docs" },
  arg3 = { name = "wsize", min = 5, max = 1000, def = 5, tip = "enveloping window: time resolution in milliseconds over which to average emplitude data (Range: 5 to length of file)" },
}
 
dsp["Envel Create - 2 the output is a breakpoint text file - Create an envelope "] = {
  cmds = { exe = "envel", mode = "create 2", tip = "2 the output is a breakpoint text file. ENVEL CREATE provides a facility for hand-crafting an envelope shape. There can be as many 'points' as you like, and specific segments may be given an exponential curve, rather than a straight line shape, by preceding the value with an e. It may be useful to study the existing envelope of a soundfile in a soundfile viewer before creating your own envelope file, because it will need to relate to the existing values in order to produce predictable results. Virtually simultaneous jumps can be accomplished by making the next time increment the minimum 0.010 sec allowed (10ms). Windows 95 users may also use the graphic BRKEDIT program. Mode 2 may at first seem redundant because both the input and the output are text breakpoint files. The reason it is here is that the createfile may contain -e's for exponential curves. These are appropriately converted in the output textfile. Musical applications; Here are just a few ideas among the myriad possibilities: create very long, gradual changes in amplitude, create many ragged, jagged changes in amplitude, introduce regular, slow amplitude pulsations, pull down the amplitude after the attack transient, tailor the attack and/or decay transients " },
  arg1 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg2 = { name = "createfile", input = "txt", tip = "input text envelope data file, for the format see docs" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it "] = {
  cmds = { exe = "envel", mode = "curtail 1", channels = "any", tip = "specify the precise times at which the file is to begin and end." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "fadeend", min = 0, max = length/1000, },
  arg5 = { name = "envtype", min = 0, max = 1, tip = "type of envelope shape for the fade, 0 linear, 1 exponential" },
  arg6 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it 2"] = {
  cmds = { exe = "envel", mode = "curtail 2", channels = "any", tip = "specify the duration of the fade and when it begins." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "fade-dur", min = 0, max = length/1000, },
  arg5 = { name = "envtype", min = 0, max = 1, tip = "type of envelope shape for the fade, 0 linear, 1 exponential" },
  arg6 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it 3"] = {
  cmds = { exe = "envel", mode = "curtail 3", channels = "any", tip = "specify when the fade begins and fade to the existing end of insndfile" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "envtype", min = 0, max = 1, tip = "type of envelope shape for the fade, 0 linear, 1 exponential" },
  arg5 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it 4"] = {
  cmds = { exe = "envel", mode = "curtail 4", channels = "any", tip = "specify the precise times at which the file is to begin and end, and apply a doubly exponential (steeper) editing slope" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "fadeend", min = 0, max = length/1000, tip = "time at which to complete the fade" },
  arg5 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it 5"] = {
  cmds = { exe = "envel", mode = "curtail 5", channels = "any", tip = "specify the duration of the fade and when it begins, and apply a doubly exponential (steeper) editing slope" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "fade-dur", min = 0, max = length/1000, },
  arg5 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Curtail - Curtail a soundfile by fading to zero at some time within it 6"] = {
  cmds = { exe = "envel", mode = "curtail 6", channels = "any", tip = "Specify when the fade begins and fade to the existing end of insndfile, applying a doubly exponential (steeper) editing slope" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fadestart", min = 0, max = length/1000, tip = "time at which to begin the fade" },
  arg4 = { name = "times", switch = "-t", min = 0, max = 2, tip = "  identifies the type of time unit being used: 0 seconds, 1 samples, 2 groupes" },
}
 
dsp["Envel Dbtoenv - Convert a (text) breakpoint file with dB values to a binary envelope file  "] = {
  cmds = { exe = "envel", mode = "dbtoenv", tip = "This is a utility to convert breakpoint data in dB format to envelope data in binary format. The input breakpoint file contains time dB values and can be accessed with a text editor. The binary envelope file can only be handled by the software. The dB format is amplitude expressed along the logarithmic decibel scale. See the reference chart which relates these to gain and the 0 to 32767 scale. Musical applications; This is a utility to convert between dB and binary formats." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "wsize", min = 0, max = 10000, def = 5, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of file) " },
}
 
dsp["Envel Dbtogain - Convert a (text) breakpoint file with dB values to gain values (0 to 1) "] = {
  cmds = { exe = "envel", mode = "dbtogain", tip = "Both the input and the output files are breakpoint files, i.e., with time points and values for each of those time points. The times will remain the same, and the amplitude values will be converted. Both files can be accessed with a text editor. Gain is a multiplier operating between 0 and 1. See the reference chart which relates gain to dB and to the 0 to 32767 scale. Musical applications; This is a utility to convert between amplitude value formats." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
}
 
dsp["Envel Dovetail - Dovetail a soundfile by enveloping its beginning and end"] = {
  cmds = { exe = "envel", mode = "dovetail 1", channels = "any", tip = "a choice of linear or exponential slopes at the beginning and the end" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "infadedur", min = 0, max = length/1000, tip = "duration of the fade-in at the start of the file" },
  arg4 = { name = "outfadedur", min = 0, max = length/1000, tip = "duration of the fade-out at the end of the file" },
  arg5 = { name = "intype", min = 0, max = 1, tip = "envelope shape for the fade-in at the start of the file; intype = 0: linear fade, intype = 1: exponential fade (the Default)" },
  arg6 = { name = "outtype", min = 0, max = 1, tip = "envelope shape for the fade-out, at the end of the file; outtype = 0: linear fade, outtype = 1: exponential fade (the Default)" },
  arg7 = { name = "times", switch = "-t", min = 0, max = 2, tip = "identifies the type of time unit being used: times = 0: is seconds (the Default), times = 1: times are in samples, times = 2: times are in grouped-samples" },
}
 
dsp["Envel Dovetail - Dovetail a soundfile by enveloping its beginning and end 2"] = {
  cmds = { exe = "envel", mode = "dovetail 2", channels = "any", tip = "a doubly exponential slope (steeper) is applied to the beginning and the end" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "infadedur", min = 0, max = length/1000, tip = "duration of the fade-in at the start of the file" },
  arg4 = { name = "outfadedur", min = 0, max = length/1000, tip = "duration of the fade-out at the end of the file" },
  arg5 = { name = "times", switch = "-t", min = 0, max = 2, tip = "identifies the type of time unit being used: times = 0: is seconds (the Default), times = 1: times are in samples, times = 2: times are in grouped-samples" },
}
 
dsp["Envel Envtobrk - Convert a binary envelope file to a (text) breakpoint envelope file"] = {
  cmds = { exe = "envel", mode = "envtobrk", tip = "Converts from binary envelope to text breakpoint envelope data. The datareduce parameter enables you to reduce the amount of data contained in the file. A low value, such as 0.2 will keep most of the data, while a high value such as 0.8 will lose most of the data – while trying to retain the basic shape of the envelope. Musical applications; ENVEL ENVTOBRK makes it possible to gain access to binary envelope data so that it can be edited, printed, or used with another function which uses a breakpoint file as input (such as pitch transposition)." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "datareduce", switch = "-d", min = 0, max = 1, def = 1, tip = "weights in favour of the quantity or the accuracy of the data written to the breakpoint file (Range: 0 [quantity] to 1 [accuracy])" },
}
 
dsp["Envel Extract 1 - create binary env - Extract the amplitude envelope from an input soundfile "] = {
  cmds = { exe = "envel", mode = "extract 1", tip = "1 extract envelope, creating a binary envelope file. The envelope of a soundfile charts its rise and fall in amplitude over time. It is the overall, usually fairly coarse, shape we see in the standard time-amplitude waveform display of a soundfile. Only at the very smallest scale to we see the signal continually rising above and falling below the zero line, showing instantaneous change in the amplitude of the signal and defining its waveshape. The total span of this rise and fall from the peak above zero to the trough below zero tells us something about the loudness of the signal at that moment. If we chart how this total span changes over time, we get an idea of how the loudness of the sound varies over time. When we see a graphic display of a waveform at a coarser scale, the instantaneous up and down motion of the amplitude cannot be seen. What we do see is a kind of block representation of the signal, whose variation in width (top to bottom) gives an indication of the changing loudness, or envelope, of the sound. The fineness or degree of resolution of the amplitude envelope extracted from the sound or as defined in a breakpoint file is determined by the windowsize parameter. This is the amount of time between measurements of the waveform's amplitude. The minimum value here is 5 milliseconds, i.e., every 200th of a second. Thus, at a sample rate of 44100 per second, an amplitude value is registered every 220.5 samples – and whatever changes in amplitude happen between these points are not registered. A resolution of 5 milliseconds is considered to be rather 'fine' and even coarser values can be adequate. It depends on the nature of the sound, and this is what you must consider when you set the windowsize. If you have a sound that tremolos through a crescendo, for example, a small windowsize will track the undulations of the tremolo as well as the crescendo. A larger windowsize will see the crescendo but ignore the tremolo fluctuations. Which windowsize you choose will depend on what musical information you want to extract from the source sound. The datareduce parameter enables you to reduce the amount of data contained in the file. A low value, such as 0.2 will keep most of the data, while a high value such as 0.8 will lose most of the data – while trying to retain the basic shape of the envelope. Musical applications; Extracting the envelope is a preliminary step for several operations, such as imposing it on a different soundfile (ENVEL IMPOSE) or using it as a breakpoint input for the parameter of some other function altogether. In this case, you may find you have too much data, so you may need to extract the envelope again, using a datareduce value – or go straight to BRKEDIT (if you have a Windows 95 system), where there is a Reduce option. BRKEDIT can also be useful to alter the vertical range of the breakpoint file to suit a given parameter, or to adjust the overall duration of the breakpoint file to match the length of a different sound. Re-using envelope data in a variety of contexts is one of the key tools for creating unity by means of aural cross-references, sometimes overt and sometimes only very subtly present to the ear." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "wsize", min = 0, max = 10000, def = 5, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of file) " },
}
 
dsp["Envel Extract 2 - create text brk - Extract the amplitude envelope from an input soundfile "] = {
  cmds = { exe = "envel", mode = "extract 2", tip = "2 extract envelope, creating a (text) breakpoint file. The envelope of a soundfile charts its rise and fall in amplitude over time. It is the overall, usually fairly coarse, shape we see in the standard time-amplitude waveform display of a soundfile. Only at the very smallest scale to we see the signal continually rising above and falling below the zero line, showing instantaneous change in the amplitude of the signal and defining its waveshape. The total span of this rise and fall from the peak above zero to the trough below zero tells us something about the loudness of the signal at that moment. If we chart how this total span changes over time, we get an idea of how the loudness of the sound varies over time. When we see a graphic display of a waveform at a coarser scale, the instantaneous up and down motion of the amplitude cannot be seen. What we do see is a kind of block representation of the signal, whose variation in width (top to bottom) gives an indication of the changing loudness, or envelope, of the sound. The fineness or degree of resolution of the amplitude envelope extracted from the sound or as defined in a breakpoint file is determined by the windowsize parameter. This is the amount of time between measurements of the waveform's amplitude. The minimum value here is 5 milliseconds, i.e., every 200th of a second. Thus, at a sample rate of 44100 per second, an amplitude value is registered every 220.5 samples – and whatever changes in amplitude happen between these points are not registered. A resolution of 5 milliseconds is considered to be rather 'fine' and even coarser values can be adequate. It depends on the nature of the sound, and this is what you must consider when you set the windowsize. If you have a sound that tremolos through a crescendo, for example, a small windowsize will track the undulations of the tremolo as well as the crescendo. A larger windowsize will see the crescendo but ignore the tremolo fluctuations. Which windowsize you choose will depend on what musical information you want to extract from the source sound. The datareduce parameter enables you to reduce the amount of data contained in the file. A low value, such as 0.2 will keep most of the data, while a high value such as 0.8 will lose most of the data – while trying to retain the basic shape of the envelope. Musical applications; Extracting the envelope is a preliminary step for several operations, such as imposing it on a different soundfile (ENVEL IMPOSE) or using it as a breakpoint input for the parameter of some other function altogether. In this case, you may find you have too much data, so you may need to extract the envelope again, using a datareduce value – or go straight to BRKEDIT (if you have a Windows 95 system), where there is a Reduce option. BRKEDIT can also be useful to alter the vertical range of the breakpoint file to suit a given parameter, or to adjust the overall duration of the breakpoint file to match the length of a different sound. Re-using envelope data in a variety of contexts is one of the key tools for creating unity by means of aural cross-references, sometimes overt and sometimes only very subtly present to the ear." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "wsize", min = 0, max = 10000, def = 5, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of file) " },
  arg4 = { name = "datareduce", switch = "-d", min = 0, max = 1, def = 1, tip = "weights in favour of the quantity or the accuracy of the data written to the breakpoint file (Range: 0 [quantity] to 1 [accuracy])" },
}
 
dsp["Envel Gaintodb - Convert a (text) breakpoint file with gain (0 to 1) values to dB values"] = {
  cmds = { exe = "envel", mode = "gaintodb", tip = "ENVEL GAINTODB makes the conversion between these two text data formats, from gain to dB. Musical applications; The conversion may be made for educational purposes, or to provide values relevant for expressing amplitude in dB in mixfiles." },
  arg1 = { name = "Input.txt", input = "txt", tip = "input time gain envelope text breakpoint file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "output time dB envelope text breakpoint file" },
}
 
dsp["Envel Impose - 1 Impose an envelope on an input soundfile - Extract an envelope from another soundfile and impose it on input_sndfile "] = {
  cmds = { exe = "envel", mode = "impose 1", tip = "Extract an envelope from another soundfile and impose it on input_sndfile - This is the matching function for ENVEL EXTRACT, which extracts an envelope from a soundfile, saving it as a binary (Mode 1) or breakpoint (Mode 2) file. For convenience, ENVEL IMPOSE can extract an envelope directly from a soundfile (Mode 1) as well as imposing envelopes from existing binary or breakpoint files. Musical applications; Mode 3 can be very useful when you would like to fine-tune the envelope of a soundfile with a breakpoint envelope file you have written yourself, e.g., to soften or sharpen an attack, or some other portion of the soundfile. Sometimes you may need a customised result which the other more generalised envelope shaping functions (such as DOVETAIL) do not provide. Another key application is to impose a changing envelope onto a soundfile which has a fairly steady-state envelope. In this way, for example, a bubbling stream of water can be made to pulse to the rhythm of spoken words. Similarly, a given sound can acquire the amplitude shape of some other specific sound, thus blending features of the two sounds. This is all part of creating relationships among sonic data. Note that this deals with amplitude only. It is quite a different matter to extract and impose timbral envelopes, because then the tonal characteristics are moved. See the Formants Group in the spectral dimension. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "wsize", min = 5, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of imposed_sndfile)" },
}
 
dsp["Envel Impose - 2 Impose an envelope from a binary envelope file - Impose an envelope on an input soundfile "] = {
  cmds = { exe = "envel", mode = "impose 2", tip = "2 Impose an envelope from a binary envelope file. This is the matching function for ENVEL EXTRACT, which extracts an envelope from a soundfile, saving it as a binary (Mode 1) or breakpoint (Mode 2) file. For convenience, ENVEL IMPOSE can extract an envelope directly from a soundfile (Mode 1) as well as imposing envelopes from existing binary or breakpoint files. Musical applications; Mode 3 can be very useful when you would like to fine-tune the envelope of a soundfile with a breakpoint envelope file you have written yourself, e.g., to soften or sharpen an attack, or some other portion of the soundfile. Sometimes you may need a customised result which the other more generalised envelope shaping functions (such as DOVETAIL) do not provide. Another key application is to impose a changing envelope onto a soundfile which has a fairly steady-state envelope. In this way, for example, a bubbling stream of water can be made to pulse to the rhythm of spoken words. Similarly, a given sound can acquire the amplitude shape of some other specific sound, thus blending features of the two sounds. This is all part of creating relationships among sonic data. Note that this deals with amplitude only. It is quite a different matter to extract and impose timbral envelopes, because then the tonal characteristics are moved. See the Formants Group in the spectral dimension. Also see ENVEL EXTRACT, ENVEL REPLACE, ENVEL REPLOT, ENVEL RESHAPE and ENVEL WARP." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.env", input = "env", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Envel Impose - 3 Impose an envelope from a (text) breakpoint file - Impose an envelope on an input soundfile "] = {
  cmds = { exe = "envel", mode = "impose 3", tip = "3 Impose an envelope from a (text) breakpoint file. This is the matching function for ENVEL EXTRACT, which extracts an envelope from a soundfile, saving it as a binary (Mode 1) or breakpoint (Mode 2) file. For convenience, ENVEL IMPOSE can extract an envelope directly from a soundfile (Mode 1) as well as imposing envelopes from existing binary or breakpoint files. Musical applications; Mode 3 can be very useful when you would like to fine-tune the envelope of a soundfile with a breakpoint envelope file you have written yourself, e.g., to soften or sharpen an attack, or some other portion of the soundfile. Sometimes you may need a customised result which the other more generalised envelope shaping functions (such as DOVETAIL) do not provide. Another key application is to impose a changing envelope onto a soundfile which has a fairly steady-state envelope. In this way, for example, a bubbling stream of water can be made to pulse to the rhythm of spoken words. Similarly, a given sound can acquire the amplitude shape of some other specific sound, thus blending features of the two sounds. This is all part of creating relationships among sonic data. Note that this deals with amplitude only. It is quite a different matter to extract and impose timbral envelopes, because then the tonal characteristics are moved. See the Formants Group in the spectral dimension. Also see ENVEL EXTRACT, ENVEL REPLACE, ENVEL REPLOT, ENVEL RESHAPE and ENVEL WARP." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.txt", input = "txt", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Envel Impose - 4 Impose an envelope from a (text) breakpoint file with dB values - Impose an envelope on an input soundfile "] = {
  cmds = { exe = "envel", mode = "impose 4", tip = "4 Impose an envelope from a (text) breakpoint file with dB values. This is the matching function for ENVEL EXTRACT, which extracts an envelope from a soundfile, saving it as a binary (Mode 1) or breakpoint (Mode 2) file. For convenience, ENVEL IMPOSE can extract an envelope directly from a soundfile (Mode 1) as well as imposing envelopes from existing binary or breakpoint files. Musical applications; Mode 3 can be very useful when you would like to fine-tune the envelope of a soundfile with a breakpoint envelope file you have written yourself, e.g., to soften or sharpen an attack, or some other portion of the soundfile. Sometimes you may need a customised result which the other more generalised envelope shaping functions (such as DOVETAIL) do not provide. Another key application is to impose a changing envelope onto a soundfile which has a fairly steady-state envelope. In this way, for example, a bubbling stream of water can be made to pulse to the rhythm of spoken words. Similarly, a given sound can acquire the amplitude shape of some other specific sound, thus blending features of the two sounds. This is all part of creating relationships among sonic data. Note that this deals with amplitude only. It is quite a different matter to extract and impose timbral envelopes, because then the tonal characteristics are moved. See the Formants Group in the spectral dimension. Also see ENVEL EXTRACT, ENVEL REPLACE, ENVEL REPLOT, ENVEL RESHAPE and ENVEL WARP." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.txt", input = "txt", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Envel Pluck - Pluck the start of a sound"] = {
  cmds = { exe = "envel", mode = "pluck", tip = "Although similar to ENVEL ATTACK, ENVEL PLUCK differs in that the attack imposed on the soundfile also has the noise and decay characteristics associated with plucked sounds. To pluck the sound, ENVEL PLUCK creates a 'pluck' constructed from a number of wavecycles and splices it into the beginning of the sound. To achieve this, it needs a goal wavecycle to pluck towards: this is located at startsamp.We can get an idea of what is going on here by reviewing how the Karplus-Strong pluck algorithm used in Csound works. It achieves a pluck effect by iteratively filtering a block of noise until it becomes pitched. It does this because noise components are an essential ingredient of a plucked sound. ENVEL PLUCK achieves the same effect by working backwards – backwards because it is starting with an existing sound. The process is to repeat wavelen backwards a number of times towards the beginning of the soundfile, becoming noisier as it goes." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "startsamp", min = 0, max = length/1000, tip = "is where the pluck effect links into the sound. It will usually be a little after the beginning of the sound, not at the very beginning, because we want the pluck to link smoothly into stable signal." },
  arg4 = { name = "wavelen", min = 4, max = 2205, tip = "the number of (absolute) samples in the wavelength of the pluck effect; its value is the number of samples in a waveform starting at startsamp in the infile " },
  arg5 = { name = "atkcycles", switch = "-a", min = 2, max = 32767, def = 32, tip = "the number of wavecycles in the pluck attack" },
  arg6 = { name = "decayrate", switch = "-d", min = 1, max = 64, def = 48, tip = "rate of decay of the pluck effect" },
}
 
dsp["Envel Replace 1 - Replaces envelope with a new one extracted from another soundfile "] = {
  cmds = { exe = "envel", mode = "replace 1", tip = "Replaces envelope with a new one extracted from another soundfile. With ENVEL IMPOSE, the imposed envelope is imposed on top of any existing envelope. The result is the combination of the imposed envelope and the existing envelope. In contrast, ENVEL REPLACE attempts to replace the existing envelope of the sound with the newly sampled envelope. (To do this it must start by flattening out the original envelope of the sound.) Care! If there are (near) zero amplitudes in the original sound, it will not be possible to replace these meaningfully. Musical applications; The author observes that Mode 1 is useful for 'restoring the amplitude contour of a sound after filtering with time-varying Q'. In this case, the amplitude envelope of the input_sndfile is extracted before applying time-varying Q. It will then be 'in storage', available, in this case, to be reapplied to the filtered sound in order to restore the original amplitude contour." },
  arg1 = { name = "input_sndfile", input = "wav", tip = "input soundfile on which to impose the envelope" },
  arg2 = { name = "replacing_sndfile", input = "wav", tip = "soundfile from which to extract the amplitude envelope" },
  arg3 = { name = "outsndfile", output = "wav", tip = "output soundfile with new amplitude envelope" },
  arg4 = { name = "wsize", min = 5, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of imposed_sndfile)" },
}

dsp["Envel Replace 2 - Replaces envelope with a new one from a binary envelope file"] = {
  cmds = { exe = "envel", mode = "replace 2", tip = "2 Replaces envelope with a new one from a binary envelope file. With ENVEL IMPOSE, the imposed envelope is imposed on top of any existing envelope. The result is the combination of the imposed envelope and the existing envelope. In contrast, ENVEL REPLACE attempts to replace the existing envelope of the sound with the newly sampled envelope. (To do this it must start by flattening out the original envelope of the sound.) Care! If there are (near) zero amplitudes in the original sound, it will not be possible to replace these meaningfully. Musical applications; The author observes that Mode 1 is useful for 'restoring the amplitude contour of a sound after filtering with time-varying Q'. In this case, the amplitude envelope of the input_sndfile is extracted before applying time-varying Q. It will then be 'in storage', available, in this case, to be reapplied to the filtered sound in order to restore the original amplitude contour." },
  arg1 = { name = "input_sndfile", input = "wav", tip = "input soundfile on which to impose the envelope" },
  arg2 = { name = "replacing_envfile", input = "env", tip = "envelope to impose is contained in a binary envelope file" },
  arg3 = { name = "outsndfile", output = "wav", tip = "output soundfile with new amplitude envelope" },
}

dsp["Envel Replace 3 - Replaces envelope with a new one from a (text) breakpoint file"] = {
  cmds = { exe = "envel", mode = "replace 3", tip = "3 Replaces envelope with a new one from a (text) breakpoint file. With ENVEL IMPOSE, the imposed envelope is imposed on top of any existing envelope. The result is the combination of the imposed envelope and the existing envelope. In contrast, ENVEL REPLACE attempts to replace the existing envelope of the sound with the newly sampled envelope. (To do this it must start by flattening out the original envelope of the sound.) Care! If there are (near) zero amplitudes in the original sound, it will not be possible to replace these meaningfully. Musical applications; The author observes that Mode 1 is useful for 'restoring the amplitude contour of a sound after filtering with time-varying Q'. In this case, the amplitude envelope of the input_sndfile is extracted before applying time-varying Q. It will then be 'in storage', available, in this case, to be reapplied to the filtered sound in order to restore the original amplitude contour." },
  arg1 = { name = "input_sndfile", input = "wav", tip = "input soundfile on which to impose the envelope" },
  arg2 = { name = "brkfile", min = 0, max = 0, def = 0, input = "brk", tip = "envelope to impose is contained in a (text) time value breakpoint file (Range of value is 0 to 1)" },
  arg3 = { name = "outsndfile", output = "wav", tip = "output soundfile with new amplitude envelope" },
  arg4 = { name = "wsize", min = 5, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of imposed_sndfile)" },
}

dsp["Envel Replace 4 - Replaces envelope with a new one from a (text) breakpoint file with dB values"] = {
  cmds = { exe = "envel", mode = "replace 4", tip = "4 Replaces envelope with a new one from a (text) breakpoint file with dB values. With ENVEL IMPOSE, the imposed envelope is imposed on top of any existing envelope. The result is the combination of the imposed envelope and the existing envelope. In contrast, ENVEL REPLACE attempts to replace the existing envelope of the sound with the newly sampled envelope. (To do this it must start by flattening out the original envelope of the sound.) Care! If there are (near) zero amplitudes in the original sound, it will not be possible to replace these meaningfully. Musical applications; The author observes that Mode 1 is useful for 'restoring the amplitude contour of a sound after filtering with time-varying Q'. In this case, the amplitude envelope of the input_sndfile is extracted before applying time-varying Q. It will then be 'in storage', available, in this case, to be reapplied to the filtered sound in order to restore the original amplitude contour." },
  arg1 = { name = "input_sndfile", input = "wav", tip = "input soundfile on which to impose the envelope" },
  arg2 = { name = "brkfile_dB", input = "txt", tip = "envelope to impose is contained in a (text) time dB_value breakpoint file (Range of dB_value is -96 to 0)" },
  arg3 = { name = "outsndfile", output = "wav", tip = "output soundfile with new amplitude envelope" },
  arg4 = { name = "wsize", min = 5, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data (Range: 5 to length of imposed_sndfile)" },
}
 
dsp["Envel Replot - 1 NORMALISE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 1", tip = "1 NORMALISE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 10 LIMIT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 10", tip = "10 LIMIT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "limit", min = 0, max = 1, input = "brk", def = 0, tip = " upper limit of range; amplitude values which were above limit are scaled down such that 'maxamp' becomes limit. Range: > threshold to 1." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", def = 1, tip = " lower limit of range; only values above threshold are affected. Range: 0 to limit." },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 11 CORRUGATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 11", tip = "11 CORRUGATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "trofdel", min = 1, max = 32767, input = "brk", def = 1, tip = "number of windows to set to zero per trough. Range: 1 to < peak_separation." },
  arg5 = { name = "peak_separation", min = 2, max = 32767, input = "brk", def = 2, tip = " minimum number of windows per peak. Range: 2 to 32767." },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 12 EXPAND -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 12", tip = "12 EXPAND. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "levels below gate are set to 0. Range: 0 to threshold." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", def = 0, tip = "lower limit of range: amplitude values that were above gate and below threshold are scaled up such that the minimum level becomes threshold. Range: gate to 1." },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 13 TRIGGER -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 13", tip = "13 TRIGGER. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "rampfile", min = 0, max = 0, input = "brk", def = 0, tip = "your new breakpoint file for the (triggered) bursts" },
  arg4 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg5 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "the average level must be above gate before triggering can occur. Range: 0 to 1." },
  arg6 = { name = "rise", min = 0, max = 1, def = 0, tip = "minimum loudness-step before triggering can occur. Range: 0 to 1." },
  arg7 = { name = "dur", min = 0, max = 30, def = 1, tip = "maximum duration over which the rise can take place before triggering can occur. Range: must be >= the envelope window duration." },
  arg8 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 14 CEILING -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 14", tip = "14 CEILING. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 15 DUCKED -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 15", tip = "15 DUCKED. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "highest level allowed for the envelope. Range: 0 to 1." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", def = 0, tip = "amplitude level the envelope must reach before 'ducking' is applied. Range: 0 to 1." },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 2 REVERSE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 2", tip = "2 REVERSE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 3 EXAGGERATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 3", tip = "3 EXAGGERATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "exaggerate", min = 0, max = 1000, input = "brk", def = 0, tip = "Exaggerate the envelope contour." },
  arg5 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 4 ATTENUATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 4", tip = "4 ATTENUATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "attenuation", min = 0, max = 1, input = "brk", def = 0, tip = "multiplier by which to scale down the envelope. Range: 0 to 1" },
  arg5 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 5 LIFT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 5", tip = "5 LIFT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "lift", min = 0, max = 1, input = "brk", def = 0, tip = "amount to add to each amplitude value. Range: 0 to 1" },
  arg5 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 6 TIMESTRETCH -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 6", tip = "6 TIMESTRETCH. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "timestretch", min = 0, max = 100, def = 2, tip = "multiplier by which to stretch the envelope. Range: > 0.0" },
  arg5 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 7 FLATTEN -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 7", tip = "7 FLATTEN. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "flatten", min = 1, max = 5000, input = "brk", def = 1, tip = "number of envelope values over which to average. Range: 1 to 5000." },
  arg5 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 8 GATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 8", tip = "8 GATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "levels less than gate are set to 0. Range: 0 to 1." },
  arg5 = { name = "smoothing", min = 0, max = 32767, def = 1, tip = "excises segments with an amplitude less than smoothing. Range: 0 to 32767" },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Replot - 9 INVERT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "replot 9", tip = "9 INVERT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.txt", input = "txt", tip = "Select an input .txt file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "wsize", min = 0, max = length, tip = "enveloping window: time resolution in milliseconds over which to average amplitude data" },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", def = 0, tip = " levels less than gate are set to 0. Range: 0 to < mirror." },
  arg5 = { name = "mirror", min = 0, max = 1, input = "brk", def = 1, tip = "reflection point: all values other than those below gate, both above and below mirror, are inverted to the other side of mirror. Range: > gate to 1." },
  arg6 = { name = "reduce", switch = "-d", min = 0, max = 1, def = 0.5, tip = "forces interpolation of data in the breakpoint output file to reduce unnecessary data output. Range: 0 to 1." },
}
 
dsp["Envel Reshape - 1 NORMALISE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 1", tip = "1 NORMALISE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
}
 
dsp["Envel Reshape - 10 LIMIT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 10", tip = "10 LIMIT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "limit", min = 0, max = 1, input = "brk", def = 0, tip = " upper limit of range; amplitude values which were above limit are scaled down such that 'maxamp' becomes limit. Range: > threshold to 1." },
  arg4 = { name = "threshold", min = 0, max = 1, input = "brk", def = 1, tip = " lower limit of range; only values above threshold are affected. Range: 0 to limit." },
}
 
dsp["Envel Reshape - 11 CORRUGATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 11", tip = "11 CORRUGATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "trofdel", min = 1, max = 32767, input = "brk", def = 1, tip = "number of windows to set to zero per trough. Range: 1 to < peak_separation." },
  arg4 = { name = "peak_separation", min = 2, max = 32767, input = "brk", def = 2, tip = " minimum number of windows per peak. Range: 2 to 32767." },
}
 
dsp["Envel Reshape - 12 EXPAND -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 12", tip = "12 EXPAND. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "levels below gate are set to 0. Range: 0 to threshold." },
  arg4 = { name = "threshold", min = 0, max = 1, input = "brk", def = 0, tip = "lower limit of range: amplitude values that were above gate and below threshold are scaled up such that the minimum level becomes threshold. Range: gate to 1." },
}
 
dsp["Envel Reshape - 13 TRIGGER -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 13", tip = "13 TRIGGER. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "rampfile", min = 0, max = 0, input = "brk", def = 0, tip = "your new breakpoint file for the (triggered) bursts" },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "the average level must be above gate before triggering can occur. Range: 0 to 1." },
  arg5 = { name = "rise", min = 0, max = 1, def = 0, tip = "minimum loudness-step before triggering can occur. Range: 0 to 1." },
  arg6 = { name = "dur", min = 0, max = 30, def = 1, tip = "maximum duration over which the rise can take place before triggering can occur. Range: must be >= the envelope window duration." },
}
 
dsp["Envel Reshape - 14 CEILING -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 14", tip = "14 CEILING. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
}
 
dsp["Envel Reshape - 15 DUCKED -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 15", tip = "15 DUCKED. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "highest level allowed for the envelope. Range: 0 to 1." },
  arg4 = { name = "threshold", min = 0, max = 1, input = "brk", def = 0, tip = "amplitude level the envelope must reach before 'ducking' is applied. Range: 0 to 1." },
}
 
dsp["Envel Reshape - 2 REVERSE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 2", tip = "2 REVERSE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
}
 
dsp["Envel Reshape - 3 EXAGGERATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 3", tip = "3 EXAGGERATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "exaggerate", min = 0, max = 1000, input = "brk", def = 0, tip = "Exaggerate the envelope contour." },
}
 
dsp["Envel Reshape - 4 ATTENUATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 4", tip = "4 ATTENUATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "attenuation", min = 0, max = 1, input = "brk", def = 0, tip = "multiplier by which to scale down the envelope. Range: 0 to 1" },
}
 
dsp["Envel Reshape - 5 LIFT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 5", tip = "5 LIFT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "lift", min = 0, max = 1, input = "brk", def = 0, tip = "amount to add to each amplitude value. Range: 0 to 1" },
}
 
dsp["Envel Reshape - 6 TIMESTRETCH -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 6", tip = "6 TIMESTRETCH. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "timestretch", min = 0, max = 100, def = 2, tip = "multiplier by which to stretch the envelope. Range: > 0.0" },
}
 
dsp["Envel Reshape - 7 FLATTEN -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 7", tip = "7 FLATTEN. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "flatten", min = 1, max = 5000, input = "brk", def = 1, tip = "number of envelope values over which to average. Range: 1 to 5000." },
}
 
dsp["Envel Reshape - 8 GATE -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 8", tip = "8 GATE. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "gate", min = 0, max = 1, input = "brk", def = 0.5, tip = "levels less than gate are set to 0. Range: 0 to 1." },
  arg4 = { name = "smoothing", min = 0, max = 32767, def = 1, tip = "excises segments with an amplitude less than smoothing. Range: 0 to 32767" },
}
 
dsp["Envel Reshape - 9 INVERT -  Warp the envelope in a (text) breakpoint envelope file "] = {
  cmds = { exe = "envel", mode = "reshape 9", tip = "9 INVERT. The envelope shaping routines for these three functions are identical. Only the output to which the changes are applied differs. Movement in time creates shapes. Amplitude change plays a key role in performance articulation and in the identification of sounds. When we hear the amplitude 'grow' as a soprano sings a glorious long tone, we are aware of its amplitude 'envelope'. When we hear the sharp 'report' of a snare drum rimshot, we are aware of the 'attack transient' of the sound: i.e., its very rapid rise in amplitude. Voices rise and fall in both pitch and amplitude. ENVEL WARP is a way of working with the shape produced by the envelope over the course of its whole duration. If we think of it as made of rubber, we can imagine it being pulled and stretched and squeezed. Musical applications; Amplitude envelope is an essential part of musical 'articulation' and has a great bearing on emotional content. These functions can be used to increase contrasts and make the music 'vivid', or to reduce the contrasts and 'tone down' the emotional effect. On the other hand, the amplitude shape can be handled as an abstract form which can be extracted from a sound, remoulded in some way, and then applied to another sound or to some other musical data. This introduces a point of connection between different parts of the musical material. For example, we might hear an amplitude shape in the form of a series of vocal bursts. Then this same/similar shape may become audible in the form of a pitch transient (pitch glissando). The musical principle is 'sameness and difference': creating unity amidst disparate material by introducing elements of sameness." },
  arg1 = { name = "Input.env", input = "env", tip = "Select an input .env file" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
  arg3 = { name = "gate", min = 0, max = 1, input = "brk", def = 0, tip = " levels less than gate are set to 0. Range: 0 to < mirror." },
  arg4 = { name = "mirror", min = 0, max = 1, input = "brk", def = 1, tip = "reflection point: all values other than those below gate, both above and below mirror, are inverted to the other side of mirror. Range: > gate to 1." },
}
 
dsp["Envel Scaled - Impose an envelope on an input soundfile, scaling it timewise to the sound's duration"] = {
  cmds = { exe = "envel", mode = "scaled", tip = "The main point here is that the times in the breakpoint file do not need to match the length of the input soundfile. The breakpoint file is therefore generic: whatever its times, this function will scale it to fit the length of the input sound. Musical applications; Because the imposed_breakpoint file needs to be a text file, you can use ENVTOBRK to convert a binary envelope file extracted from a sound to a text breakpoint file. This function can be convenient when you want to apply existing envelope files to a given sound. Sometimes it is useful to apply a process to many sounds, slightly modifying that process to suit the different durations of the source files. This is especially useful when using BULK PROCESSING on the Sound Loom, so that new data files do not need to be written for every single input source. For example, a simple fade in, fade-out envelope could be applied to hundreds of sounds of slightly different duration, at a single BULK PROCESSING pass, using this process." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.txt", input = "txt", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Envel Swell - Cause sound to fade in to and out from a peak moment"] = {
  cmds = { exe = "envel", mode = "swell", channels = "any", tip = "    his is a generic envelope type which can be applied very easily by specifying the time of the peak moment and the shape to be applied.The peak moment (peaktime) can be the location of the maximum amplitude in the soundfile, which can be found with SNDINFO MAXSAMP). It could also be any other place in the file that you define. For example, you can take a completely flat sound (in terms of loudness) and make it swell to a peak (at any point you want) and die away again. The linear peaktype produces a less gradual fade. This function makes it easy to create an audible peak anywhere in a soundfile, and implement a swell to and from it without having to create a breakpoint file." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "peaktime", min = 0, max = length/1000, tip = "time in infile at which the peak moment is located." },
  arg4 = { name = "peaktype", min = 0, max = 1, tip = "type of swell shape: 0 (linear) or 1 (exponential – the Default)" },
}
 
dsp["Envel Tremolo - Tremolo a sound linearly between frequencies"] = {
  cmds = { exe = "envel", mode = "tremolo 1", channels = "any", tip = "Tremolo is a pulsation in the amplitude of a sound. The frequency of the tremolo is the rate of the amplitude pulsation. ('Vibrato' is a frequency displacement.) This technique is familiar to us as a feature of various singing styles, and also as a control available on synthesisers. It can be used to enliven or enrich a sound, or to create more marked flutter effects, such as the 'flutter-tongue' technique on a wind instrument" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 0, max = 500, input = "brk", def = 3, tip = "frequency of the tremolo (a low frequency oscillation: Range: 0 to 500)" },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", tip = "frequency displacement of the oscillation (Range: 0 to 1; the Default is 0.250)" },
  arg5 = { name = "gain", min = 0, max = 1, input = "brk", tip = "the overall signal gain, or envelope (Range 0 to 1; the Default is 1)" },
}
 
dsp["Envel Tremolo - Tremolo a sound logarithmically"] = {
  cmds = { exe = "envel", mode = "tremolo 2", channels = "any", tip = "Tremolo is a pulsation in the amplitude of a sound. The frequency of the tremolo is the rate of the amplitude pulsation. ('Vibrato' is a frequency displacement.) This technique is familiar to us as a feature of various singing styles, and also as a control available on synthesisers. It can be used to enliven or enrich a sound, or to create more marked flutter effects, such as the 'flutter-tongue' technique on a wind instrument" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 0, max = 500, input = "brk", def = 3, tip = "frequency of the tremolo (a low frequency oscillation: Range: 0 to 500)" },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", tip = "frequency displacement of the oscillation (Range: 0 to 1; the Default is 0.250)" },
  arg5 = { name = "gain", min = 0, max = 1, input = "brk", tip = "the overall signal gain, or envelope (Range 0 to 1; the Default is 1)" },
}
 
dsp["Envel Warp - Attenuate the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 4", channels = "any", tip = "Reduce the amplitude of an envelope." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "attenuation", min = 0, max = 1, input = "brk", tip = "multiplier by which to scale down the envelope. Range: 0 to 1" },
}
 
dsp["Envel Warp - Ceiling of the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 14", channels = "any", tip = "Force envelope up to its maximum level, everywhere." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 30, tip = "the duration of the enveloping window, in milliseconds " },
}
 
dsp["Envel Warp - Corrugate the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 11", channels = "any", tip = "Tighten the peaks into close-set ridges by taking troughs (downward dips) in the envelope to zero." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "trofdel", min = 0, max = 32767, input = "brk", tip = "number of windows to set to zero per trough. Range: 1 to < peak_separation." },
  arg5 = { name = "peak_separation", min = 2, max = 32767, input = "brk", tip = "minimum number of windows per peak. Range: 2 to 32767." },
}
 
dsp["Envel Warp - Duck the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 15", channels = "any", tip = "Create 'ducking' envelope: envelope kept below a set level." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", tip = "highest level allowed for the envelope. Range: 0 to 1." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", tip = "Amplitude level the envelope must reach before 'ducking' is applied. Range: 0 to 1." },
}
 
dsp["Envel Warp - Exaggerate the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 3", channels = "any", tip = "Exaggerate the envelope contour" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "exaggerate", min = 0, max = 50, input = "brk", tip = "Range: > 0.0 < 1, low values are boosted = 1, no change, > 1, high values are boosted" },
}
 
dsp["Envel Warp - Expand the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 12", channels = "any", tip = "Inflate the envelope" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", tip = "levels below gate are set to 0. Range: 0 to threshold." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", tip = "lower limit of range: amplitude values that were above gate and below threshold are scaled up such that the minimum level becomes threshold. Range: gate to 1" },
  arg6 = { name = "smoothing", min = 0, max = 32767, tip = "Excises low-level segments which are less than smoothing windows in length. Range: 0 to 32767. 0 turns off the smoothing effect." },
}
 
dsp["Envel Warp - Flatten the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 7", channels = "any", tip = "Even out the envelope contour." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "flatten", min = 1, max = 5000, input = "brk", tip = "number of envelope values over which to average." },
}
 
dsp["Envel Warp - Gate the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 8", channels = "any", tip = "Zero lower portion of envelope." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", tip = "levels less than gate are set to 0. Range: 0 to 1." },
  arg5 = { name = "smoothing", min = 0, max = 32767, tip = "smoothing – excises segments with an amplitude less than smoothing. Range: 0 to 32767." },
}
 
dsp["Envel Warp - Invert the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 9", channels = "any", tip = "Turn envelope upside down: negative values become positive, and positive values become negative." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", tip = "levels less than gate are set to 0. Range: 0 to < mirror." },
  arg5 = { name = "mirror", min = 0, max = 1, input = "brk", tip = "reflection point: all values other than those below gate, both above and below mirror, are inverted to the other side of mirror. Range: > gate to 1." },
}
 
dsp["Envel Warp - Lift the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 5", channels = "any", tip = "Raise the envelope by a fixed amount" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "lift", min = 0, max = 1, input = "brk", tip = "amount to add to each amplitude value. Range: 0 to 1" },
}
 
dsp["Envel Warp - Limit the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 10", channels = "any", tip = "Squeeze down amplitude into a specified range." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "limit", min = 0, max = 1, input = "brk", tip = "upper limit of range; amplitude values which were above limit are scaled down such that 'maxamp' becomes limit. Range: > threshold to 1." },
  arg5 = { name = "threshold", min = 0, max = 1, input = "brk", tip = "lower limit of range; only values above threshold are affected. Range: 0 to limit." },
}
 
dsp["Envel Warp - Normalise the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 1", channels = "any", tip = "NORMALISE; Expand envelope so that the highest envelope point is at the maximum possible amplitude value." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
}
 
dsp["Envel Warp - Reverse the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 2", channels = "any", tip = "Reverse the envelope in time." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
}
 
dsp["Envel Warp - Timestretch the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 6", channels = "any", tip = "Stretch or compress an envelope in time." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "timestretch", min = 0, max = 20, tip = "multiplier by which to stretch the envelope. Range: > 0.0 < 1.0 compresses the envelope in time, = 1.0 no change, > 1.0 stretches the envelope in time" },
}
 
dsp["Envel Warp - Trigger the envelope of a soundfile "] = {
  cmds = { exe = "envel", mode = "warp 13", channels = "any", tip = "Create a new envelope of sudden on bursts, triggered by the rate of rise of the current envelope." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 5, max = 3000, def = 50, tip = "the duration of the enveloping window, in milliseconds " },
  arg4 = { name = "gate", min = 0, max = 1, input = "brk", tip = "the average level must be above gate before triggering can occur. Range: 0 to 1." },
  arg5 = { name = "rise", min = 0, max = 1, tip = "minimum loudness-step before triggering can occur. Range: 0 to 1." },
  arg6 = { name = "dur", min = 0, max = 1000, tip = "maximum duration over which the rise can take place before triggering can occur. Range: must be >= the envelope window duration." },
}

--[[

dsp["[Doesn't work - multiple sound outputs] Envel Timegrid - Partition a soundfile into a sequence of 'windows' separated by silence  "] = { 
  cmds = { exe = "envel", mode = "timegrid", tip = "Imagine you have a regular fence composed of uprights separated by gaps. If you look through the fence, the view will be broken up into vertical slices, with the bits inbetween hidden by the fence posts. If you move slightly to your right you will see a different set of vertical slices of the view beyond the fence. Move a little more, and you will see the original view again, as the N+1 th fencepost blocks the same bits of the view as was previously blocked by the Nth fencepost. ENVEL TIMEGRID therefore creates a series of 'views' of a sound as if seen through a fence (or mask). Where the fence-posts block our 'view', there is only silence.  Musical applications; This function, therefore, partitions a sound into a number of grids. Each grid is like the sound seen through a 'fence' with regular gaps. Were we to mix these output sounds together, all starting at time 0, the original sound would be reconstituted. We could also mix them together in a different order. And if we made several of these grids from different sounds (with the same grid distance in time, i.e., gridwidth), they could be be mixed together to form hybrid sounds. ENVEL TIMEGRID is a new take on the idea of segmentation. What is original is that separate soundfiles are formed. The length of the segments (gridwidth) are user-defined, but you probably get the most interesting results when they are very short. With a mixing operation, with SUBMIX MIX (MIXFILES) or with an audio sequencer, the segments from the same or different sounds can be reassembled, creating strange hybrids, or collage effects if the segments are fairly long. Note that there are various parameters involved in exploring and fine-tuning the results: you can use just one soundfile and create staccato or stuttering effects, you can repeat ENVEL TIMEGRID with more soundfiles, creating grids from several soundfiles that can be mixed, and the order of these files in the mix can varied, gridcount – the number of output soundfiles from each input soundfile that you can put into the mix, gridwidth – the temporal width of the grid, which can be different for each input sound that you use, or could also vary over time(!), spliclen – which can be sharper or more gradual, adjusted in an appropriate way to gridwidth or also vary over time, starttime – the soundfile start times in the mixfile; they may be synchronous or offset by varying amounts, As with many CDP programs, therefore, there are many possibilities to explore. Finally, bear in mind that SUBMIX DUMMY can help you to create a mixfile from the outputs of ENVEL TIMEGRID very quickly. You just submit your list of soundfiles to SUBMIX DUMMY and it creates a basic mixfile that you can edit. In Sound Loom the process can be even quicker, because you can select all the files you want in the mix in CHOSEN FILES mode, go straight to SUBMIX DUMMY and create the mixfile." }, 
   arg1 = { name = "gridcount", min = 1, max = 32, def = 1, tip = "number of grids (and hence output files)" }, 
   arg2 = { name = "gridwidth", min = 0, max = 32, def = 1, input = "brk", tip = "duration of grid windows, in seconds" }, 
   arg3 = { name = "slicelen", min = 0, max = 1000, def = 50, input = "brk", tip = "splice length, in milliseconds" },
}

--]]
 
------------------------------------------------
-- envnu
------------------------------------------------
 
dsp["Envnu Expdecay - Produce a true exponential decay to zero on a sound"] = {
  cmds = { exe = "envnu", mode = "expdecay", channels = "any", tip = "An exponential curve involves a steady increase in the exponent of a number, its 'power': 21 = 2, 22 = 4, 23 = 8 – i.e., a progressive doubling, as opposed to a linear curve, which adds a constant value each time: 2 - 4 - 6 - 9 - 10 etc. In the case of an amplitude envelope, 1 is divided by these progressively increasing numbers: 1 ÷ 2 = 0.5, 1 ÷ 4 = 0.25, 1 ÷ 8 = 0.125 etc. This means that the rate of decay will be large to start with and then get progressively less (halving each time). The effect is that the decay slope starts very steeply and gradually becomes less steep as it proceeds to zero. It reaches zero (-100 dB) very gradually so that the tail-off is very smooth – and much of the last part of the soundfile is likely to be less than -50dB." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "starttime", min = 0, max = length/1000, def = 0, tip = "(time at which the decay begins" },
  arg4 = { name = "endtime", min = 0, max = length/1000, def = length/1000, tip = "(time at which the decay reaches zero" },
}
 
dsp["Peakchop - Isolate peaks in a source and play back at a specified tempo"] = {
  cmds = { exe = "envnu", mode = "peakchop 1", channels = "any", tip = "ENVNU PEAKCHOP first finds the peaks in the source. It then defines an area surrounding/following the peak, whose duration is determined by pkwidth. It then cuts chunks from the source at these places, cutting each chunk independently. This means that the first chunk can extend beyond the beginning of the next chunk(s) to be cut. These chunks are then played back in turn, at the specified tempo" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsize", min = 1, max = 64, def = 50, tip = "windowsize (in milliseconds) for extracting the envelope (Range: 1 to 64, Default: 50)" },
  arg4 = { name = "pkwidth", min = 0, max = 1000, def = 20, tip = "width in milliseconds of retained peaks (Range: 0 to 1000, Default: 20ms)" },
  arg5 = { name = "risetime", min = 0, max = 1000, def = 10, tip = "risetime in milliseconds from zero to peak (Default: 10ms)" },
  arg6 = { name = "tempo", min = 20, max = 3000, def = 100, tip = "tempo of resulting output (Metronome Mark = events per minute: as is customary, 60 means 1 per second, 120 means 2 per second etc.) (Range: 20 to 3000)" },
  arg7 = { name = "gain", min = 0, max = 1, def = 0.9, tip = "lower this if rapid tempo causes peaks to overlap (The program should produce a warning message should clipping result from the settings.) (Range: 0 to 1, Default 1)" },
  arg8 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "level (relative to max) below which peaks are ignored (Range: 0 to 1, Default: 0)" },
  arg9 = { name = "skew", switch = "-q-", min = 0, max = 1, def = 0.25, tip = "envelope centring on peak (Range: 0 to 1, Default: 0.25)" },
  arg10 = { name = "scatter", switch = "-s", min = 0, max = 1, tip = "randomisation of transposition contour from impulse to impulse" },
  arg11 = { name = "-norm", switch = "-norm", min = 0, max = 1, def = 0, tip = "force peakevent levels towards that of the loudest peak (Range: 0 to 1, Default: 0)" },
  arg12 = { name = "repeat", switch = "-r", min = 0, max = 256, def = 0, tip = "number of times peakevents are repeated (Range: 0 to 256, Default: 0)" },
  arg13 = { name = "miss", switch = "-m", min = 0, max = 64, def = 0, tip = "use peakevent, then skip the next miss peakevents (Range: 0 to 64, Default: 0)" },
}
 
dsp["Peakchop 2 - Output a peak-isolating envelope "] = {
  cmds = { exe = "envnu", mode = "peakchop 2", tip = "Output a peak-isolating envelope. ENVNU PEAKCHOP first finds the peaks in the source. It then defines an area surrounding/following the peak, whose duration is determined by pkwidth. It then cuts chunks from the source at these places, cutting each chunk independently. This means that the first chunk can extend beyond the beginning of the next chunk(s) to be cut. These chunks are then played back in turn, at the specified tempo" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "wsize", min = 1, max = 64, def = 50, tip = "windowsize (in milliseconds) for extracting the envelope (Range: 1 to 64, Default: 50)" },
  arg4 = { name = "pkwidth", min = 0, max = 1000, def = 20, tip = "width in milliseconds of retained peaks (Range: 0 to 1000, Default: 20ms)" },
  arg5 = { name = "risetime", min = 0, max = 1000, def = 10, tip = "risetime in milliseconds from zero to peak (Default: 10ms)" },
  arg6 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "level (relative to max) below which peaks are ignored (Range: 0 to 1, Default: 0)" },
  arg7 = { name = "skew", switch = "-q", min = 0, max = 1, def = 0.25, tip = "envelope centring on peak (Range: 0 to 1, Default: 0.25)" },
}
 
------------------------------------------------
-- extend
------------------------------------------------
 
dsp["Extend Baktobak - Join a time-reversed copy of the sound to a normal copy, in that order"] = {
  cmds = { exe = "extend", mode = "baktobak", channels = "any", tip = "I call the outputs of this process (learnt from Denis Smalley), fugu sounds. There is a particular Japanese fish delicacy, the fugu fish, which has a poisonous liver, but tastes best the nearer to the liver you slice the fish (!). Fugu sounds are made using an attack-resonance source (a sound with a sharp attack which then fades away to nothing). A Reverse copy of the sound is made and then spliced onto the original so the sound now grows from nothing until it reaches a peak of loudness and spectral brightness and then fades once again to zero. Musical applications; If the crossfade is made just before the peak is reached, the sound is less loud and bright in the centre, and in fact a whole sequence of such musically related sounds can be made, each of different loudness/brightness in the centre. This process allows such sounds to be made in a single pass." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "join_time", min = 0, max = length/1000, tip = "time in infile where join-cut is to be made" },
  arg4 = { name = "splice-length", min = 0, max = length, tip = "(length of the splice, in milliseconds" },
}
 
dsp["Extend Doublets - Divide a sound into segments that repeat, and splice them together"] = {
  cmds = { exe = "extend", mode = "doublets", channels = "any", tip = "EXTEND DOUBLETS is a 'slice' function, like the ones we are familiar with in the visual realm. The difference, here, in the temporal realm, is the repetition parameter. We specify the length of the segments (slices) and the number of times it repeats. What we hear depends, as usual, on the sonic material. With voices or conventional music, the effect will be like the needle getting stuck on a vinyl record: a short passage repeats. With more complex sonic material, we would get a pulsing, mechanical effect. Especially note that the length of the segment (segdur) parameter can vary over time. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "segdur", min = 0, max = length/1000, input = "brk", tip = "duration of segments" },
  arg4 = { name = "repets", min = 2, max = 32, def = 4, tip = "number of times each segment is repeated" },
  arg5 = { name = "-s", switch = "-s", tip = "option to have outfile try to stay synchronised with the infile" },
}
 
dsp["Extend Drunk - Drunken walk 2 through source file (chosen segments read forwards)"] = {
  cmds = { exe = "extend", mode = "drunk 2", channels = "any", tip = "Another approach to segmentation, EXTEND DRUNK takes a series of segments selected from the infile and splices them together to form the outfile. The process starts at some time in the file, called the locus and selects a segment, randomly, from within an 'ambit'. The length of 'ambit' is 2 * ambitus, and stretches to both sides of the locus position. Once a segment is read, the program moves (randomly) to a new position in either direction, within the ambit, and not more than step from the start location of the previous segment, from where it starts the next read. This is called a 'random walk'; hence the name 'drunk'. The function is based on a drunken walk algorithm implemented by Miller Puckette. While it is doing this walk, one can shift the locus, e.g., progressing slowly through the file. (NB: note above about times in the breakpoint file.) The 'ambit' – the portion of the soundfile being used at any one time, can be varied by altering the size of the ambitus, which is one-half the full 'ambit' width. The step, which is the maximum distance between the start of one read and the start of the next read (but must lie within the ambit), can also be varied. For example, if the ambitus is small, segments very close to one another will be selected. Or if the step is much smaller than the segment size (a slow clock produces longer segments), selected segments will tend to overlap, producing random echoes or pre-echoes. The length of the segment, determined by the clock parameter is NOT constrained to the size of the ambit, so segments may begin within the ambit and end outside it. Segments which would end beyond the specified outdur are truncated. To summarise, then, locus, ambitus and step all refer to start locations. Clock refers to segment length. The process continues until outdur is filled, which makes it a useful program with which to generate material." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outdur", min = 1, max = 60, def = 10, tip = "total minimum duration of output soundfile (seconds)" },
  arg4 = { name = "locus", min = 0, max = length/1000, input = "brk", tip = "time in infile at which the drunken walk occurs (seconds)" },
  arg5 = { name = "ambitus", min = 0, max = length/1000, input = "brk", tip = "half-width of the region from within which the sound segments are read (seconds)" },
  arg6 = { name = "step", min = 0.002, max = 10, input = "brk", def = 1, tip = "maximum length of (random) step between segment reads (> 0.002 seconds); this always falls within the ambitus: it is automatically adjusted where too large" },
  arg7 = { name = "clock", min = 0, max = 10, input = "brk", def = 0.2, tip = "time between segment reads: this is the segment duration (> splicelen * 2) (seconds)" },
  arg8 = { name = "mindrnk", min = 1, max = 32767, input = "brk", def = 5, tip = "minimum number of clock ticks between sober plays (1 - 32767 Default: 10)" },
  arg9 = { name = "maxdrnk", min = 1, max = 32767, input = "brk", def = 100, tip = "maximum number of clock ticks between sober plays (1 - 32767 Default: 30)" },
  arg10 = { name = "splicelen", switch = "-s", min = 0, max = 1000, def = 10, tip = "length in milliseconds of the splice slope (Default: 15ms)" },
  arg11 = { name = "clokrand", switch = "-c-", min = 0, max = 1, input = "brk", def = 0.6, tip = "randomisation of clock ticks (Range: 0 to 1 Default: 0)" },
  arg12 = { name = "overlap", switch = "-o", min = 0, max = 0.99, input = "brk", def = 0, tip = "mutual overlap of segments in output (Range: 0 to 0.9900 Default: 0)" },
  arg13 = { name = "seed", switch = "-r", tip = "any set value gives reproducible output" },
  arg14 = { name = "losober", switch = "-l", min = 0, max = length/1000, tip = "minimum duration of sober plays (seconds) (Range: > 0 to duration of infile+. If >= duration of infile, all sober plays go to the end of the source." },
  arg15 = { name = "hisober", switch = "-h", min = 0, max = length/1000, tip = "maximum duration of sober plays (seconds) " },
}
 
dsp["Extend Drunk - Drunken walk through source file (chosen segments read forwards)"] = {
  cmds = { exe = "extend", mode = "drunk 1", channels = "any", tip = "Another approach to segmentation, EXTEND DRUNK takes a series of segments selected from the infile and splices them together to form the outfile. The process starts at some time in the file, called the locus and selects a segment, randomly, from within an 'ambit'. The length of 'ambit' is 2 * ambitus, and stretches to both sides of the locus position. Once a segment is read, the program moves (randomly) to a new position in either direction, within the ambit, and not more than step from the start location of the previous segment, from where it starts the next read. This is called a 'random walk'; hence the name 'drunk'. The function is based on a drunken walk algorithm implemented by Miller Puckette. While it is doing this walk, one can shift the locus, e.g., progressing slowly through the file. (NB: note above about times in the breakpoint file.) The 'ambit' – the portion of the soundfile being used at any one time, can be varied by altering the size of the ambitus, which is one-half the full 'ambit' width. The step, which is the maximum distance between the start of one read and the start of the next read (but must lie within the ambit), can also be varied. For example, if the ambitus is small, segments very close to one another will be selected. Or if the step is much smaller than the segment size (a slow clock produces longer segments), selected segments will tend to overlap, producing random echoes or pre-echoes. The length of the segment, determined by the clock parameter is NOT constrained to the size of the ambit, so segments may begin within the ambit and end outside it. Segments which would end beyond the specified outdur are truncated. To summarise, then, locus, ambitus and step all refer to start locations. Clock refers to segment length. The process continues until outdur is filled, which makes it a useful program with which to generate material." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outdur", min = 1, max = 60, def = 5, tip = "total minimum duration of output soundfile (seconds)" },
  arg4 = { name = "locus", min = 0, max = length/1000, input = "brk", tip = "time in infile at which the drunken walk occurs (seconds)" },
  arg5 = { name = "ambitus", min = 0, max = length/1000, input = "brk", tip = "half-width of the region from within which the sound segments are read (seconds)" },
  arg6 = { name = "step", min = 0.002, max = 10, input = "brk", def = 1, tip = "maximum length of (random) step between segment reads (> 0.002 seconds); this always falls within the ambitus: it is automatically adjusted where too large" },
  arg7 = { name = "clock", min = 0, max = 10, input = "brk", def = 0.5, tip = "time between segment reads: this is the segment duration (> splicelen * 2) (seconds)" },
  arg8 = { name = "splicelen", switch = "-s", min = 0, max = 1000, def = 15, tip = "length in milliseconds of the splice slope (Default: 15ms)" },
  arg9 = { name = "clokrand", switch = "-c-", min = 0, max = 1, input = "brk", def = 0, tip = "randomisation of clock ticks (Range: 0 to 1 Default: 0)" },
  arg10 = { name = "overlap", switch = "-o", min = 0, max = 0.99, input = "brk", def = 0, tip = "mutual overlap of segments in output (Range: 0 to 0.9900 Default: 0)" },
  arg11 = { name = "seed", switch = "-r", tip = "any set value gives reproducible output" },
}
 
dsp["Extend Freeze 1 - Freeze a segment of a sound by iteration in a fluid manner"] = {
  cmds = { exe = "extend", mode = "freeze 1", channels = "any", tip = "Extend a specific part of a sound using the iteration procedure. This tends to give a more convincing time-stretching result than any of the other time-stretch procedures, particularly as the non-time-stretched portions of the sound are not subject to any processing. The internal proportions of a sound event can be manipulated using this process. Musical applications; The start and end times of the freeze enable you to focus on very specific parts of the sound, such as the 'a' in 'star' or the 's' in 'star'. With this program you can extend these to form sounds such as 'staaaaaaaaaaaaaaaaaaaar' or 'ssssssssssssssssssstar'. Given the claim that it gives 'a more convincing time-stretching result than any of the other time-stretch procedures', it is a program well worth exploring thoroughly." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outduration", min = 1, max = 30, def = 5, tip = "Desired duration of resultant soundfile." },
  arg4 = { name = "delay", min = 0, max = 1, def = 0.5, tip = "The (average) delay between iterations: <= length of frozen segment." },
  arg5 = { name = "rand", min = 0, max = 1, def = 0.6, tip = "Delaytime randomisation. Range: 0 to 1. Default: 0." },
  arg6 = { name = "pshift", min = 0, max = 12, tip = "Maximum of random pitchshift of each iteration. Range: 0 to 12 semitones. E.g., 2.5 = 2.5 semitones up or down." },
  arg7 = { name = "ampcut", min = 0, max = 1, tip = "Maximum of random amplitude reduction on each iteration. Range: 0 to 1. Default: 0." },
  arg8 = { name = "starttime_of_freeze", min = 0, max = length/1000, def = 0, tip = "Time where the frozen segment begins in the original sound." },
  arg9 = { name = "endtime", min = 0, max = length/1000, tip = "Time where the frozen segment ends in the original sound. " },
  arg10 = { name = "gain", min = 0, max = 1, tip = "gain, innit." },
  arg11 = { name = "seed", switch = "-s-", tip = "The same seed number will produce identical output on rerun. Default: 0 – random sequence is different every time." },
}
 
dsp["Extend Freeze 2 - Freeze a segment of a sound by iteration in a fluid manner - specify repetitions"] = {
  cmds = { exe = "extend", mode = "freeze 2", channels = "any", tip = "Extend a specific part of a sound using the iteration procedure. This tends to give a more convincing time-stretching result than any of the other time-stretch procedures, particularly as the non-time-stretched portions of the sound are not subject to any processing. The internal proportions of a sound event can be manipulated using this process. Musical applications; The start and end times of the freeze enable you to focus on very specific parts of the sound, such as the 'a' in 'star' or the 's' in 'star'. With this program you can extend these to form sounds such as 'staaaaaaaaaaaaaaaaaaaar' or 'ssssssssssssssssssstar'. Given the claim that it gives 'a more convincing time-stretching result than any of the other time-stretch procedures', it is a program well worth exploring thoroughly." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "repetitions", min = 1, max = 3000, def = 10, tip = "Number of repetitions of frozen segment." },
  arg4 = { name = "delay", min = 0, max = 1, def = 0.5, tip = "The (average) delay between iterations: <= length of frozen segment." },
  arg5 = { name = "rand", min = 0, max = 1, tip = "Delaytime randomisation. Range: 0 to 1. Default: 0." },
  arg6 = { name = "pshift", min = 0, max = 12, tip = "Maximum of random pitchshift of each iteration. Range: 0 to 12 semitones. E.g., 2.5 = 2.5 semitones up or down." },
  arg7 = { name = "ampcut", min = 0, max = 1, tip = "Maximum of random amplitude reduction on each iteration. Range: 0 to 1. Default: 0." },
  arg8 = { name = "starttime_of_freeze", min = 0, max = length/1000, def = 0, tip = "Time where the frozen segment begins in the original sound." },
  arg9 = { name = "endtime", min = 0, max = length/1000, tip = "Time where the frozen segment ends in the original sound. " },
  arg10 = { name = "gain", min = 0, max = 1, tip = "gain, innit." },
  arg11 = { name = "seed", switch = "-s-", tip = "The same seed number will produce identical output on rerun. Default: 0 – random sequence is different every time." },
}
 
dsp["Extend Iterate - Repeat sound with subtle variations to a specified duration"] = {
  cmds = { exe = "extend", mode = "iterate 1", channels = "any", tip = "EXTEND ITERATE was written as a way of achieving more natural sounding iterations of a soundfile by introducing a randomisation of the delay time between each iterated segment, and slight variations in pitch or amplitude between the segments, as would occur in a naturally iterating source (e.g., a rolled 'rr' vocal sound). These randomisations can be selected (e.g., one might omit pitch variation, or not apply randomisation to the delay times), or applied in an exaggerated fashion, to achieve a number of different musical results. The rand parameter introduces slight variations in delay between iterations, which may increase the 'naturalness' of the result. Omitting the rand parameter will produce a more mechanical echo effect. The gain parameter allows some control over the amplitude of the mixed portions; the amount of gain suitable is dependent on the amplitude of the signal at the beginning and end of the soundfile (where the repeated units overlap). This can be examined with a soundfile viewer (such as VIEWSF, which can display the amplitude of each individual sample), and the gain adjusted accordingly if the defaults don't seem to be handling it properly. When randomisation is used, the gain is further reduced in the expectation that there will be a greater degree of overlap. Musical applications; This function produces a series of (usually overlapping) repeats of a soundfile. The nature of the attack portion of the soundfile – sharp or gradual – will greatly affect the way these repetitions are perceived. The use of a very short soundfile, e.g., 0.2 seconds, especially one with a sharp attack, will result in a rapid-fire succession of easily perceived iterations." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outduration", min = length/1000, max = length/1000+60, def = 10, tip = "length in seconds of outfile" },
  arg4 = { name = "delay", switch = "-d", min = 0, max = length/1000, tip = "(average) delay between iterations in seconds (Default: length of infile)" },
  arg5 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "delay-time randomisation (Range: 0 to 1, Default: 0)" },
  arg6 = { name = "pshift", switch = "-p", min = 0, max = 12, tip = "maximum random pitchshift of each iteration in semitones (Range: 0 to 12 semitones; e.g., 2.5 = 2.5 semitones up or down)" },
  arg7 = { name = "ampcut", switch = "-a", min = 0, max = 1, tip = "maximum random amplitude reduction on each iteration (Range: 0 to 1, Default 0)" },
  arg8 = { name = "fade", switch = "-f", min = 0, max = 1, tip = "(average) amplitude fade between iterations (Range: 0 to 1, Default 0)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, tip = "overall gain (Range: 0 to 1, Default: 0, which gives the best guess for no distortion)" },
  arg10 = { name = "seed", switch = "-s-", tip = "the same seed number will produce identical output on rerun (Default: 0 – the random sequence is different every time)" },
}
 
dsp["Extend Iterate 2 - Repeat sound with subtle variations a specific number of times"] = {
  cmds = { exe = "extend", mode = "iterate 2", channels = "any", tip = "EXTEND ITERATE was written as a way of achieving more natural sounding iterations of a soundfile by introducing a randomisation of the delay time between each iterated segment, and slight variations in pitch or amplitude between the segments, as would occur in a naturally iterating source (e.g., a rolled 'rr' vocal sound). These randomisations can be selected (e.g., one might omit pitch variation, or not apply randomisation to the delay times), or applied in an exaggerated fashion, to achieve a number of different musical results. The rand parameter introduces slight variations in delay between iterations, which may increase the 'naturalness' of the result. Omitting the rand parameter will produce a more mechanical echo effect. The gain parameter allows some control over the amplitude of the mixed portions; the amount of gain suitable is dependent on the amplitude of the signal at the beginning and end of the soundfile (where the repeated units overlap). This can be examined with a soundfile viewer (such as VIEWSF, which can display the amplitude of each individual sample), and the gain adjusted accordingly if the defaults don't seem to be handling it properly. When randomisation is used, the gain is further reduced in the expectation that there will be a greater degree of overlap. Musical applications; This function produces a series of (usually overlapping) repeats of a soundfile. The nature of the attack portion of the soundfile – sharp or gradual – will greatly affect the way these repetitions are perceived. The use of a very short soundfile, e.g., 0.2 seconds, especially one with a sharp attack, will result in a rapid-fire succession of easily perceived iterations." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "repetitions", min = 1, max = 100, def = 2, tip = "number of repetitions in the iteration" },
  arg4 = { name = "delay", switch = "-d", min = 0, max = 25, def = 0.6, tip = "(average) delay between iterations in seconds (Default: length of infile)" },
  arg5 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "delay-time randomisation (Range: 0 to 1, Default: 0)" },
  arg6 = { name = "pshift", switch = "-p", min = 0, max = 12, tip = "maximum random pitchshift of each iteration in semitones (Range: 0 to 12 semitones; e.g., 2.5 = 2.5 semitones up or down)" },
  arg7 = { name = "ampcut", switch = "-a", min = 0, max = 1, tip = "maximum random amplitude reduction on each iteration (Range: 0 to 1, Default 0)" },
  arg8 = { name = "fade", switch = "-f", min = 0, max = 1, tip = "(average) amplitude fade between iterations (Range: 0 to 1, Default 0)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, tip = "overall gain (Range: 0 to 1, Default: 0, which gives the best guess for no distortion)" },
  arg10 = { name = "seed", switch = "-s-", tip = "the same seed number will produce identical output on rerun (Default: 0 – the random sequence is different every time)" },
}
 
dsp["Extend Loop - 1 Loop (repeat [advancing] segments) inside soundfile "] = {
  cmds = { exe = "extend", mode = "loop 1", channels = "any", tip = "The key feature of this process is that it joins together, end-to-end, a series of segments taken from the file, each with a splice slope to avoid clicks. These segments are all of the same length, so one way or another, the result may appear to have some degree of regular pulsation. This does not (usually) result from the presence of splices, but rather is a perceptual result caused by the repetition of sonic material. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000, tip = "time (in seconds) in infile at which the looping process begins" },
  arg4 = { name = "len", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg5 = { name = "step", min = 0, max = 1000, def = 50, tip = "advance in infile from the start of one loop to the next (in milliseconds) " },
  arg6 = { name = "splen", switch = "-w", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg7 = { name = "scat", switch = "-s", min = 0, max = 30, tip = "make step advance irregularly, within the timeframe given by scat" },
  arg8 = { name = "-b", switch = "-b", tip = "play from beginning of infile (even if looping doesn't begin there)" },
}
 
dsp["Extend Loop - 2 Loop (repeat [advancing] segments) inside soundfile "] = {
  cmds = { exe = "extend", mode = "loop 2", channels = "any", tip = "The key feature of this process is that it joins together, end-to-end, a series of segments taken from the file, each with a splice slope to avoid clicks. These segments are all of the same length, so one way or another, the result may appear to have some degree of regular pulsation. This does not (usually) result from the presence of splices, but rather is a perceptual result caused by the repetition of sonic material. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "dur", min = 0, max = 30, def = 10, tip = "duration of outfile required (in seconds)" },
  arg4 = { name = "start", min = 0, max = length/1000, tip = "time (in seconds) in infile at which the looping process begins" },
  arg5 = { name = "len", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg6 = { name = "step", switch = "-l", min = 0, max = 1000, def = 50, tip = "advance in infile from the start of one loop to the next (in milliseconds) " },
  arg7 = { name = "splen", switch = "-w", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg8 = { name = "scat", switch = "-s", min = 0, max = 30, tip = "make step advance irregularly, within the timeframe given by scat" },
  arg9 = { name = "-b", switch = "-b", tip = "play from beginning of infile (even if looping doesn't begin there)" },
}
 
dsp["Extend Loop - 3 Loop (repeat [advancing] segments) inside soundfile "] = {
  cmds = { exe = "extend", mode = "loop 3", channels = "any", tip = "The key feature of this process is that it joins together, end-to-end, a series of segments taken from the file, each with a splice slope to avoid clicks. These segments are all of the same length, so one way or another, the result may appear to have some degree of regular pulsation. This does not (usually) result from the presence of splices, but rather is a perceptual result caused by the repetition of sonic material. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cnt", min = 0, max = 1000, def = 10, tip = "number of loop repeats required" },
  arg4 = { name = "start", min = 0, max = length/1000, tip = "time (in seconds) in infile at which the looping process begins" },
  arg5 = { name = "len", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg6 = { name = "step", switch = "-l", min = 0, max = 1000, def = 100, tip = "advance in infile from the start of one loop to the next (in milliseconds) " },
  arg7 = { name = "splen", switch = "-w", min = 0, max = 1000, def = 25, tip = "length of splice (in milliseconds) (Default: 25ms)" },
  arg8 = { name = "scat", switch = "-s", min = 0, max = 30, tip = "make step advance irregularly, within the timeframe given by scat" },
  arg9 = { name = "-b", switch = "-b", tip = "play from beginning of infile (even if looping doesn't begin there)" },
}
 
dsp["Extend Repetitions - Repeat source at given times from text file"] = {
  cmds = { exe = "extend", mode = "repetitions", channels = "any", tip = "This program can be thought of either as a more controlled looping function or a simple rhythm sequencer. Musical applications; Controlled looping or rhythmic sequencing." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "timesfile", input = "txt", tip = "Textfile of times (in seconds) at which the source plays" },
  arg4 = { name = "level", min = 0, max = 1, def = 0.9, tip = "Level of output. Range: 0 to 1." },
}
 
dsp["Extend Scramble 1 - Scramble soundfile and write to any given length"] = {
  cmds = { exe = "extend", mode = "scramble 1", channels = "any", tip = "With EXTEND SCRAMBLE, segments of soundfile are selected from a wide variety of locations in the infile, jumping back and forth a great deal.Mode 1 takes the infile, chooses a random chunk of it, and then chooses another random chunk of it which may overlap with the first choice, then another chunk which may overlap with either of the other two ... etc. Then it splices them all together. Thus, any bits of the file may be repeated quite quickly if overlapping material is selected in consecutive chunks, and some bits may not appear at all if never randomly selected. The size of the chunks will be a random length somewhere between minseglen and maxseglen. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "minseglen", min = 0, max = length/1000, tip = "minimum chunksize to cut" },
  arg4 = { name = "maxseglen", min = 0.045, max = length/1000, tip = "maximum chunksize to cut (Range: 0.045 to length of infile – must be > minseglen)" },
  arg5 = { name = "outdur", min = 0, max = 30, def = 10, tip = "duration of outfile required (> maxseglen" },
  arg6 = { name = "splen", switch = "-w", min = 0, max = 1000, def = 25, tip = "duration of splice in milliseconds Default: 25ms)" },
  arg7 = { name = "seed", switch = "-s", min = 0, max = 1, tip = "the same seed number will produce identical output on rerun (Default: 0, random sequence is different every time)" },
  arg8 = { name = "-b", switch = "-b", tip = "force start of outfile to be beginning of infile" },
  arg9 = { name = "-e", switch = "-e", tip = "force end of outfile to be end of infile" },
}
 
dsp["Extend Scramble 2 - Scramble soundfile and write to any given length"] = {
  cmds = { exe = "extend", mode = "scramble 2", channels = "any", tip = "Mode 2, cuts the entire file into random-length chunks which do not overlap. It arranges these at random. The process is then repeated, but the random cuts are of course in different positions in the file. Consequently, the entire file is used, and used only once, before the process starts to use the file again.In Mode 2 an average chunksize is specified plus a random factor (scatter). The formula which shows what the maximum scatter factor can be reveals that Mode 2 can be used to make chunks which vary a great deal in length. For example, if the infile is 2 seconds long and seglen is 0.3, the maximum value for scatter will be 6.0 (rounded down). (This value was accepted – and worked – even with an outdur of 4.0.)The 2nd Mode also provides the option to rerun with identical output.The ability to write to any length of outfile makes it possible to give the process plenty of time to make full use of the infile." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "seglen", min = 0, max = length/1000, tip = "average chunksize to cut" },
  arg4 = { name = "scatter", min = 0.045, max = length/1000, tip = "randomisation of chunk lengths (>= 0) Cannot be greater than infilesize/seglen (rounded down) " },
  arg5 = { name = "outdur", min = 0, max = 30, def = 10, tip = "duration of outfile required (> maxseglen" },
  arg6 = { name = "splen", switch = "-w", min = 0, max = 1000, def = 25, tip = "duration of splice in milliseconds Default: 25ms)" },
  arg7 = { name = "seed", switch = "-s", min = 0, max = 1, tip = "the same seed number will produce identical output on rerun (Default: 0, random sequence is different every time)" },
  arg8 = { name = "-b", switch = "-b", tip = "force start of outfile to be beginning of infile" },
  arg9 = { name = "-e", switch = "-e", tip = "force end of outfile to be end of infile" },
}
 
dsp["Extend Sequence - Produce a sequence from an input sound played at specified transpositions and times from sequence text file "] = {
  cmds = { exe = "extend", mode = "sequence", channels = "any", tip = "This program works like a simple conventional sequencer except that it takes only one input (see EXTEND SEQUENCE2 for multiple soundfile input. The process takes a sequence-file of triple-values: output-time – the time when you want the soundfile to come in again in the outfile, transposition – the pitch-level of that entry, given in (possibly fractional) semitones, amplitude – the relative level of that sound in the output sequence, louder (> 1.0) or softer (< 1.0) for each event in the sequence. The source sound is then copied at each output-time, transposed by each transposition) amount in (fractional) semitones, and attenuated to the level specified. The result is a sequence of events derived from the one source sample. Musical applications; This function was used, for example, to make the underlying sequence of the 'Gamelan' in my (T Wishart) composition, Imago. TW: in Trevor Wishart's compositon, Imago. Transposition or time sequences might be derived from data from other sounds, generated in the Sound Loom Table Editor or with COLUMNS, or entered by hand in a text file. EXTEND SEQUENCE is therefore a useful way to create rhythmic textures, whether simple or very intricate." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "sequence-file ", input = "txt", tip = "contains 3 values on each line, separated by tabs or spaces, one line per event: output-time semitone-transposition loudness value triples, where loudness is a loudness multiplier. There needs to be one value-triple for each event in the sequence." },
  arg4 = { name = "attenuation", min = -10, max = 10, def = 0.9, tip = "overall attenuation to apply to the source, should outfile overload " },
}
 
dsp["Extend Sequence 2 - Produce a sequence from several sounds played at specified transpositions and times from sequence text file "] = {
  cmds = { exe = "extend", mode = "sequence 2", channels = "any", tip = "Here we have a convenient way to arrange several different files in a rhythmic way, with several additional parameter settings. There are two input sounds. The first one given to the program will be No. 1 and the second one will be No. 2. The Start times set the times at which they begin to play. The MPVs (MIDI Pitch Values) specify the pitch level relative to the notional MPVs given in the first line, one for each file, which may or may not be the actual pitch level of the sound. Then the amplitude level is specified, so that certain sounds can be emphasised, different original amplitudes readjusted, etc. Finally, the duration of the note event is given. It may be shorter than the original sound, but, of course, not longer. This provides an easy way to work with the attack portion of a sound, or to layer longer sounds. Musical applications; We are familiar with standard MIX files. They specify the sound by using its name. Here we specify the sound by using a number. This means that it is easy to list and order the sounds, as well as to use a numerical pattern generated in some other way, such as algorithmically. Also different from the standard MIX files are the pitch transposition and duration fields. If the sound is fairly clearly pitched, such as a bell sound, then EXTEND SEQUENCE2 enables us quickly to: Create a melody, Create a lively rhythmic pattern, with the same or different pitches, Pattern the loudness in a sequence of sounds, Use the attack of the sound we want while discarding the remainder, Create some 'changes' as used by bell-ringers – or any other intricate sequence of notes. Note that the same sound can be repeated. The functionality of the program is shown by combining its use with the table editing software in the CDP System. Thus we can massage the columnar data with the Table Editor in Sound Loom (= DATA, Columns in Soundshaper or just columns on the Command Line). Several different versions could be made and then each one realised with EXTEND SEQUENCE2 by loading in the various sequence-files in turn. For example, a structural ritardando could be made by adding a value to the start_time column and subtracting a value from the pitch_level column 3 or 4 times. The result is a series of output soundfiles in which several sounds repeat, placed further and further apart in time, while getting closer together in pitch. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "sequence-file ", input = "txt", tip = "data file in which the first line contains notional MIDI pitch values for each input soundfile and each subsequent line contains 5 values - see docs" },
  arg5 = { name = "attenuation", min = -10, max = 10, def = 0.9, tip = "overall attenuation to apply to the source, should outfile overload " },
}
 
------------------------------------------------
-- fastconv
------------------------------------------------
 
dsp["Fastconv - Multi-channel fast convolution "] = {
  cmds = { exe = "fastconv", mode = "", tip = "'Convolution' is, according to Wikipedia, 'a mathematical operation on two functions, producing a third function that is typically viewed as a modified version of the original function.' In this case, we are using an impulse_file to act upon an input soundfile. With FASTCONV we have a specialised way to create reverberation. The key to its use lies in the impulse_file, which contains the reverberation information to be applied to the input soundfile. It was written to provide a fast and efficient way to carry out this process. The unexpected can happen when a soundfile is convolved using itself as the impulse_file. Musical Applications; The primary application of FASTCONV is convolution-based reverberation using a sampled impulse response of a building or other responsive space. The term 'fast' refers to the use of the Fast Fourier Transform (FFT) to perform the convolution. The program can also be used more experimentally, as the impulse response input can be any Mono or Multi-channel file (see details of available channel combinations above). A file can also be convolved with itself. The sample rates of the two soundfiles must be the same. The program uses double-precision processing throughout, and files of considerable length can be processed cleanly. Note however that the FFT of the whole impulse response is stored in main memory, so very large files may raise memory demands to critical levels. The input impulse response file is padded if necessary with silence to bring its size up to the next largest power-of-two size (required for the FFT process). This can result in a much larger memory footprint (in the worst case, twice as much) than the size of the file itself. A future version of the program may implement 'partitioned convolution', which will not require this substantial memory overhead.  " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "X", switch = "-a", min = 0, max = 4096, def = 512, tip = "scale output amplitude by X amount; values below 1 reduce the amplitude, and values above 1 increase it" },
  arg5 = { name = "-f", switch = "-f", tip = "write the output as floats (no clipping)." },
  arg6 = { name = "dry", min = 0, max = 1, def = 0, tip = "option to set a dry/wet mix, e.g., for reverb. Range: 0.0 to 1.0 (Default = 0.0)" },
}
 
------------------------------------------------
-- filter
------------------------------------------------
 
dsp["Filter Bank 1 - Harmonic"] = {
  cmds = { exe = "filter", mode = "bank 1", channels = "any", tip = "The harmonic series is formed by multiplying a given frequency by an ascending series of integers: 1, 2, 3, 4, etc. The harmonic relationships produced are sonorous, but depart from the structure of triadic chords higher in the series. This Mode builds this type of harmonic relationship onto lofrq, which may be the actual pitch of a pitched tone or any arbitrary pitch. Because this is being done with a filter process, other sonic material is cleaned away, so a cleaner (and possibly quieter) sound may result, with more complex noise elements removed. Quite a bit of gain may be required – values of 40 or 50+ in this situation will not be unusual. Set hifrq as high as possible less too few harmonics are produced. This clean sound may provide a good input for spectral time-stretching, for example, to achieve a well-tuned sound with a minimum of artefacts." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", tip = "Sets the Q of the effect" },
  arg4 = { name = "Gain", min = 0.001, max = 10000, def = 2.5, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "Scatter", switch = "-s", min = 0, max = 1, tip = "Random scatter of frequencies" },
  arg8 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bank 2 - Alt Harmonic"] = {
  cmds = { exe = "filter", mode = "bank 2", channels = "any", tip = "This will be like Mode 1 but with every other harmonic omitted: the odd numbered partials are retained. The resulting sound will probably sound more 'hollow'." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "Scatter", switch = "-s", min = 0, max = 1, },
  arg8 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bank 3 - Sub Harmonic"] = {
  cmds = { exe = "filter", mode = "bank 3", channels = "any", tip = "The 'subharmonic series' is the intervallic inverse of the harmonic series. Thus the departure from triadic-type intervals increases as the series descends. The resulting sound has a deeper tone and is somewhat hollow and somewhat inharmonic." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "Scatter", switch = "-s", min = 0, max = 1, },
  arg8 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bank 4 - Harmonic Linear"] = {
  cmds = { exe = "filter", mode = "bank 4", channels = "any", tip = "The offset in Hz is added to each harmonic. This will displace them so that they will no longer be exact multiples of the fundamental. This means that the cycles of the waveforms no longer line up at nodes, which introduces an 'inharmonic' dimension into the sound, heard as an increase in timbral components." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "Offset (Hz)", min = 10, max = srate/3, },
  arg8 = { name = "Scatter", switch = "-s", min = 0, max = 1, },
  arg9 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bank 5 - Filter Intervals"] = {
  cmds = { exe = "filter", mode = "bank 5", channels = "any", tip = "Here we depart from the harmonic series and simply divide up the specified frequency range into an equal number filters: how many is specified by the user with param. The first effect this equal spacing will have is to create inharmonic (rather than integer multiple) relationships between the partials. A low number of filters will produce an 'open' sound, and a high number of filters will produce a denser, richer sound. These might be very interesting sounds to timestretch." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "No. Filters", min = 1, max = 100, },
  arg8 = { name = "Scatter", switch = "-s", min = 0, max = 1, },
  arg9 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bank 6 - Semitone Intervals"] = {
  cmds = { exe = "filter", mode = "bank 6", channels = "any", tip = "By specifying the interval (in semitones), we in effect repeat these intervals within the frequency space between lofrq and hifrq, thus producing complex chords composed of the same interval 'piled up'. This is a quick way to 'harmonise' a sound, with the resulting density dependent on the size of the interval. It is well worth re-running this function, entering intervals from 1 to 7, for example, to hear what kind of transformations will be produced. Some unexpected resonances may result; they could be filtered out later." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Q", min = 0.001, max = 10000, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/3, },
  arg6 = { name = "HiFrq", min = 10, max = srate/3, },
  arg7 = { name = "Semitones", min = 1, max = 12, },
  arg8 = { name = "Scatter", switch = "-s", min = 0, max = 1, },
  arg9 = { name = "Double", switch = "-d", },
}
 
dsp["Filter Bankfrqs - 1 HARMONIC SERIES over lofrq - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 1", tip = "1 HARMONIC SERIES over lofrq. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Bankfrqs - 2 ALTERNATE HARMONICS over lofrq - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 2", tip = "2 ALTERNATE HARMONICS over lofrq. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Bankfrqs - 3 SUBHARMONIC SERIES below hifrq - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 3", tip = "3 SUBHARMONIC SERIES below hifrq. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Bankfrqs - 4 HARMONIC SERIES WITH LINEAR OFFSET: param = offset in Hz - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 4", tip = "4 HARMONIC SERIES WITH LINEAR OFFSET: param = offset in Hz. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "param", min = 0, max = 44100, def = 10, tip = "offset in Hz" },
  arg6 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Bankfrqs - 5 EQUAL INTERVALS BETWEEN lofrq and hifrq: param = number of filters - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 5", tip = "5 EQUAL INTERVALS BETWEEN lofrq and hifrq: param = number of filters. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "param", min = 0, max = 1000, def = 10, tip = "number of filters" },
  arg6 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Bankfrqs - 6 EQUAL INTERVALS BETWEEN lofrq and hifrq: param = number of semitones in the interval - Generate a bank of frequencies for use as a filterbank"] = {
  cmds = { exe = "filter", mode = "bankfrqs 6", tip = "6 EQUAL INTERVALS BETWEEN lofrq and hifrq: param = number of semitones in the interval. This function carries out the same operations as FILTER BANK, but writes the resulting frequencies to a textfile rather than applying them to a soundfile. NB: An existing outtextfile of the same name will be overwritten without checking with you first. See FILTER BANK for the meaning of each Mode. Musical applications; This textfile can then be edited and is then available as an input to FILTER USERBANK and FILTER VARIBANK, the filter bank functions which take a user-defined set of filter centre-frequencies as an input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "lofrq", min = 10, max = srate/3, def = 10, tip = "low frequency limit of filters (Range: 10 to sample_rate/3)" },
  arg4 = { name = "hifrq", min = 10, max = srate/3, tip = "high frequency limit of filters (Range: lofrq+ to sample_rate/3)" },
  arg5 = { name = "param", min = 0, max = 24, def = 2, tip = "number of semitones in interval" },
  arg6 = { name = "scat", switch = "-s", min = 0, max = 1, def = 0.5, tip = "random scatter of filter frequencies (Range: 0 to 1; the Default is 0)" },
}
 
dsp["Filter Fixed 1 - Boost or Cut below a given frequency "] = {
  cmds = { exe = "filter", mode = "fixed 1", channels = "any", tip = "This filter is referred to as 'fixed' because a single frequency of fixed amplitude roll-off (Q), is preset within the function. FILTER FIXED requires very precise frequency inputs. It can therefore be useful to analyse the sound and use SPECINFO PEAK get a profile of the frequency bands in which most energy occurs. For example, a trombone sound playing an F below Middle-C (174.61 Hz) showed almost no change when this frequency was used as the input to FILTER FIXED. SPECINFO PEAK showed that the energy was concentrated between 640 Hz and 905 Hz. Mode 1 and Mode 2 inputs relating to these frequencies worked fine: e.g., filtering out below 905 Hz or above 640 Hz really made a difference; similarly, filtering out a 200 Hz band centered on 700 Hz. Musical applications; FILTER FIXED can be a quick way of achieving three common filtering operations: hi-pass (Mode 1: cut below), lo-pass (Mode 2: cut above), or notch (Mode 3: cut around), given a precise knowledge of the frequency area to be filtered and an acceptance of a fixed, average amount of Q." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "boost/cut", min = -96, max = 96, def = 0, tip = "boost/cut – amplitude boost or cut, in dB" },
  arg4 = { name = "frequency", min = 0, max = 22050, tip = "the frequency of the filter in Hz" },
  arg5 = { name = "preScale", switch = "-s", min = 0, max = 1, tip = "scales gain on the input to the filter" },
}
 
dsp["Filter Fixed 2 - Boost or Cut above a given frequency "] = {
  cmds = { exe = "filter", mode = "fixed 2", channels = "any", tip = "This filter is referred to as 'fixed' because a single frequency of fixed amplitude roll-off (Q), is preset within the function. FILTER FIXED requires very precise frequency inputs. It can therefore be useful to analyse the sound and use SPECINFO PEAK get a profile of the frequency bands in which most energy occurs. For example, a trombone sound playing an F below Middle-C (174.61 Hz) showed almost no change when this frequency was used as the input to FILTER FIXED. SPECINFO PEAK showed that the energy was concentrated between 640 Hz and 905 Hz. Mode 1 and Mode 2 inputs relating to these frequencies worked fine: e.g., filtering out below 905 Hz or above 640 Hz really made a difference; similarly, filtering out a 200 Hz band centered on 700 Hz. Musical applications; FILTER FIXED can be a quick way of achieving three common filtering operations: hi-pass (Mode 1: cut below), lo-pass (Mode 2: cut above), or notch (Mode 3: cut around), given a precise knowledge of the frequency area to be filtered and an acceptance of a fixed, average amount of Q." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "boost/cut", min = -96, max = 96, def = 0, tip = "boost/cut – amplitude boost or cut, in dB" },
  arg4 = { name = "frequency", min = 0, max = 22050, tip = "the frequency of the filter in Hz" },
  arg5 = { name = "preScale", switch = "-s", min = 0, max = 1, tip = "scales gain on the input to the filter" },
}
 
dsp["Filter Fixed 3 - Boost or Cut a band centered on a given frequency "] = {
  cmds = { exe = "filter", mode = "fixed 3", channels = "any", tip = "This filter is referred to as 'fixed' because a single frequency of fixed amplitude roll-off (Q), is preset within the function. FILTER FIXED requires very precise frequency inputs. It can therefore be useful to analyse the sound and use SPECINFO PEAK get a profile of the frequency bands in which most energy occurs. For example, a trombone sound playing an F below Middle-C (174.61 Hz) showed almost no change when this frequency was used as the input to FILTER FIXED. SPECINFO PEAK showed that the energy was concentrated between 640 Hz and 905 Hz. Mode 1 and Mode 2 inputs relating to these frequencies worked fine: e.g., filtering out below 905 Hz or above 640 Hz really made a difference; similarly, filtering out a 200 Hz band centered on 700 Hz. Musical applications; FILTER FIXED can be a quick way of achieving three common filtering operations: hi-pass (Mode 1: cut below), lo-pass (Mode 2: cut above), or notch (Mode 3: cut around), given a precise knowledge of the frequency area to be filtered and an acceptance of a fixed, average amount of Q." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "bandwidth", min = 0, max = 22050, tip = "filter bandwidth in Hz" },
  arg4 = { name = "boost/cut", min = -96, max = 96, def = 0, tip = "boost/cut – amplitude boost or cut, in dB" },
  arg5 = { name = "frequency", min = 0, max = 22050, tip = "the frequency of the filter in Hz" },
  arg6 = { name = "preScale", switch = "-s", min = 0, max = 1, tip = "scales gain on the input to the filter" },
}
 
dsp["Filter Iterated 1 - Iterate a sound, with cumulative filtering by a filterbank specified in a data text file"] = {
  cmds = { exe = "filter", mode = "iterated 1", channels = "any", tip = "1 Enter filter pitches as frequency, in Hz. Note that the duration of the output soundfile is set with dur, it will usually be considerably longer than infile to provide space in which the iterations can occur. In effect, this determines the number of iterations there will be, given the value for delay, the time between iterations. If you receive a message about 'insufficient memory', you can increase the buffer capacity by increasing the buffer size. This is done by setting the environment variable CDP_MEMORY_BBSIZE. This is 1 megabyte by default. The units are 1K each, so a buffer size of e.g., 6 megabytes can be set with the phrase: 'set CDP_MEMORY_BBSIZE=6000'. Note that there must NOT be spaces before and after the equals sign. This can be done in the existing DOS window, or in autoexec.bat so that it sets this size upon boot-up. If the latter, the new size will not come into force until you re-boot. More iterations means that the sound will be more filtered by the time it reaches the end. This has an important relationship to Q, because the higher the Q, the more the original sound material disappears into a pure and clean resonance as the sound progresses. Similarly, this resonance can be made to modulate nicely by using some pitch transposition with pshift, producing a weaving of differently tuned strands. Delay is perhaps the key parameter, because it determines the degree of overlap with each overlap – or no overlap if it is longer than infile. A long dur (many iterations) with a delay longer than infile means a series of differently filtered repeats, with gaps between them. Higher values of rand, will close some of these gaps by introducing random overlaps. However, with no or occasional overlap, there will be no (or very little) resonance effect. NB: Each iteration begins at the beginning of infile and plays delay amount of infile, before the next iteration commences over the top of it. How your source sound begins will greatly affect the results, especially the sharpness of attack and initial amplitude. Normally, it should have a high amplitude in order to maximise signal as the filtering progresses. A soundfile with a strong attack and a short delay (e.g., 0.2 or 0.1 sec) will produce a series of pulsations which gradually disappear into resonance, depending on Q. It takes quite a low Q (e.g., well under 10) to hold off the resonance effect for a bit. The timing of the transition from the source sound to a resonant effect is handled by balancing dur, delay and Q. FILTER ITERATED is processor intensive. Very short values for delay will increase the processing time, though this does not appear to affect the demands on available RAM. Musical applications; The results achieved with FILTER ITERATED can vary widely. Here is a listing of some of the main effects: long dur – many iterations and plenty of time for the repeated filtering to alter the sound, very short delay (< 0.05) – a granular sound which gradually becomes a rapidly pulsating resonance; add some pshift and it may become a soft, modulating wash, short delay (e.g., ca 0.1 or 0.2) – rapidfire pulsations, the use of exponential decay (-e) will increase the sense of pulsation by dipping the amplitude between iterations, long delay – gross iterations of the beginning of the sound, becoming a bit softer as the filtering takes effect, delay longer than infile – delay amounts of infile with gaps between each iteration; add some rand factor and there will be some random overlapping of segments, high Q – the tendency to dissolve into resonance will be considerably greater, high Q with pshift – the resonance produced as the sound progresses will tend to warble and weave tuned strands " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "contains the pitch and amplitude of the filters (paired, one pair on each line) " },
  arg4 = { name = "Q", min = 0.01, max = 10000, input = "brk", def = 5000, tip = "'tightness' of filters (Range: 0.00100 <= Q < 10000) – the higher the value the more tightly the filter is focused on the centre frequency of that filter." },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.00100 to 10000.0)" },
  arg6 = { name = "delay", min = 0, max = 60, def = 1.5, tip = "average delay in seconds between iterations" },
  arg7 = { name = "dur", min = 0, max = 60, def = 10, tip = "(min) duration of output file (must be longer than infile" },
  arg8 = { name = "prescale", switch = "-s", min = 0, max = 1, def = 1, tip = "scales gain on the input to the filtering process (Range: 0.0 to 1.0; the Default is 1.0)" },
  arg9 = { name = "rand", switch = "-r", min = 0, max = 1, tip = "randomisation of the delay time (Range: 0 [none] to 1 [max])" },
  arg10 = { name = "pshift", switch = "-p", min = 0, max = 100, tip = "maxmimum pitch shift of any segment in (fractions of) semitones (Range: >= 0)" },
  arg11 = { name = "ashift", switch = "-a", min = 0, max = 1, tip = "maximum amplitude reduction of any segment (Range: 0.0 to 1.0)" },
  arg12 = { name = "-d", switch = "-d-", tip = "double filtering" },
  arg13 = { name = "-i", switch = "-i", tip = "turn off interpolation during filtering (makes it fast, but dirty)" },
  arg14 = { name = "-e", switch = "-e", tip = "add exponential decay: each segment gets quieter before the next segment enters" },
  arg15 = { name = "-n", switch = "-n", tip = "turn off normalisation: segments may grow or fall in amplitude quickly; gain settings will have more impact when -n is used – but use this cautiously, as overload may occur, which FILTER ITERATED does not report. " },
}
 
dsp["Filter Iterated 2 - Iterate a sound, with cumulative filtering by a filterbank specified in a data text file"] = {
  cmds = { exe = "filter", mode = "iterated 2", channels = "any", tip = "2 Enter filter pitches as MIDI note values. Note that the duration of the output soundfile is set with dur, it will usually be considerably longer than infile to provide space in which the iterations can occur. In effect, this determines the number of iterations there will be, given the value for delay, the time between iterations. If you receive a message about 'insufficient memory', you can increase the buffer capacity by increasing the buffer size. This is done by setting the environment variable CDP_MEMORY_BBSIZE. This is 1 megabyte by default. The units are 1K each, so a buffer size of e.g., 6 megabytes can be set with the phrase: 'set CDP_MEMORY_BBSIZE=6000'. Note that there must NOT be spaces before and after the equals sign. This can be done in the existing DOS window, or in autoexec.bat so that it sets this size upon boot-up. If the latter, the new size will not come into force until you re-boot. More iterations means that the sound will be more filtered by the time it reaches the end. This has an important relationship to Q, because the higher the Q, the more the original sound material disappears into a pure and clean resonance as the sound progresses. Similarly, this resonance can be made to modulate nicely by using some pitch transposition with pshift, producing a weaving of differently tuned strands. Delay is perhaps the key parameter, because it determines the degree of overlap with each overlap – or no overlap if it is longer than infile. A long dur (many iterations) with a delay longer than infile means a series of differently filtered repeats, with gaps between them. Higher values of rand, will close some of these gaps by introducing random overlaps. However, with no or occasional overlap, there will be no (or very little) resonance effect. NB: Each iteration begins at the beginning of infile and plays delay amount of infile, before the next iteration commences over the top of it. How your source sound begins will greatly affect the results, especially the sharpness of attack and initial amplitude. Normally, it should have a high amplitude in order to maximise signal as the filtering progresses. A soundfile with a strong attack and a short delay (e.g., 0.2 or 0.1 sec) will produce a series of pulsations which gradually disappear into resonance, depending on Q. It takes quite a low Q (e.g., well under 10) to hold off the resonance effect for a bit. The timing of the transition from the source sound to a resonant effect is handled by balancing dur, delay and Q. FILTER ITERATED is processor intensive. Very short values for delay will increase the processing time, though this does not appear to affect the demands on available RAM. Musical applications; The results achieved with FILTER ITERATED can vary widely. Here is a listing of some of the main effects: long dur – many iterations and plenty of time for the repeated filtering to alter the sound, very short delay (< 0.05) – a granular sound which gradually becomes a rapidly pulsating resonance; add some pshift and it may become a soft, modulating wash, short delay (e.g., ca 0.1 or 0.2) – rapidfire pulsations, the use of exponential decay (-e) will increase the sense of pulsation by dipping the amplitude between iterations, long delay – gross iterations of the beginning of the sound, becoming a bit softer as the filtering takes effect, delay longer than infile – delay amounts of infile with gaps between each iteration; add some rand factor and there will be some random overlapping of segments, high Q – the tendency to dissolve into resonance will be considerably greater, high Q with pshift – the resonance produced as the sound progresses will tend to warble and weave tuned strands " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "contains the pitch and amplitude of the filters (paired, one pair on each line) " },
  arg4 = { name = "Q", min = 0.01, max = 10000, input = "brk", def = 5000, tip = "'tightness' of filters (Range: 0.00100 <= Q < 10000) – the higher the value the more tightly the filter is focused on the centre frequency of that filter." },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.00100 to 10000.0)" },
  arg6 = { name = "delay", min = 0, max = 60, def = 1.5, tip = "average delay in seconds between iterations" },
  arg7 = { name = "dur", min = 0, max = 60, def = 10, tip = "(min) duration of output file (must be longer than infile" },
  arg8 = { name = "prescale", switch = "-s", min = 0, max = 1, def = 1, tip = "scales gain on the input to the filtering process (Range: 0.0 to 1.0; the Default is 1.0)" },
  arg9 = { name = "rand", switch = "-r", min = 0, max = 1, tip = "randomisation of the delay time (Range: 0 [none] to 1 [max])" },
  arg10 = { name = "pshift", switch = "-p", min = 0, max = 100, tip = "maxmimum pitch shift of any segment in (fractions of) semitones (Range: >= 0)" },
  arg11 = { name = "ashift", switch = "-a", min = 0, max = 1, tip = "maximum amplitude reduction of any segment (Range: 0.0 to 1.0)" },
  arg12 = { name = "-d", switch = "-d-", tip = "double filtering" },
  arg13 = { name = "-i", switch = "-i", tip = "turn off interpolation during filtering (makes it fast, but dirty)" },
  arg14 = { name = "-e", switch = "-e", tip = "add exponential decay: each segment gets quieter before the next segment enters" },
  arg15 = { name = "-n", switch = "-n", tip = "turn off normalisation: segments may grow or fall in amplitude quickly; gain settings will have more impact when -n is used – but use this cautiously, as overload may occur, which FILTER ITERATED does not report. " },
}
 
dsp["Filter Lohi - 1 Pass-band and stop-band are given as frequency in Hz. Fixed low pass or high pass filter  "] = {
  cmds = { exe = "filter", mode = "lohi 1", channels = "any", tip = "1 Pass-band and stop-band are given as frequency in Hz. The lo-pass filter 'lets through' all of the sound below pass-band and attenuates (makes gradually softer) the frequencies higher than pass-band, ending at stop-band. The high-pass filter 'lets through' all of the sound above pass-band and attenuates (makes gradually softer) the frequencies lower than pass-band, ending at stop-band. The key is how the pass-band and stop-band are placed against the significant frequencies of the source sound. As mentioned elsewhere, it may be useful to perform a spectral analysis on infile and make a report with SPECINFO PEAK. An interesting fact about the operation of the filters has recently been observed. With the hipass-lopass filter (FILTER LOHI – and possibly with other filters?), if your source file has LOTS of silence at the end, the filter suddenly slows down by a factor of 20 (or more!!) in the silent buffers. At the moment, we don't know why this happens – there's no known bug in the code. So you are recommended to 'topntail' (ENVEL DOVETAIL) any long sounds that fade away to nothing before running them through filters. Musical applications; These basic filter operations can be used to clean up a sound by removing excessive bass with a high-pass filter, or some hiss (if it is above significant frequencies in the source) with a low-pass filter.Musically, a high-pass filter makes the sound 'thin' by removing the bottom, and a low-pass filter makes the sound deep and rumbling by removing the top." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "attenuation", min = -96, max = 0, tip = "gain reduction of the filter, in dB (Range: 0 to -96)" },
  arg4 = { name = "pass-band", min = 0, max = 22050, def = 0, tip = "last pitch to be passed by the filter" },
  arg5 = { name = "stop-band", min = 0, max = 22050, tip = "first pitch to be stopped by the filter" },
  arg6 = { name = "preScale", switch = "-s", min = 0.005, max = 200, tip = "apply gain on the input to the filtering process – avoid overflows (Range: 0.005 to 200.0)" },
}

dsp["Filter Lohi - 2 Pass-band and stop-band are given as (possibly fractional) MIDI note values. Fixed low pass or high pass filter  "] = {
  cmds = { exe = "filter", mode = "lohi 2", channels = "any", tip = "2 Pass-band and stop-band are given as (possibly fractional) MIDI note values. The lo-pass filter 'lets through' all of the sound below pass-band and attenuates (makes gradually softer) the frequencies higher than pass-band, ending at stop-band. The high-pass filter 'lets through' all of the sound above pass-band and attenuates (makes gradually softer) the frequencies lower than pass-band, ending at stop-band. The key is how the pass-band and stop-band are placed against the significant frequencies of the source sound. As mentioned elsewhere, it may be useful to perform a spectral analysis on infile and make a report with SPECINFO PEAK. An interesting fact about the operation of the filters has recently been observed. With the hipass-lopass filter (FILTER LOHI – and possibly with other filters?), if your source file has LOTS of silence at the end, the filter suddenly slows down by a factor of 20 (or more!!) in the silent buffers. At the moment, we don't know why this happens – there's no known bug in the code. So you are recommended to 'topntail' (ENVEL DOVETAIL) any long sounds that fade away to nothing before running them through filters. Musical applications; These basic filter operations can be used to clean up a sound by removing excessive bass with a high-pass filter, or some hiss (if it is above significant frequencies in the source) with a low-pass filter.Musically, a high-pass filter makes the sound 'thin' by removing the bottom, and a low-pass filter makes the sound deep and rumbling by removing the top." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "attenuation", min = -96, max = 0, tip = "gain reduction of the filter, in dB (Range: 0 to -96)" },
  arg4 = { name = "pass-band", min = 3.486820, max = 127, def = 40, tip = "last pitch to be passed by the filter" },
  arg5 = { name = "stop-band", min = 3.486820, max = 127, def = 99,  tip = "first pitch to be stopped by the filter" },
  arg6 = { name = "preScale", switch = "-s", min = 0.005, max = 200, tip = "apply gain on the input to the filtering process – avoid overflows (Range: 0.005 to 200.0)" },
}
 
dsp["Filter Phasing 1 - Allpass Filter"] = {
  cmds = { exe = "filter", mode = "phasing 1", channels = "any", tip = "The 'phasing effect' is a kind of sweeping band passing through the sound, such as is sometimes heard when aeroplanes fly overhead, and is much used in popular music.Increasing the gain factor has some effect on the reverberant quality of the sound. So does increasing the delay time. The sound will still sound quite dry with delay < 20 (ms). Between about 20 and 45 ms there is a touch of resonance in the sound. Around 50 ms there is significant echoey reverberation (though somewhat granulated), and after 100 ms we start to hear larger portions of the sound repeating.Overall, the degree of reverberant effect is controlled by increasing both the gain and delay parameters in tandem" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Gain", min = -1, max = 1, },
  arg4 = { name = "Delay", min = 0.045, max = 1644.35, input = "brk", },
  arg5 = { name = "PreScale", switch = "-s", min = 0, max = 1, },
  arg6 = { name = "Linear", switch = "-l", },
}
 
dsp["Filter Phasing 2 - Phasing Effect"] = {
  cmds = { exe = "filter", mode = "phasing 2", channels = "any", tip = "The 'phasing effect' is a kind of sweeping band passing through the sound, such as is sometimes heard when aeroplanes fly overhead, and is much used in popular music.Increasing the gain factor has some effect on the reverberant quality of the sound. So does increasing the delay time. The sound will still sound quite dry with delay < 20 (ms). Between about 20 and 45 ms there is a touch of resonance in the sound. Around 50 ms there is significant echoey reverberation (though somewhat granulated), and after 100 ms we start to hear larger portions of the sound repeating.Overall, the degree of reverberant effect is controlled by increasing both the gain and delay parameters in tandem" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Gain", min = -1, max = 1, },
  arg4 = { name = "Delay", min = 0.045, max = 1644.35, input = "brk", },
  arg5 = { name = "PreScale", switch = "-s", min = 0, max = 1, },
  arg6 = { name = "Linear", switch = "-l", },
}
 
dsp["Filter Sweeping 1 - High Pass"] = {
  cmds = { exe = "filter", mode = "sweeping 1", channels = "any", tip = "This filter is most effective in Band-pass or Low-pass mode, with low acuity. It is difficult to get a result in High-pass mode. A tight filter makes the sweep more audible, but too tight and it may appear as a thin sine wave (resonance) which doesn't really connect with the sound.A fairly long soundfile and a fairly broad range of frequencies gives this function something to work with. Watch for overflows and reduce the gain as necessary." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Acuity", min = 0.0001, max = 1, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/2, input = "brk", },
  arg6 = { name = "HiFrq", min = 10, max = srate/2, input = "brk", },
  arg7 = { name = "SweepFrq", min = 0, max = 200, input = "brk", },
  arg8 = { name = "Phase", switch = "-p", min = 0, max = 1, },
}
 
dsp["Filter Sweeping 2 - Low Pass"] = {
  cmds = { exe = "filter", mode = "sweeping 2", channels = "any", tip = "This filter is most effective in Band-pass or Low-pass mode, with low acuity. It is difficult to get a result in High-pass mode. A tight filter makes the sweep more audible, but too tight and it may appear as a thin sine wave (resonance) which doesn't really connect with the sound.A fairly long soundfile and a fairly broad range of frequencies gives this function something to work with. Watch for overflows and reduce the gain as necessary." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Acuity", min = 0.0001, max = 1, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/2, input = "brk", },
  arg6 = { name = "HiFrq", min = 10, max = srate/2, input = "brk", },
  arg7 = { name = "SweepFrq", min = 0, max = 200, input = "brk", },
  arg8 = { name = "Phase", switch = "-p", min = 0, max = 1, },
}
 
dsp["Filter Sweeping 3 - Band Pass"] = {
  cmds = { exe = "filter", mode = "sweeping 3", channels = "any", tip = "This filter is most effective in Band-pass or Low-pass mode, with low acuity. It is difficult to get a result in High-pass mode. A tight filter makes the sweep more audible, but too tight and it may appear as a thin sine wave (resonance) which doesn't really connect with the sound.A fairly long soundfile and a fairly broad range of frequencies gives this function something to work with. Watch for overflows and reduce the gain as necessary." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Acuity", min = 0.0001, max = 1, input = "brk", },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/2, input = "brk", },
  arg6 = { name = "HiFrq", min = 10, max = srate/2, input = "brk", },
  arg7 = { name = "SweepFrq", min = 0, max = 200, input = "brk", },
  arg8 = { name = "Phase", switch = "-p", min = 0, max = 1, },
}
 
dsp["Filter Sweeping 4 - Notch"] = {
  cmds = { exe = "filter", mode = "sweeping 4", channels = "any", tip = "This filter is most effective in Band-pass or Low-pass mode, with low acuity. It is difficult to get a result in High-pass mode. A tight filter makes the sweep more audible, but too tight and it may appear as a thin sine wave (resonance) which doesn't really connect with the sound.A fairly long soundfile and a fairly broad range of frequencies gives this function something to work with. Watch for overflows and reduce the gain as necessary." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Acuity", min = 0.0001, max = 1, },
  arg4 = { name = "Gain", min = 0.001, max = 10000, },
  arg5 = { name = "LoFrq", min = 10, max = srate/2, },
  arg6 = { name = "HiFrq", min = 10, max = srate/2, },
  arg7 = { name = "SweepFrq", min = 0, max = 200, },
  arg8 = { name = "Phase", switch = "-p", min = 0, max = 1, },
}
 
dsp["Filter Userbank 1 - User-defined filterbank, with time-varying Q specified in data text file"] = {
  cmds = { exe = "filter", mode = "userbank 1", channels = "any", tip = "1 The pitches to filter are entered as frequency in Hz. Here Q may vary over time, but the frequency settings are fixed for the duration of the sound. The tightness of the Q determines how much of the original sound 'comes through', or, to put it another way, the degree to which the sound is tuned to the frequencies defined in the filter bank. Much of the effectiveness of the function therefore depends on the design of the datafile. Normally, dB amplitude settings will fall within a 0dB to -96dB range, but the function does allow dB greater than 0 (i.e., applying gain to the sound). If you have existing filter bank files with dB greater than 0 and they overload, you can use gain with values below zero to scale the amplitudes back within range – i.e., without having to edit your file. Musical applications; As with any graphic equaliser, this function can be used to emphasize or de-emphasize different frequency regions of the source sound. It can also be used to impart a harmonic sonority to a sound. The higher the Q, the clearer the pitched effect (e.g., values between 300 and 1000). Given a source with a rapidly changing amplitude envelope, this may come across as a series of harmonic scintillations, whereas a sound with a steadier amplitude envelope will become a chord." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing the pitch and amplitude of the filters (paired, one pair on each line) " },
  arg4 = { name = "Q", min = 0.001, max = 10000, input = "brk", def = 5000, tip = "tightness of filter (Range: 0.001000 to 10000.0) – the higher the value the more tightly the filter is focused on the centre frequency of that filter" },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.001000 to 10000.0)" },
  arg6 = { name = "-d", switch = "-d", tip = "double filtering" },
}
 
dsp["Filter Userbank 2 - User-defined filterbank, with time-varying Q specified in data text file"] = {
  cmds = { exe = "filter", mode = "userbank 2", channels = "any", tip = "2 The pitches to filter are entered as MIDI note values. Here Q may vary over time, but the frequency settings are fixed for the duration of the sound. The tightness of the Q determines how much of the original sound 'comes through', or, to put it another way, the degree to which the sound is tuned to the frequencies defined in the filter bank. Much of the effectiveness of the function therefore depends on the design of the datafile. Normally, dB amplitude settings will fall within a 0dB to -96dB range, but the function does allow dB greater than 0 (i.e., applying gain to the sound). If you have existing filter bank files with dB greater than 0 and they overload, you can use gain with values below zero to scale the amplitudes back within range – i.e., without having to edit your file. Musical applications; As with any graphic equaliser, this function can be used to emphasize or de-emphasize different frequency regions of the source sound. It can also be used to impart a harmonic sonority to a sound. The higher the Q, the clearer the pitched effect (e.g., values between 300 and 1000). Given a source with a rapidly changing amplitude envelope, this may come across as a series of harmonic scintillations, whereas a sound with a steadier amplitude envelope will become a chord." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing the pitch and amplitude of the filters (paired, one pair on each line) " },
  arg4 = { name = "Q", min = 0.001, max = 10000, input = "brk", def = 5000, tip = "tightness of filter (Range: 0.001000 to 10000.0) – the higher the value the more tightly the filter is focused on the centre frequency of that filter" },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.001000 to 10000.0)" },
  arg6 = { name = "-d", switch = "-d", tip = "double filtering" },
}
 
dsp["Filter Variable - Bandpass"] = {
  cmds = { exe = "filter", mode = "variable 3", channels = "any", tip = "The acuity parameter controls the rate at which the amplitude decreases as one moves away from frq. The faster the rate, or, to put it more technically, the steeper the slope of the roll-off, the more the resulting sound is focused on frq (Band-pass), or the area above (Hi-pass) or below (Lo-pass) frq, or omits the area around frq (Notch). It is best not to make the acuity too tight with this filter, as the resonance produced may become unpredictable. Hi-pass and Lo-pass are not very effective in FILTER VARIABLE, which has been designed as a 'lightweight', fast filter. For more powerful Hi-pass and Lo-pass filtering, it is recommended that you use FILTER LOHI. Because a filter often creates resonance (boosts the amplitude of frequencies in a certain frequency area), especially with a broader roll-off region, overload can easily occur. FILTER VARIABLE reports any overflows, in which case the outfile should be deleted without being played and the function run again with a reduction in the gain. Musical applications; FILTER VARIABLE provides a convenient environment for exploring the effect of a variety of filter types on a given sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "acuity", min = 0.0001, max = 1, input = "brk", tip = "tightness of the filter (Range: 0.000100 to 1.0) Smaller values give a tighter filter." },
  arg4 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain on output (Range: 0.001000 to 10000.0)" },
  arg5 = { name = "freq", min = 10, max = srate/2, input = "brk", tip = "frequency of the filter (Range: 10.0 to sample_rate/2)" },
}
 
dsp["Filter Variable - High-pass"] = {
  cmds = { exe = "filter", mode = "variable 1", channels = "any", tip = "The acuity parameter controls the rate at which the amplitude decreases as one moves away from frq. The faster the rate, or, to put it more technically, the steeper the slope of the roll-off, the more the resulting sound is focused on frq (Band-pass), or the area above (Hi-pass) or below (Lo-pass) frq, or omits the area around frq (Notch). It is best not to make the acuity too tight with this filter, as the resonance produced may become unpredictable. Hi-pass and Lo-pass are not very effective in FILTER VARIABLE, which has been designed as a 'lightweight', fast filter. For more powerful Hi-pass and Lo-pass filtering, it is recommended that you use FILTER LOHI. Because a filter often creates resonance (boosts the amplitude of frequencies in a certain frequency area), especially with a broader roll-off region, overload can easily occur. FILTER VARIABLE reports any overflows, in which case the outfile should be deleted without being played and the function run again with a reduction in the gain. Musical applications; FILTER VARIABLE provides a convenient environment for exploring the effect of a variety of filter types on a given sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "acuity", min = 0.0001, max = 1, input = "brk", tip = "tightness of the filter (Range: 0.000100 to 1.0) Smaller values give a tighter filter." },
  arg4 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain on output (Range: 0.001000 to 10000.0)" },
  arg5 = { name = "freq", min = 10, max = srate/2, input = "brk", tip = "frequency of the filter (Range: 10.0 to sample_rate/2)" },
}
 
dsp["Filter Variable - Lowpass"] = {
  cmds = { exe = "filter", mode = "variable 2", channels = "any", tip = "The acuity parameter controls the rate at which the amplitude decreases as one moves away from frq. The faster the rate, or, to put it more technically, the steeper the slope of the roll-off, the more the resulting sound is focused on frq (Band-pass), or the area above (Hi-pass) or below (Lo-pass) frq, or omits the area around frq (Notch). It is best not to make the acuity too tight with this filter, as the resonance produced may become unpredictable. Hi-pass and Lo-pass are not very effective in FILTER VARIABLE, which has been designed as a 'lightweight', fast filter. For more powerful Hi-pass and Lo-pass filtering, it is recommended that you use FILTER LOHI. Because a filter often creates resonance (boosts the amplitude of frequencies in a certain frequency area), especially with a broader roll-off region, overload can easily occur. FILTER VARIABLE reports any overflows, in which case the outfile should be deleted without being played and the function run again with a reduction in the gain. Musical applications; FILTER VARIABLE provides a convenient environment for exploring the effect of a variety of filter types on a given sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "acuity", min = 0.0001, max = 1, input = "brk", tip = "tightness of the filter (Range: 0.000100 to 1.0) Smaller values give a tighter filter." },
  arg4 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain on output (Range: 0.001000 to 10000.0)" },
  arg5 = { name = "freq", min = 10, max = srate/2, input = "brk", tip = "frequency of the filter (Range: 10.0 to sample_rate/2)" },
}
 
dsp["Filter Variable - Notch (Band reject)"] = {
  cmds = { exe = "filter", mode = "variable 4", channels = "any", tip = "The acuity parameter controls the rate at which the amplitude decreases as one moves away from frq. The faster the rate, or, to put it more technically, the steeper the slope of the roll-off, the more the resulting sound is focused on frq (Band-pass), or the area above (Hi-pass) or below (Lo-pass) frq, or omits the area around frq (Notch). It is best not to make the acuity too tight with this filter, as the resonance produced may become unpredictable. Hi-pass and Lo-pass are not very effective in FILTER VARIABLE, which has been designed as a 'lightweight', fast filter. For more powerful Hi-pass and Lo-pass filtering, it is recommended that you use FILTER LOHI. Because a filter often creates resonance (boosts the amplitude of frequencies in a certain frequency area), especially with a broader roll-off region, overload can easily occur. FILTER VARIABLE reports any overflows, in which case the outfile should be deleted without being played and the function run again with a reduction in the gain. Musical applications; FILTER VARIABLE provides a convenient environment for exploring the effect of a variety of filter types on a given sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "acuity", min = 0.0001, max = 1, input = "brk", tip = "tightness of the filter (Range: 0.000100 to 1.0) Smaller values give a tighter filter." },
  arg4 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain on output (Range: 0.001000 to 10000.0)" },
  arg5 = { name = "freq", min = 10, max = srate/2, input = "brk", tip = "frequency of the filter (Range: 10.0 to sample_rate/2)" },
}
 
dsp["Filter Varibank 1 - User-defined time-varying filterbank, with time-varying Q specified in data text file"] = {
  cmds = { exe = "filter", mode = "varibank 1", channels = "any", tip = "1 The pitches to filter are entered as frequency in Hz. The frequencies specified in the datafile are in fact the centre-frequencies of a filter band. With a tight Q, these specific frequencies will be heard as pitches; with a relaxed Q, these specific frequencies will locate a relatively fuzzy pitch region. The pitch will glissando between different pitch levels when these differ between time points. Note that if the datafile has, for example, 10 Pitch/Amplitude pairs on a line, each column of paired values relates to a given 'band'. Thus the change over time for each band will be specified by the sequence of paired values in a given column. The value for Q is not contained in the datafile, but the time points for changes in Q can be made to match or vary from those in the datafile. In the additional lines supplied for the FILTER VARIBANK2 option, the first value in each line is a time. The following values are in pairs representing the partials (they can in fact be ANY number) and their relative amplitude. Note that as with the frequency (or MIDI) data already given, there must be the same number of entries in each line. If you want to omit a partial at given times, give it an amplitude of 0. Musical applications; The facilities of FILTER VARIBANK are a composer's dream. Not only can the frequencies of the filterbank be specified in precise detail, but the frequencies may move in time. Not only can the tightness of the filters – the degree to which they focus on pitches – be specified, but this focus can also change over time. The number of harmonics and the amplitude 'rolloff' are also under the composer's control. It is possible, therefore, to play with relationships between pitched tones and complex sounds with some degree of noise components. Working with sounds accepts all kinds of sonic material into the fabric of musical discourse – but pitch need not be excluded simply because a broader range of materials is being used. A tool like FILTER VARIBANK makes it possible gradually to focus complex sonic material into specific pitches, thus creating links with other pitched elements, such as a broader harmonic scheme and the use of acoustic musical instruments." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing lines of data for filter bands at successive times, see docs for syntax" },
  arg4 = { name = "Q", min = 0.001, max = 10000, input = "brk", def = 5000, tip = "tightness of filter (Range: 0.001000 to 10000.0) – the higher the value the more tightly the filter is focused on the centre frequency of that filter" },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.001000 to 10000.0)" },
  arg6 = { name = "hcount", switch = "-h", min = 0, max = 100, def = 1, tip = "number of harmonics of each pitch to use (Default: 1)" },
  arg7 = { name = "rolloff", switch = "-r", min = -96, max = 0, tip = "decrease in amplitude level in dB from one harmonic to the next (Range: 0 to -96)" },
  arg8 = { name = "-d", switch = "-d", tip = "double filtering" },
}
 
dsp["Filter Varibank 2 - User-defined time-varying filterbank, with time-varying Q specified in data text file"] = {
  cmds = { exe = "filter", mode = "varibank 2", channels = "any", tip = "2 The pitches to filter are entered as MIDI note values. The frequencies specified in the datafile are in fact the centre-frequencies of a filter band. With a tight Q, these specific frequencies will be heard as pitches; with a relaxed Q, these specific frequencies will locate a relatively fuzzy pitch region. The pitch will glissando between different pitch levels when these differ between time points. Note that if the datafile has, for example, 10 Pitch/Amplitude pairs on a line, each column of paired values relates to a given 'band'. Thus the change over time for each band will be specified by the sequence of paired values in a given column. The value for Q is not contained in the datafile, but the time points for changes in Q can be made to match or vary from those in the datafile. In the additional lines supplied for the FILTER VARIBANK2 option, the first value in each line is a time. The following values are in pairs representing the partials (they can in fact be ANY number) and their relative amplitude. Note that as with the frequency (or MIDI) data already given, there must be the same number of entries in each line. If you want to omit a partial at given times, give it an amplitude of 0. Musical applications; The facilities of FILTER VARIBANK are a composer's dream. Not only can the frequencies of the filterbank be specified in precise detail, but the frequencies may move in time. Not only can the tightness of the filters – the degree to which they focus on pitches – be specified, but this focus can also change over time. The number of harmonics and the amplitude 'rolloff' are also under the composer's control. It is possible, therefore, to play with relationships between pitched tones and complex sounds with some degree of noise components. Working with sounds accepts all kinds of sonic material into the fabric of musical discourse – but pitch need not be excluded simply because a broader range of materials is being used. A tool like FILTER VARIBANK makes it possible gradually to focus complex sonic material into specific pitches, thus creating links with other pitched elements, such as a broader harmonic scheme and the use of acoustic musical instruments." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing lines of data for filter bands at successive times, see docs for syntax" },
  arg4 = { name = "Q", min = 0.001, max = 10000, input = "brk", def = 5000, tip = "tightness of filter (Range: 0.001000 to 10000.0) – the higher the value the more tightly the filter is focused on the centre frequency of that filter" },
  arg5 = { name = "gain", min = 0.001, max = 10000, def = 1, tip = "overall gain (Range: 0.001000 to 10000.0)" },
  arg6 = { name = "hcount", switch = "-h", min = 0, max = 100, def = 1, tip = "number of harmonics of each pitch to use (Default: 1)" },
  arg7 = { name = "rolloff", switch = "-r", min = -96, max = 0, tip = "decrease in amplitude level in dB from one harmonic to the next (Range: 0 to -96)" },
  arg8 = { name = "-d", switch = "-d", tip = "double filtering" },
}
 
dsp["Filter Vfilters - Make datafiles for fixed-pitch FILTER VARIBANK filters "] = {
  cmds = { exe = "filter", mode = "vfilters", tip = "FILTER VFILTERS creates one or more data file(s) for the FILTER VARIBANK process from a list of MIDI Pitch Values (MPV's), one file for each line of input. The output basic filterbank file is a fixed (not time-varying) pitches filter. If the inmidifile is: 48 63.5 70 74 81,The second line with time = 10000 means that the filter values given extend (without changing) from 0 seconds to 10000 seconds (i.e., the end of the file). The program adds it because it needs to have two time values in a varibank data file. It actually doesn't matter whether the values in inmidifile are MIDI or frequency values. The input is just treated as numbers which go into the output. If you put in what you think is MIDI, the output will be in MIDI. If you put in what you think is frequency, the output will be frequency. Many musicians find it easier to think of harmonies in the MIDI Pitch Value system, so this is illustrated here. These MIDI Pitch Values, please note, may be fractional (microtonal). Note that the data in VARIBANK filters can also be generated, and transposed from the Sound Loom Table Editor. BATCHFILES can be expanded to use a sequence of different values on the same, or a series of different sources, using the facilities in the Sound Loom Table Editor. Musical applications; FILTER VFILTERS will help you to generate data files for the FILTER VARIBANK process more quickly" },
  arg1 = { name = "inpitchfile", input = "txt", tip = "contains a list of MIDI Pitch Values or frequency pitch values, with one or more values on each line" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .frq file" },
}
 
------------------------------------------------
-- filtrage
------------------------------------------------
 
dsp["Filtrage - 1 Generate a fixed filter data file - Generate randomised VARIBANK filterbank files"] = {
  cmds = { exe = "filtrage", mode = "filtrage 1", tip = "1 Generate a fixed filter data file. Filtrage produces a filter data file for FILTER VARIBANK, with random frequencies. These are distributed within the desired frequency range either evenly, or skewed toward the top or bottom of the range." },
  arg1 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
  arg2 = { name = "dur", min = 0, max = 30, def = 10, tip = "the duration spanned by the output filter file" },
  arg3 = { name = "cnt", min = 1, max = 1000, def = 2, tip = "the number of parallel filters" },
  arg4 = { name = "min", min = 0, max = 250, def = 64, tip = "the minimum MIDI value for the filters " },
  arg5 = { name = "max", min = 0, max = 250, def = 127, tip = "the maximum MIDI value for the filters" },
  arg6 = { name = "distrib", min = 0.01, max = 100, def = 0.1, tip = "the distribution of pitch values" },
  arg7 = { name = "rand", min = 0, max = 1, def = 0.5, tip = "randomisation of the filter pitches" },
  arg8 = { name = "ampmin", min = 0, max = 1, tip = "the minimum filter amplitude. The maximum amplitude allowed here is = 1.0" },
  arg9 = { name = "amprand", min = 0, max = 1, def = 1, tip = "randomisation of the filter amplitudes" },
  arg10 = { name = "ampdistrib", min = -1, max = 1, def = 0, tip = "the distribution of the filter amplitudes" },
  arg11 = { name = "seed", switch = "-s", tip = "non-zero values fix the randomisation so that, on repeating the process identical random values are produced." },
}
 
dsp["Filtrage - 2 Generate a time-varying filter data file  - Generate randomised VARIBANK filterbank files"] = {
  cmds = { exe = "filtrage", mode = "filtrage 2", tip = "2 Generate a time-varying filter data file  Filtrage produces a filter data file for FILTER VARIBANK, with random frequencies. These are distributed within the desired frequency range either evenly, or skewed toward the top or bottom of the range." },
  arg1 = { name = "Output.txt", output = "txt", },
  arg2 = { name = "dur", min = 0, max = 30, def = 10, tip = "the duration spanned by the output filter file" },
  arg3 = { name = "cnt", min = 1, max = 1000, def = 2, tip = "the number of parallel filters" },
  arg4 = { name = "min", min = 0, max = 250, def = 64, tip = "the minimum MIDI value for the filters " },
  arg5 = { name = "max", min = 0, max = 250, def = 127, tip = "the maximum MIDI value for the filters" },
  arg6 = { name = "distrib", min = 0.01, max = 100, def = 0.1, tip = "the distribution of pitch values" },
  arg7 = { name = "rand", min = 0, max = 1, def = 0.5, tip = "randomisation of the filter pitches" },
  arg8 = { name = "ampmin", min = 0, max = 1, tip = "the minimum filter amplitude. The maximum amplitude allowed here is = 1.0" },
  arg9 = { name = "amprand", min = 0, max = 1, def = 1, tip = "randomisation of the filter amplitudes" },
  arg10 = { name = "ampdistrib", min = -1, max = 1, def = 0, tip = "the distribution of the filter amplitudes" },
  arg11 = { name = "timestep", min = 0, max = length/1000, tip = "the amount of time in seconds between each specified set of filter-pitches" },
  arg12 = { name = "timerand", min = 0, max = 10, def = 1, tip = "randomisation of the timestep" },
  arg13 = { name = "seed", switch = "-s", tip = "non-zero values fix the randomisation so that, on repeating the process identical random values are produced." },
}
 
------------------------------------------------
-- focus
------------------------------------------------
 
dsp["Focus Accu - Sustain (accumulate) each spectral band, until louder data appears in that band"] = {
  cmds = { exe = "focus", mode = "accu", tip = " Frequencies are sustained into subsequent windows. The overall effect is one of sustaining, but one which also makes the spectrum more complex. The gliss parameter appears to change the result the most. It produces glissandos within the spectrum of the sound. Very effective slow glissandos are produced when gliss is near 0, e.g., -0.9 or 0.1. At 0.5, there are several glissandos, at 1, they are fairly fast, and at 10 it becomes a wash. The range allowed for gliss is really very extreme, especially in the negative range below zero. Values e.g., of -5.0 or less lead to the sound settling down to a steady, sustained pitch after an initial short glisssando. Recommended effective range is from -0.5 to 10. The delay parameter is important for making the glissandos more perceptible. A low value of e.g., 0.1 is really very dry, whereas something like 0.75 brings out the glissandos nicely, with a kind of reverberant aura. Musical applications; The main type of output from this function creates a supple, internally modulating sound, moving slowly at the low end of the gliss range and increasingly quickly as one moves towards the high end. With a clearly pitched input, the results are aurally clearer, but otherwise similar. TIMESTRETCH applied to these results produces nicely slow-moving timbral and pitch changes. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "decay", switch = "-d", min = 1e-06, max = 1, def = 0.5, tip = "sustained channel data decays by a factor of decay each second" },
  arg4 = { name = "glis", switch = "-g", min = -11.7, max = 11.7, def = 0, tip = "sustained channel data glissandos at glis octaves per second" },
}
 
dsp["Focus Exag - Exaggerate spectral contour"] = {
  cmds = { exe = "focus", mode = "exag", tip = " FOCUS EXAG plays with the frequency focus of regions of high amplitude among the partials of a spectrum. A spectral peak is an area of high amplitude among the frequencies of the spectrum. A trough is an area of low amplitude between spectral peaks. Formants are slightly more than spectral peaks: i.e., they are the area around a spectral peak, and might be narrow, wide etc.). Narrowing the formants (< 1) increases the frequency focus of each peak area, and therefore acts like an increasingly tight filter. Widening the formants (> 1) reduces the frequency focus of the formants, and therefore makes the sound increasingly fuzzy. Musical applications; FOCUS EXAG can be used to make a sound thinner and pitched, just as a bandpass filter can create pitches within a complex sound. Working in the spectral domain, it does this by manipulating frequency regions of high amplitude, otherwise known as 'formants'. These can be visualised as little mountain peaks. 'Narrowing' the formants means making the 'mountains' narrower and its sides steeper; furthermore, the material inbetween virtually disappears. 'Widening' the formants means making the mountainsides slope more gradually, therefore encompassing more of the surrounding region. Because this is actually moving the position of partials, this results in timbral change and the addition of noise elements, hence the fuzziness refered to above. I have found that a value of exag = 0.5 maintains a nice balance between the original and the treated sound. Lower values start to produce results reminiscent of BLUR TRACE, while higher values, e.g., 10, produce quite a fuzzy, noisy result, at least with a complex sound as input. 5 with a pitched sound produced a light buzz sheen on it. It can also be used in connection with the extraction of a spectral envelope in order to alter the contour shape. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "exaggeration", min = 0.001, max = 1000, def = 0.5, tip = "< 1 widens troughs and narrows formants (focuses on the peaks); > 1 narrows troughs and widens formants (diffusing the peaks). Range: 0.00100 to 1000. " },
}
 
dsp["Focus Focus - Focus spectral energy onto the peaks in the spectrum "] = {
  cmds = { exe = "focus", mode = "focus", tip = " The program searches out the most prominent and persistent spectral peaks in a sound, creates a set of filters centred on those peaks, and then filters the sound with those filters. This focuses the spectral energy of the sound around those persistent spectral peaks. The channels and bandwidth parameters are both the key sensitive areas affecting the outcome. Increasing the number of channels moves the output towards a BLUR TRACE result, while wider bandwidths, as one would expect, 'let through' more of the original sound. I found that there was a good balance between original and processed sound with these parameter settings: Channels = 1, Peaks = 16, Bandwidth = 0.09, Lofrq = 20 and HiFrq = 20000. Musical applications; The results depend very much on the spectral configuration of the source, so this function needs to be explored on a 'try it and see' basis. On the whole, it appears to be an alternative approach to BLUR TRACE type effects, i.e., data reduction. However, trials with a pitched sound did produce fairly similar results. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 1, max = 256, def = 200, tip = " extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 10, def = 2, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "pk", min = 1, max = 16, def = 2, tip = "(maximum) number of peaks to find Range 1 – 16" },
  arg7 = { name = "bw", min = 0, max = 10, input = "brk", def = 2, tip = "bandwidth of peak-centred filters, in octaves" },
  arg8 = { name = "bt", switch = "-b", min = 0, max = 22050, input = "brk", def = 1000, tip = "bottom frequency at which to start the search for peaks" },
  arg9 = { name = "tp", switch = "-t", min = 0, max = 22050, input = "brk", def = 12000, tip = "top frequency at which to end the search for peaks" },
  arg10 = { name = "val", switch = "-s", min = 2, max = 4097, input = "brk", def = 9, tip = "the number of windows over which the peaks are averaged. This is an attempt to retain only peaks which are STABLE over time. Range: 2 – 4097" },
}
 
dsp["Focus Fold - Octave-transpose spectral components into a specified frequency range"] = {
  cmds = { exe = "focus", mode = "fold", tip = " This process octave transposes all the analysis data of a sound into a specified frequency range. Anything below the range is octave-transposed up until it is in (the bottom of) the range. Anything above gets octave-transposed down until it is in (the top of) the range. If more than two pieces of transposed information compete for the same channel in the resultant file, the data of the lowest amplitude is thrown away. Musical applications; The key here is the frequency band: high wide it is and where in the frequency range it is placed. Fairly narrow bands seem the most useful for this function, otherwise too much of the original comes through, so there is not very much change to the sound. A low-placed range such as 100 to 900 (Hz) produces a deep, muffled sound, with the amount of internal modulation dependent on how much the spectrum of the original input changes. With a rasping spring sound, this produced an 'eerie wind', such as one might imagine a passing spaceship to sound like. On a pitched sound, these parameters resulted in a double pitch: the original and the folded down. A high-placed range such as 1800 to 3200 yields a fairly high and thin, slightly ringing sound. With the rasping spring sound, this prouced a somewhat ethereal sound. On a pitched sound, these parameters were rather high, and not very much of the input remained. The fuller spectrum option does seem to strengthen the effect, perhaps bringing through a little more of the original sound. This process will focus the engergy in a tight band while retaining the pitch characteristics (because of the octave-transpose). It is therefore a way to intensify a sound. However there may also be a radical and somewhat unpredictable transformation of the sound, depending on the input. It seems most effective with sounds containing many and complex partials. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "lofrq", min = 0, max = 22050, input = "brk", def = 500, tip = "lowest frequency of range into which the spectrum is folded" },
  arg4 = { name = "hifrq", min = 0, max = 22050, input = "brk", def = 14000, tip = "highest frequency of range into which the spectrum is folded" },
  arg5 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Focus Freeze - 1 freeze channel amplitudes - Freeze spectral characteristics in a sound, a given times in text file "] = {
  cmds = { exe = "focus", mode = "freeze 1", tip = "1 freeze channel amplitudes - FOCUS FREEZE freezes the spectrum on one window's frequencies or amplitudes. If the frequencies are frozen, the amplitudes continue to move, and v.vs.: if the amplitudes are frozen, the frequencies continue to move.  Musical applications; This process prolongs specific timbral configurations in the sound, so it is a way of stopping the continuous spectral evolution which normally takes place in a sound. The results will vary enormously depending on the source and of course the contents of the spectrum at the freeze points. Static and stepped qualities are introduced into the sound by this process. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing times at which the spectrum is frozen. These times may be preceded by character markers: a use window here as freezewindow for spectrum AFTER this time. b use window here as freezewindow for spectrum BEFORE this time. Otherwise, times are end/start of freeze established at one of these markers." },
}
 
dsp["Focus Freeze - 2 freeze channel frequencies - Freeze spectral characteristics in a sound, a given times in text file "] = {
  cmds = { exe = "focus", mode = "freeze 2", tip = "2 freeze channel frequencies - FOCUS FREEZE freezes the spectrum on one window's frequencies or amplitudes. If the frequencies are frozen, the amplitudes continue to move, and v.vs.: if the amplitudes are frozen, the frequencies continue to move.  Musical applications; This process prolongs specific timbral configurations in the sound, so it is a way of stopping the continuous spectral evolution which normally takes place in a sound. The results will vary enormously depending on the source and of course the contents of the spectrum at the freeze points. Static and stepped qualities are introduced into the sound by this process. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing times at which the spectrum is frozen. These times may be preceded by character markers: a use window here as freezewindow for spectrum AFTER this time. b use window here as freezewindow for spectrum BEFORE this time. Otherwise, times are end/start of freeze established at one of these markers." },
}
 
dsp["Focus Freeze - 3 freeze channel amplitudes & frequencies - Freeze spectral characteristics in a sound, a given times in text file "] = {
  cmds = { exe = "focus", mode = "freeze 3", tip = "3 freeze channel amplitudes & frequencies - FOCUS FREEZE freezes the spectrum on one window's frequencies or amplitudes. If the frequencies are frozen, the amplitudes continue to move, and v.vs.: if the amplitudes are frozen, the frequencies continue to move.  Musical applications; This process prolongs specific timbral configurations in the sound, so it is a way of stopping the continuous spectral evolution which normally takes place in a sound. The results will vary enormously depending on the source and of course the contents of the spectrum at the freeze points. Static and stepped qualities are introduced into the sound by this process. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "text file containing times at which the spectrum is frozen. These times may be preceded by character markers: a use window here as freezewindow for spectrum AFTER this time. b use window here as freezewindow for spectrum BEFORE this time. Otherwise, times are end/start of freeze established at one of these markers." },
}
 
dsp["Focus Hold - Hold sound spectrum, at given times specified in text file"] = {
  cmds = { exe = "focus", mode = "hold", tip = " The process expands each hold window to the duration given, before proceeding to the next window. The output file is therefore longer than the input file. The timbral quality of the held sections depends on what is happening at the point(s), the window(s), where the holds occur. Musical applications; This is like SPEC MAGNIFY except that a list of times and hold-durations can be given. The key is to identify the times when the most interesting spectral data is present. Alternatively, you may just want to introduce some additional sustaining. Note that the soundfile plays normally at the times when the holds finish. This means that you can create outputs which mix normal and held sections in various ways. For example, a useful objective is to alternate the original sound with the held spectrum. This is done in the following file (for a 5 sec. source), where the length of time held (0.6 sec.) is shorter than the time-interval to the next hold point (1 sec.). Thus the sound is 'released' after 1 second and the original sounds for 0.4 second until the next hold point is reached: Hold_start_time Length_of_time_held 0.5    0.6, 1.5    0.6, 2.5    0.6, 3.5    0.6, 4.5    0.4. The above alternation of original and held was very noticeable with a complex input such as the spring strum (raspdt.wav). However, with a clear, steady pitch such as a horn sound, there were some light surface changes, but overall a smooth flow between original and held – this could therefore be a useful way to extend, i.e., lengthen, a pitched sound, as it adds the held sections to the duration of the soundfile." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "contains a list of paired times at which the spectrum is held and a hold-duration for each time. These data items must be paired correctly. " },
}
 
dsp["Focus Step - Step-frame through sound by freezing spectrum at regular time intervals"] = {
  cmds = { exe = "focus", mode = "step", tip = "  This is an automatic FREEZE of the spectrum at intervals of timestep, similar to the familiar 'sample-hold' effect. The main thing here is that the time intervals are regular. Thus the spectrum of the sound, which normally is constantly changing, is frozen with the values at the beginning of each timestep. The effect with fairly large timesteps can be a relatively slow-moving sample-hold effect. But as the duration of the timestep was shortened, the complex source developed regular 'jangling' effect, while the clearly pitched source only changed its tone. The 'jangling' effect seems to occur with timestep at about 0.1 or even 0.05. At 0.25, the sample-hold is quite noticeable, rather slow at 0.5 and probably too slow at 1.0 – but this setting could be used to produce sections of steady-state material that can then be CUT and used elsewhere. Timestep can be very small. The Phase Vocoder reference manual explains that the value shown by DIRSF for the sample rate of an analysis file is actually frame rate: the number of analysis frames per second, e.g. 172 or 344 etc. (The sample rate and value for -N both alter the frame rate.) So if you divide 1 sec by the frame rate, you will learn the duration of one frame: e.g., 1/344 = 0.0029 (2.9ms). As timestep gets larger, timbral alteration increases. Musical applications; It is recommended that you start exploring this function by using very small values for timestep. This should produce a granulation effect. But note that it is doing this without creating enveloped 'grains' as does MODIFY BRASSAGE. It is working directly on the spectrum of the sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "timestep", min = 0, max = length/1000, tip = "duration of the steps. Must be >= the duration of two analysis frames. The value here is rounded internally to a multiple of analysis frame time." },
}
 
------------------------------------------------
-- formants
------------------------------------------------
 
dsp["Formants Get - extract evolving formant envelope from an analysis file "] = {
  cmds = { exe = "formants", mode = "get", tip = " In earlier CDP systems this process was called SPECFGET. It is a straightforward utility to extract and store the evolving spectral envelope of a sound. On Extracting Formants The two extraction options (-fN | -pN) relate to resolution. Essential information:  the logarithmic nature of the pitch scale means that as one goes higher, an increasingly wider band of frequencies is encompassed by the 'octave'. The octave of a tone is double that tone's frequency. Thus the octave between 220Hz and 440Hz encompasses 220Hz, while the octave between 2200Hz and 4400Hz encompasses 2200Hz. -fN – 'linear frequency-wise' is an 'equal spacing' comprising equal differences. -pN – 'linear pitchwise' is an 'equal spacing' comprising equal ratios.  With 'linear frequency-wise', the analysis channels define frequency bands of identical bandwidth:  each band will encompass the same frequency range. With 'linear pitchwise', the frequency bands actually get wider as one goes higher. For example, if linear pitchwise is set at N = 12, the octave between 220Hz and 440 Hz is divided into 12 semitones; if N = 3, the octave is divided into 4 minor thirds (each of slightly increasing bandwidth). In the octave between 2200Hz and 4400Hz these same divisions will consist in considerably wider bandwidths. In other words, linear pitchwise is working via ratios with the logarithmic relationship of pitches. Linear frequency-wise should therefore be chosen if the main formant area of interest is in the higher frequencies, because there will be better resolution here when extracting the spectral envelope. But if the main interest in the sound is in the lower frequencies, then the linear pitchwise option will be suitable: the frequency bandwidths relate to the octave bandwidths. You are also able more easily to think in terms of normal musical terminology for intervals. If in doubt, you'll need to experiment.  The nature of the source, we should also observe, affects the results. With a noisy source, there is energy all over the signal. It is therefore easy to find the envelope of the source and extract the formants. With a pitched source, especially a high voice, there will be just a few partials with significant energy and, although these lie under the formant envelope, there is often not enough data to get a good indication of where the whole formant is. A sonogram is not really the best way to identify formants, says Richard Dobson. Better would be a per frame FFT-style display of amplitude/frequency along with a display of the spectral envelope. The Snack-based sonogram display available in Sound Loom does not have a facility for selecting a frequency band directly from it. We would recommend Audacity for inspecting the spectrum of a sound and picking out a formant region. We are currently exploring peak tracking and ConstQ Pvoc as ways to improve pitched partials and formants extraction. For more detailed information and a software option, please see Prosoniq. Formant extraction and preservation is therefore a difficult process and ideal results are not obtained in every case. Musical applications; This process is essential when wanting to preseve the timbral qualities of a sound. They can be transferred to another sound, or they may be re-imposed on the original sound after it has undergone some other transformations. The re-imposing is done with FORMANTS PUT." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.for", output = "for", },
  arg3 = { name = "N", switch = "-f", min = 0, max = 256, tip = "extract formant envelope 'linear frequency-wise', using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 256, tip = "extract formant envelope 'linear pitchwise', using N equally-spaced pitch-bands per octave" },
}
 
dsp["Formants Put - 1 impose spectral envelope in a formant file on spectrum in a PVOC analysis file"] = {
  cmds = { exe = "formants", mode = "put 1", tip = " 1 New formant envelope REPLACES the sound's own formant envelope. Previously SPECFPUT, FORMANTS PUT reads a spectral trajectory as stored in formant file and imposes it on infile. In many sounds, the amplitude peaks are focused around certain frequencies; these are called formant areas, and are a key factor in creating the timbral colouration of a sound. These formants may themselves change over time, forming a spectral trajectory. The spectral trajectory is extracted (prior to running FORMANTS PUT) with FORMANTS GET. This trajectory (i.e., spectral envelope) is the rising and falling amplitude pattern of the partials (frequencies). When this pattern is imposed on another file, the precise spectral envelope changes ­ and colourations ­ are introduced into the receiving file. Musical applications; The presence of the spectral trajectory data in the fmntfile makes it possible to impose the same data on more than one file. There are several musical purposes which can be achieved with this process: simply as a means to link/unify musical data, to re-colour a sound with the timbral characteristics of another, to give one sound the meanings associated with another sound (this applies especially to recognisable sounds from the world around us)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate) " },
  arg5 = { name = "lof", switch = "-l", min = 0, max = 22100, def = 50, tip = "low frequency, below which spectrum is set to zero" },
  arg6 = { name = "hif", switch = "-h", min = 0, max = 22100, def = 10000, tip = "high frequency, above which spectrum is set to zero" },
  arg7 = { name = "gain", switch = "-g", min = 0, max = 100, def = 1, tip = "adjustment to spectrum loudness (normally < 1.0)" },
}
 
dsp["Formants Put - 2 impose spectral envelope in a formant file on spectrum in a PVOC analysis file"] = {
  cmds = { exe = "formants", mode = "put 2", tip = "2 New formant envelope is IMPOSED ON TOP OF the sound's own formant envelope. Previously SPECFPUT, FORMANTS PUT reads a spectral trajectory as stored in formant file and imposes it on infile. In many sounds, the amplitude peaks are focused around certain frequencies; these are called formant areas, and are a key factor in creating the timbral colouration of a sound. These formants may themselves change over time, forming a spectral trajectory. The spectral trajectory is extracted (prior to running FORMANTS PUT) with FORMANTS GET. This trajectory (i.e., spectral envelope) is the rising and falling amplitude pattern of the partials (frequencies). When this pattern is imposed on another file, the precise spectral envelope changes ­ and colourations ­ are introduced into the receiving file. Musical applications; The presence of the spectral trajectory data in the fmntfile makes it possible to impose the same data on more than one file. There are several musical purposes which can be achieved with this process: simply as a means to link/unify musical data, to re-colour a sound with the timbral characteristics of another, to give one sound the meanings associated with another sound (this applies especially to recognisable sounds from the world around us)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lof", switch = "-l", min = 0, max = 22100, def = 50, tip = "low frequency, below which spectrum is set to zero" },
  arg5 = { name = "hif", switch = "-h", min = 0, max = 22100, def = 10000, tip = "high frequency, above which spectrum is set to zero" },
  arg6 = { name = "gain", switch = "-g", min = 0, max = 100, def = 1, tip = "adjustment to spectrum loudness (normally < 1.0)" },
}
 
dsp["Formants Vocode - impose spectral envelope of 2nd sound onto 1st sound "] = {
  cmds = { exe = "formants", mode = "vocode", tip = "For this one to work you need to enable one of the N toggles first! FORMANTS VOCODE achieves 'cross-synthesis'; it was formerly called SPECVOCO, but here has been re-written extensively. Cross-synthesis takes the time-evolving spectral envelope of one sound and imposes it on another sound. For example, vocal shouts will have a strong spectral profile. If this is imposed onto the sound of running water, we should hear the water shout. Strictly speaking, this is not 'morphing', which is more of a gradual transition from one sound to another. But in a broader sense, it certainly is the reshaping of one sound with the characteristics of another. Musical applications; The difference between frequency-wise and pitch-wise could be important. As discussed in FORMANTS GET above, linear frequency-wise divides into absolutely equal frequency bands, while linear pitchwise divides octaves (of varying frequency widths) into equal-ratio bands, but these bands will vary in size from octave to octave, wider as one goes higher. It was recommended above to use frequency-wise if the sound is concentrated in the higher frequency areas (to get better resolution), and pitchwise for lower frequency areas. Experimentation is still the name of the game. Frequencies below lof and above hif are simply ignored when the new spectrum is reconstructed. Gain is not predictable because it depends on the level of the signals you put in, their relative levels, and the way those levels are distributed over the spectrum. A process that boosts level a great deal at the low end of the spectrum can cause distortion. Try it with no gain first." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "N", switch = "-f", min = 0, max = 256, def = 12, tip = " extract formant envelope linear frequency-wise, using 1 point for every N equally spaced frequency channels" },
  arg5 = { name = "N", switch = "-p", min = 0, max = 12, def = 1, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg6 = { name = "lof", switch = "-l", min = 0, max = 22050, def = 400, tip = "low frquency, below which data is filtered out" },
  arg7 = { name = "hif", switch = "-h", min = 0, max = 22050, def = 14000, tip = "high frquency, above which data is filtered out" },
  arg8 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0.5, tip = "is the amplitude adjustment to the signal (normally < 1.0)" },
}

dsp["Formants Getsee - extract formants from analfile and write as 'soundfile' for viewing "] = { 
  cmds = { exe = "formants", mode = "getsee", input = "pvoc", tip = "See a detailed discussion of the two extraction options in the text for FORMANTS GET for further information on this choice. Previously a part of SPECFSEE, this process produces a pseudo soundfile which can be viewed in the same way as a normal soundfile. In the display we see a series of spectral windows, separated by zeroes. The horizontal display represents the spectral frequencies and this may be displayed in a manner which increases regularly with frequency (linear frequency-wise) or in a manner which increases regularly with pitch (linear pitch-wise). The vertical display represents the amplitude of each spectral band. The range of amplitudes is very large indeed, and in order to display them in the soundfile format, the log of the amplitude values is used and scaled to utilise fully the range of sound-sample values (-32768 to 32767). The display therefore indicates only the shape of the spectrum, and how that shape changes through time - no absolute amplitude values can be read off from it. FORMNTS GETSEE uses linear interpolation in determining the spectral contour. The output pseudo-soundfile is viewed by loading it into a soundfile display program, such as CDP's VIEWSF. SPECIAL NOTE – It is important to realise that output 'soundfile' is in this case a 'pseudo-soundfile'. All pseudo-soundfiles and binary files are displayed by DIRSF, and on PC are automatically given a .wav extension. It is therefore recommended that you adopt a naming convention that indicates which type of file it is, such as 'fmnt...' for the formant file produced by FORMANTS GET, 'psnd...' for the output of GETSEE, and 'bpdf...' for the binary pitch data file produced by PITCH. The different types of file all have their own 'property strings' written in the header, so the system will know what they are and prevent inappropriate use; the naming conventions are for your own convenience. When a program produces a textfile, it is NOT given a .wav extension and will not be displayed by DIRSF. Look for textfiles in the current directory, with the command 'dir'. Also see FORMANTS GET and FORMANTS PUT." },  
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "fN", switch = "-f", min = 0, max = 256, tip = "extract formant envelope 'linear frequency-wise', using 1 point for every N equally-spaced frequency-channels" }, 
  arg4 = { name = "pN", switch = "-p", min = 0, max = 256, tip = "extract formant envelope 'linear pitchwise', using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-s", switch = "-s", tip = "semitone bands for display (Default: equal Hz bands)" }, 
}

dsp["Formants See - convert formant data in a binary formant data file to a 'soundfile' for viewing"] = { 
  cmds = { exe = "formants", mode = "see", input = "data", tip = "The resulting logarithmically scaled display indicates formant shapes, but not the absolute amplitude values. This process used to form a part of the CDP program SPECFSEE. In the display we see a series of spectral windows, separated by zeroes. The horizontal display represents the spectral frequencies and this may be displayed in a manner which increases regularly with frequency (if linear frequency-wise was selected when using FORMANTS GET) or in a manner which increases regularly with pitch (if linear pitch-wise was selected). The vertical display represents the amplitude of each spectral band. The range of amplitudes is very large indeed, and in order to display them in the soundfile format, the log of the amplitude value is used and scaled to utilise fully the range of sound-sample values (-32768 to 32767). " },  
   arg1 = { name = "Input.for", input = "for" },
   arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
   arg3 = { name = "-v", switch = "-v", tip = "display data about formant-band parameters " },
}

------------------------------------------------
-- gate
------------------------------------------------

dsp["Gate 1 - remove low-level sound from signal - replaced by silence"] = { 
  cmds = { exe = "gate", mode = "gate 1", tip = "(doesn't work in windows vista?) Low level sound is replaced by silence (output will be the same length as the source). Everything below the gatelevel is removed. Therefore, the closer gatelevel is to 1.0, the greater amount will be removed. Note that it does not reduce the amplitude, but cuts out signal below the threshold. It reminds me of the sign in The Hitchhikers Guide to the Galaxy that had sunk into the ground, such that only the top part was visible, reading 'Go stick your head in a pig.' With GATE GATE, only the top part of the sound, above gatelevel, remains. It is not quieter, but there may be bits missing. Musical Applications; The applications can be twofold. On the one hand, low level noise can be removed. On the other hand, a sound can be reduced to a staccato image by using a gatelevel close to 1.0. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gatelevel", min = -96.0, max = 1.0, def = 0, tip = "level below which sound is to be removed (in dB - put value, but not 'dB') – Range: 1.0 (full amplitude) to -96.0 (silence) " },
}

dsp["Gate 2 - remove low-level sound from signal - low level sound is edited out"] = { 
  cmds = { exe = "gate", mode = "gate 2", tip = "(doesn't work in windows vista?) Low level sound is replaced by silence (output will be the same length as the source). Everything below the gatelevel is removed. Therefore, the closer gatelevel is to 1.0, the greater amount will be removed. Note that it does not reduce the amplitude, but cuts out signal below the threshold. It reminds me of the sign in The Hitchhikers Guide to the Galaxy that had sunk into the ground, such that only the top part was visible, reading 'Go stick your head in a pig.' With GATE GATE, only the top part of the sound, above gatelevel, remains. It is not quieter, but there may be bits missing. Musical Applications; The applications can be twofold. On the one hand, low level noise can be removed. On the other hand, a sound can be reduced to a staccato image by using a gatelevel close to 1.0. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gatelevel", min = -96.0, max = 1.0, def = 0, tip = "level below which sound is to be removed (in dB - put value, but not 'dB') – Range: 1.0 (full amplitude) to -96.0 (silence) " },
}

------------------------------------------------
-- get_partials
------------------------------------------------
 
dsp["Get_Partials Harmonic 1 – Convert single analysis window to FRQ and amplitude list - Extract relative amplitudes of partials in a pitched source "] = {
  cmds = { exe = "get_partials", mode = "harmonic 1", tip = "1 – Convert single analysis window to FRQ and amplitude list - The importance of this function is that it is looking for harmonic partials, i.e., those related by an integer multiple. This is easily seen with the display in frequency values (200, 400, 500 etc.), but not so easily seen with the MIDI values, though they are accurate. (See our Equivalent Pitch Notations Chart.) If you wanted the harmonics of a specific fundamental, e.g., MIDI 48 (Low C), you can look it up on the above-mentioned Chart (it's 130.81) or use SNDINFO UNITS to convert from MIDI to frequency. E.g., MIDI 50.2 = 148.538494. The output from GET_PARTIALS then enables you to work with the harmonics specifically related to that fundamental. Without the -v flag, the output text file can be used with FILTER USERBANK, and when the -v flag is invoked, the output text file can be used with FILTER VARIBANK, which can handle a time-varying filterbank.Also see HARMONIC in the Technical Glossary." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "fundamental", min = 0, max = 44100, def = 512, tip = "fundamental frequency for the harmonics" },
  arg4 = { name = "threshold", min = 2e-05, max = 1, def = 0.5, tip = "(relative) level below which partials are ignored – lower values capture more partials (Range: 0.000002 to 1.0)" },
  arg5 = { name = "-v", switch = "-v", tip = "gives output in format for FILTER VARIBANK file." },
}
 
dsp["Get_Partials Harmonic 2 – Convert single analysis window to MIDI and amplitude list - Extract relative amplitudes of partials in a pitched source "] = {
  cmds = { exe = "get_partials", mode = "harmonic 2", tip = "2 – Convert single analysis window to MIDI and amplitude list - The importance of this function is that it is looking for harmonic partials, i.e., those related by an integer multiple. This is easily seen with the display in frequency values (200, 400, 500 etc.), but not so easily seen with the MIDI values, though they are accurate. (See our Equivalent Pitch Notations Chart.) If you wanted the harmonics of a specific fundamental, e.g., MIDI 48 (Low C), you can look it up on the above-mentioned Chart (it's 130.81) or use SNDINFO UNITS to convert from MIDI to frequency. E.g., MIDI 50.2 = 148.538494. The output from GET_PARTIALS then enables you to work with the harmonics specifically related to that fundamental. Without the -v flag, the output text file can be used with FILTER USERBANK, and when the -v flag is invoked, the output text file can be used with FILTER VARIBANK, which can handle a time-varying filterbank.Also see HARMONIC in the Technical Glossary." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "fundamental", min = 0, max = 44100, def = 512, tip = "fundamental frequency for the harmonics" },
  arg4 = { name = "threshold", min = 2e-05, max = 1, def = 0.5, tip = "(relative) level below which partials are ignored – lower values capture more partials (Range: 0.000002 to 1.0)" },
  arg5 = { name = "-v", switch = "-v", tip = "gives output in format for FILTER VARIBANK file." },
}
 
dsp["Get_Partials Harmonic 3 – Convert window at time time to FRQ and amplitude list - Extract relative amplitudes of partials in a pitched source "] = {
  cmds = { exe = "get_partials", mode = "harmonic 3", tip = "3 – Convert window at time time to FRQ and amplitude list - The importance of this function is that it is looking for harmonic partials, i.e., those related by an integer multiple. This is easily seen with the display in frequency values (200, 400, 500 etc.), but not so easily seen with the MIDI values, though they are accurate. (See our Equivalent Pitch Notations Chart.) If you wanted the harmonics of a specific fundamental, e.g., MIDI 48 (Low C), you can look it up on the above-mentioned Chart (it's 130.81) or use SNDINFO UNITS to convert from MIDI to frequency. E.g., MIDI 50.2 = 148.538494. The output from GET_PARTIALS then enables you to work with the harmonics specifically related to that fundamental. Without the -v flag, the output text file can be used with FILTER USERBANK, and when the -v flag is invoked, the output text file can be used with FILTER VARIBANK, which can handle a time-varying filterbank.Also see HARMONIC in the Technical Glossary." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "fundamental", min = 0, max = 44100, def = 512, tip = "fundamental frequency for the harmonics" },
  arg4 = { name = "threshold", min = 2e-05, max = 1, def = 0.5, tip = "(relative) level below which partials are ignored – lower values capture more partials (Range: 0.000002 to 1.0)" },
  arg5 = { name = "time", min = 0, max = length/1000, tip = "(multiple window files only): time of window to use" },
  arg6 = { name = "-v", switch = "-v", tip = "gives output in format for FILTER VARIBANK file." },
}
 
dsp["Get_Partials Harmonic 4 – Convert window at time time to FRQ and amplitude list - Extract relative amplitudes of partials in a pitched source "] = {
  cmds = { exe = "get_partials", mode = "harmonic 4", tip = "4 - Convert window at time time to MIDI and amplitude list  - The importance of this function is that it is looking for harmonic partials, i.e., those related by an integer multiple. This is easily seen with the display in frequency values (200, 400, 500 etc.), but not so easily seen with the MIDI values, though they are accurate. (See our Equivalent Pitch Notations Chart.) If you wanted the harmonics of a specific fundamental, e.g., MIDI 48 (Low C), you can look it up on the above-mentioned Chart (it's 130.81) or use SNDINFO UNITS to convert from MIDI to frequency. E.g., MIDI 50.2 = 148.538494. The output from GET_PARTIALS then enables you to work with the harmonics specifically related to that fundamental. Without the -v flag, the output text file can be used with FILTER USERBANK, and when the -v flag is invoked, the output text file can be used with FILTER VARIBANK, which can handle a time-varying filterbank.Also see HARMONIC in the Technical Glossary." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "fundamental", min = 0, max = 44100, def = 512, tip = "fundamental frequency for the harmonics" },
  arg4 = { name = "threshold", min = 2e-05, max = 1, def = 0.5, tip = "(relative) level below which partials are ignored – lower values capture more partials (Range: 0.000002 to 1.0)" },
  arg5 = { name = "time", min = 0, max = length/1000, tip = "(multiple window files only): time of window to use" },
  arg6 = { name = "-v", switch = "-v", tip = "gives output in format for FILTER VARIBANK file." },
}
 
------------------------------------------------
-- glisten
------------------------------------------------
 
dsp["Glisten - Randomly partition the spectrum into bins and play back in order  "] = {
  cmds = { exe = "glisten", mode = "glisten", tip = "All the channels of the analysis are partitioned into N mutually exclusive sets, with channels assigned at random to each set. These sets form a complete group. Then channels in the first set are played, for setdur windows, at which point the channels in the 2nd set are played for setdur windows and so on. As this happens, we are progressing through the original file at its original rate, the process determining merely which channels are used – the others being zeroed. Once all N sets are exhausted, a new group is made by random partition, and so on. Musical Applications; This process is meant to 'animate' a fixed or relatively static spectrum. The channels of the spectrum are selected (in groups) at random and played back in sequence." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "grpdiv", min = 2, max = 32768, def = 2, tip = "Note that grpdiv must be an exact divisor of the channel count. (Range: 2 to channel-count)" },
  arg4 = { name = "setdur", min = 1, max = 1024, def = 256, tip = "the number of windows for which a set-of-channels persists before we switch to the next set-of-chans. Range: 1 to 1024." },
  arg5 = { name = "pitchshift", switch = "-p", min = 0, max = 12, def = 0, tip = "The maximum range in (possibly fractional) semitones of random plus or minus pitch shifting (i.e., upwards or downwards) of each channel set. Range: 0.0 to 12.0. Default: 0.0" },
  arg6 = { name = "durrand", switch = "-d", min = 0, max = 1, tip = "the randomisation of setdur between 1 and setdur. Range: 0 to 1" },
  arg7 = { name = "divrand", switch = "-v", min = 0, max = 1, tip = "randomise the number of channels in each set in a group. Range: 0 to 1" },
}
 
------------------------------------------------
-- grain
------------------------------------------------
 
dsp["Grain Align - Synchronise grain onsets in 2nd grainy sound with those in the 1st "] = {
  cmds = { exe = "grain", mode = "align", tip = " The timing of the grains of infile2 is made to match those of infile1. It may be unwise not to specify a value for the optional parameter -lgate (gate level for infile1). Musical applications; If you achieve an effective grain timing for two sounds, GRAIN ALIGN makes it possible to create a subtle correspondence between the two soundfiles. For example, the fine attack-structure of the two sounds can be synchronised, so that they appear to pulse in parallel. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "offset", min = 0, max = 1000, def = 5, tip = "add this value to all grain timings" },
  arg5 = { name = "gate2", min = 0, max = 1, input = "brk", def = 0.5, tip = "minimum signal level to register grain in infile2 (Range: 0 to 1)" },
  arg6 = { name = "len", switch = "-b", min = 0, max = length/1000, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg7 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "minimum signal level to register grain in infile1 (Range: 0 to 1, Default 0.3)" },
  arg8 = { name = "minhole", switch = "-h", min = 0.032, max = 22050, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg9 = { name = "winsize", switch = "-t", min = 0, max = length/1000, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile1)" },
  arg10 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Duplicate - Duplicate grains in a grainy sound "] = {
  cmds = { exe = "grain", mode = "duplicate", channels = "any", tip = "This process makes N copies of each grain, proceding through the infile grain by grain. You are recommended to use GRAIN COUNT with the gate flag to determine which gate level works best with the file (produces a good granulation of the source). This gate level should be used with GRAIN DUPLICATE. Not using it appears to produce anomalous results, such as no grain duplications and a long period of silence. Musical applications; The overall effect is that of rapid-fire repeats or a stuttering effect, depending on the source. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 1.0, max = 32767.0, def = 2, input = "brk", tip = "number of repetitions of each grain." },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains. (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "equired signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = length/1000, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Find - Locate timings of grain-onsets in a grainy sound "] = {
  cmds = { exe = "grain", mode = "find", tip = "This function locates and writes to a text file the time in seconds at which each grain begins. Again, the effective gate level determined with GRAIN COUNT should be used. Musical applications; Examination of this text file provides a way to check on the results of the various GRAIN functions which affect the grain timings. The data can be used to retime the grains in another grainy sound. See GRAIN REPOSITION." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "len", switch = "-b", min = 0, max = length/1000, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg4 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3) " },
  arg5 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.5, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg6 = { name = "winsize", switch = "-t", min = 0, max = length/1000, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg7 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Grev 1 - REVERSE – the grain-units isolated are played in reverse order"] = {
  cmds = { exe = "grain", mode = "grev 1", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 0, max = length, def = 5, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 7, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
}
 
dsp["Grain Grev 2 - REPEAT – gpcnt-sized units are repeated"] = {
  cmds = { exe = "grain", mode = "grev 2", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out. Mode 2 reveals how this program works. The example command line given above has these parameters: wsiz = 10 ms, trof = 0.5 (half-way can be a useful place to start with a parameter), gpcnt = 4 (four grains to a group), and repets = 3 (repeat the 4-grain-units 3 times). When run, we find that the syllables of the spoken text are, for the most part, clearly isolated and repeat 3 times each without overlaps. Thus the program is working effectively with the spaces between the syllables." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 0, max = length, def = 5.5, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 3, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
  arg6 = { name = "repets", min = 0, max = 100, input = "brk", def = 5, tip = "the number of repetitions of each unit" },
}
 
dsp["Grain Grev 3 - DELETE – remove the specified number of units"] = {
  cmds = { exe = "grain", mode = "grev 3", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out. Modes 3 and 4 operate as expected, deleting grains or replacing them with silence. Mode 1 plays back the isolated grains in reverse order, i.e., you hear syllabic fragments from the end of the file first and proceed back to the beginning – but the text fragments themselves are 'forward' as normal. Higher values for wsiz and gpcnt result in longer passages of the text treated as a unit, while smaller values fragment the text more. It's a very odd effect." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 0, max = length, def = 4, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 3, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
  arg6 = { name = "keep", min = 0, max = 100, input = "brk", def = 3, tip = "the number of units to keep" },
  arg7 = { name = "outof", min = 0, max = 100, def = 5, tip = "out of" },
}
 
dsp["Grain Grev 4 - OMIT – replace specified number of units with silence"] = {
  cmds = { exe = "grain", mode = "grev 4", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out. Modes 3 and 4 operate as expected, deleting grains or replacing them with silence. Mode 1 plays back the isolated grains in reverse order, i.e., you hear syllabic fragments from the end of the file first and proceed back to the beginning – but the text fragments themselves are 'forward' as normal. Higher values for wsiz and gpcnt result in longer passages of the text treated as a unit, while smaller values fragment the text more. It's a very odd effect." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 0, max = length, def = 5.5, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 3, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
  arg6 = { name = "keep", min = 0, max = 100, input = "brk", def = 2, tip = "the number of units to keep" },
  arg7 = { name = "outof", min = 0, max = 100, def = 5, tip = "out of" },
}
 
dsp["Grain Grev 5 - TIMESTRETCH – the time expansion is happening in the troughs: the grains of sound are not themselves stretched"] = {
  cmds = { exe = "grain", mode = "grev 5", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 0, max = length, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 3, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
  arg6 = { name = "tstretch", min = 0.01, max = 100, input = "brk", tip = "the amount to timestretch the output (grains NOT stretched) Range: 0.01 to 100)" },
}
 
dsp["Grain Grev 6 - GET - writes grain time-positions to a textfile"] = {
  cmds = { exe = "grain", mode = "grev 6", tip = "This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "wsiz", min = 0, max = length, def = 5, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg5 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 7, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
}
 
dsp["Grain Grev 7 - PUT - places grains in time according to textfile "] = {
  cmds = { exe = "grain", mode = "grev 7", tip = " This program was written to assist in separating the large-scale 'grains' in a stream of sound, e.g. the syllables of speech. It can be a useful alternative to SFEDIT SYLLABLES, which is for the precise manual editing of syllabic units. The other 'grain' programs rely on a gate which searches for moments when the input sound falls below a certain level, and then splices the sound at those points. This works fine when the sound grains are fairly consistent (e.g. a sequence of pizzicato sounds). However, the syllables of speech are usually quite different from one another. So the gating procedure finds some good places to cut, and misses others. The 'grev' process does not depend on setting a gate level. It simply looks for troughs (low points) in the envelope (at a suitable timescale), then within those troughs, searches for a zero-crossing at the lowest point of the signal in the trough. Once it finds these it can separate the grains with zero-length splices. The key thing is to set the envelope window about three times smaller than the features (e.g. syllables) you want to tease out." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "in_timesfile", input = "txt", tip = "textfile containg times at which the program is to place grains" },
  arg4 = { name = "wsiz", min = 0, max = length, def = 5, tip = "windowsize in milliseconds, determining the size of the grains to find" },
  arg5 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "the acceptable trough height, relative to adjacent peaks. Range: greater than 0 to less than 1 (> 0 Range < 1)" },
  arg6 = { name = "gpcnt", min = 0, max = 1000, input = "brk", def = 7, tip = "('groupcount') the number of grains to treat as a unit in the operations" },
}
 
dsp["Grain Noise_Extend - Find and timestretch noise component in a sound "] = {
  cmds = { exe = "grain", mode = "noise_extend", tip = " This process is complementary to GRAIN R_EXTEND. It searches for sybillants in speech (or noise materials in any input sound) then allows them (and only them) to be sustained. Note that different types of values are involved with the mindur and maxdur parameters. The minimum duration is given in milliseconds, but the maximum duration, because you're thinking in terms of the length of the soundfile, is given in seconds. The -x option enables you to save only the extended noise component, discarding the rest of the file. When -x is not invoked, you hear the whole soundfile (up to the maxdur you specified) with that first (extended) noise component in its original position in the soundfile. Musical applications; This program acually looks for and extends (only) the first noise component that it finds at the given parameter specifications. The extended noise component, especially if saved on its own via the -x option, can be a useful bit of source material for other sound transformations." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "duration", min = 0, max = length/1000, tip = "duration of the noise part of the output soundfile" },
  arg4 = { name = "minfrq", min = 1000, max = 22050, def = 6000, tip = "the lowest 'frequency' in Hz acceptable as noise. Range: 1000 to 22050Hz." },
  arg5 = { name = "mindur", min = 0, max = 50, def = 1, tip = "the minimum duration of signal in milliseconds acceptable as noise source. Range: 0 to 50 milliseconds" },
  arg6 = { name = "maxdur", min = 0, max = length/1000, tip = "the maximum duration of signal in seconds acceptable as noise source. Range: 0.0 to length of input soundfile (in seconds)" },
  arg7 = { name = "-x", switch = "-x", tip = "keep only the extended noise. The Default is to keep the rest of the input source sound as well." },
}
 
dsp["Grain Omit - Omit a proportion of grains in a grainy sound"] = {
  cmds = { exe = "grain", mode = "omit", channels = "any", tip = "The key here is the out-of parameter. This defines the size of the group which serves as the unit of operations: out of every unit, keep grains are retained. Thus, if 5 out of 10 are retained, 50% of the file is omitted, if 7 out of 10 are retained, 3 out of the 10 are omitted (30%go), if 4 out of 10 are retained, 6 out of the 10 are omitted (60% go). What is significant about the out-of parameter is that its size affects the result, Thus, a large value for out-of with a smaller value for keep , e.g., 20 and 10, will break up the file much more than it would if out-of were smaller: e.g., 4 and 2 – even though the proportion of omitted material is the same (50%). This is because larger continuous chunks of the source (10 grains rather than 2 grains) are lost in the former case. Musical applications; GRAIN OMIT provides a useful way to contract sound material without altering the sonic substance (i.e. the grains are not themselves, time-contracted)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "keep", min = 0, max = 1000, input = "brk", def = 3, tip = "number of grains to keep from each set of out-of grains. Keep may vary over time, but must not exceed out-of" },
  arg4 = { name = "out-of", min = 0, max = 1000, def = 10, tip = "keep grains retained from start of each set of out-of grains" },
  arg5 = { name = "len", switch = "-b", min = 0, max = length/1000, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg6 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1)" },
  arg7 = { name = "minhole", switch = "-h", min = 0.032, max = length/1000, tip = "minimum duration of holes between grains" },
  arg8 = { name = "winsize", switch = "-t", min = 0, max = length, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg9 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain R_Extend - 1 Time-stretch natural sounds like the rolled 'rrr' in speech"] = {
  cmds = { exe = "grain", mode = "r_extend 1", tip = "1 Mark where the iterative part of the sound is located. The key here is to listen to the source sound to ascertain where iterative material starts and ends. 'Iterative material' means a rapid pulsation as in a vocal rolled 'r', vocal grit generally, rattling train windows, etc. GRAIN R_EXTEND enables you to prolong this material, with subtle variants in order to make the flow supple. Notice that rep introduces a varying degree of randomised order in the repetitions of the grain units, the randomisation increasing with higher values. Rep has to do with how the grain sequence is generated by controlling how many identical grains can be adjacent. GRAIN R_EXTEND provides some middle ground between larger-scale segmentation techniques and smaller-scale granulation techniques. You can also focus on a particular part of a sound and prolong it with variants without having to cut and resplice the material." },
  arg1 = { name = "infile", input = "wav", tip = "input soundfile containing iterative material" },
  arg2 = { name = "outfile", output = "wav", tip = "output soundfile with the iterations extended" },
  arg3 = { name = "stt", min = 0, max = length/1000, def = 0, tip = "time of start of iterated material within source soundfile" },
  arg4 = { name = "end", min = 0, max = length/1000, def = length/1000, tip = "time of end of iterated material within source soundfile" },
  arg5 = { name = "ts", min = 1.0, max = 32767.00, def = 2, tip = "('time-stretch') - multiplier that specifies how much to time-stretch the marked (or found) material" },
  arg6 = { name = "pr", min = 2, max = 100, def = 2, tip = "guesstimate of the pitch-range of the iteration in the source, in octaves, or parts of octaves." },
  arg7 = { name = "rep", min = 0, max = 4, def = 1, tip = "('repeats') - the iterated material is extended by reusing individual segments in a randomised pattern." },
  arg8 = { name = "get", min = 0, max = 100, def = 3, tip = "guesstimate of the number of iterations you expect to find " },
  arg9 = { name = "asc", min = 0, max = 1, def = 0.5, tip = "random amplitude variation of the output segments." },
  arg10 = { name = "psc", min = 0, max = 24, tip = "random pitch variation of the output segments." },
  arg11 = { name = "-x", switch = "-x", tip = "keep rrr-extension only" },
}
 
dsp["Grain R_Extend - 2 Time-stretch natural sounds like the rolled 'rrr' in speech"] = {
  cmds = { exe = "grain", mode = "r_extend 2", tip = "The program attempts to find the iterative part, using envelope tracing.. The key here is to listen to the source sound to ascertain where iterative material starts and ends. 'Iterative material' means a rapid pulsation as in a vocal rolled 'r', vocal grit generally, rattling train windows, etc. GRAIN R_EXTEND enables you to prolong this material, with subtle variants in order to make the flow supple. Notice that rep introduces a varying degree of randomised order in the repetitions of the grain units, the randomisation increasing with higher values. Rep has to do with how the grain sequence is generated by controlling how many identical grains can be adjacent. GRAIN R_EXTEND provides some middle ground between larger-scale segmentation techniques and smaller-scale granulation techniques. You can also focus on a particular part of a sound and prolong it with variants without having to cut and resplice the material." },
  arg1 = { name = "infile", input = "wav", tip = "input soundfile containing iterative material" },
  arg2 = { name = "outfile", output = "wav", tip = "output soundfile with the iterations extended" },
  arg3 = { name = "gate", min = 0, max = 1, def = 0.5, tip = "minimum level in source soundfile for envelope tracing to kick in. (Range: 0 to 1)" },
  arg4 = { name = "usz", min = 0, max = length, def = 15, tip = "size of unit searched for, in milliseconds (15 for rolled 'rr')" },
  arg5 = { name = "ts", min = 1, max = 32767, def = 2, tip = "('time-stretch') - multiplier that specifies how much to time-stretch the marked (or found) material" },
  arg6 = { name = "pr", min = 2, max = 100, def = 2, tip = "guesstimate of the pitch-range of the iteration in the source, in octaves, or parts of octaves." },
  arg7 = { name = "rep", min = 0, max = 4, def = 1, tip = "('repeats') - the iterated material is extended by reusing individual segments in a randomised pattern." },
  arg8 = { name = "get", min = 0, max = 100, def = 3, tip = "guesstimate of the number of iterations you expect to find " },
  arg9 = { name = "asc", min = 0, max = 1, def = 0.5, tip = "random amplitude variation of the output segments." },
  arg10 = { name = "psc", min = 0, max = 24, def = 1, tip = "random pitch variation of the output segments." },
  arg11 = { name = "skp", min = 0, max = 100, def = 0, tip = "the number of found (iterative) units to skip before processing" },
  arg12 = { name = "at T", min = 0, max = length/1000, def = 0, tip = "iterative ritard, after at seconds to event-separation T" },
  arg13 = { name = "by", min = 0.015, max = 60, def = 1, tip = "ritard end point reached after a further by seconds; by = 0 gives no ritard" },
  arg14 = { name = "-s", switch = "-s", tip = "keep sound before the extended material" },
  arg15 = { name = "-e", switch = "-e", tip = "keep sound after the extended material " },
}
 
dsp["Grain Remotif 1 - Change pitch and rhythm of grains in a grainy sound based on transposition text file"] = {
  cmds = { exe = "grain", mode = "remotif 1", channels = "any", tip = "1 Transform each grain in turn, without repeating any grains: on reaching the end of the transpmultfile data, cycle back to its start. Implementing this process is really very easy. The transpmultfile does not have to match the number of grains in the infile. You just need to design the pitch and time shapes into which you want the grains to flow. The pitch transpositions are placed in the left column as positive or negative semitones, and the time-multipliers are placed in the right column, > 1 to make the gap longer, < 1 to make it shorter. In Mode 1 a single line of the transpmultfile is applied to a single grain, the next line to the next grain etc., cycling around the textfile until reaching the last grain of infile. Any difference in duration between infile and outfile is due to the timing difference between the grains. You can make the shape occur once by having as many lines in the textfile as there are grains in the infile. Musical applications; GRAIN REMOTIF can be used to apply a gross shape to the grains of the infile, with controlled accelerandi or other timing changes. This shape can occur once, cycle around the textfile with one line per grain (Mode 1), or the shape can occur once per grain:  each grain repeats with transposition and timing differences as many times as there are lines in the textfile (Mode 2)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transpmultfile ", input = "txt", tip = "transpmultfile – is a file containing transposition time multiplier pairs. See CDP docs for syntax" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Remotif 2 - Change pitch and rhythm of grains in a grainy sound based on transposition text file"] = {
  cmds = { exe = "grain", mode = "remotif 2", channels = "any", tip = "2 Transform grain in each specified way before proceeding to the next grain. Implementing this process is really very easy. The transpmultfile does not have to match the number of grains in the infile. You just need to design the pitch and time shapes into which you want the grains to flow. The pitch transpositions are placed in the left column as positive or negative semitones, and the time-multipliers are placed in the right column, > 1 to make the gap longer, < 1 to make it shorter. In Mode 2 ALL of the lines of the transpmultfile are applied to each grain in turn. Thus every grain repeats for as many times as there are lines in the textfile, and one hears the whole shape defined in the textfile repeated as many times as there are grains in the infile. This makes the outfile much longer than the infile. Be careful with Mode 2, because an overly long textfile will create a very long outfile (and be very repetitive).  Musical applications; GRAIN REMOTIF can be used to apply a gross shape to the grains of the infile, with controlled accelerandi or other timing changes. This shape can occur once, cycle around the textfile with one line per grain (Mode 1), or the shape can occur once per grain: each grain repeats with transposition and timing differences as many times as there are lines in the textfile (Mode 2)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transpmultfile ", input = "txt", tip = "transpmultfile – is a file containing transposition time multiplier pairs. See CDP docs for syntax" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Reorder - Reorder grains in a grainy sound based on a text string"] = {
  cmds = { exe = "grain", mode = "reorder", tip = "This is so much fun. Comparable with DISTORT SHUFFLE, GRAIN REORDER provides another way for controlled fragmentation. The illustration code used in the Usage causes a fair amount of fragmentation. But a code such as abcdefg:b steps through 7 grains in their normal sequence, then goes back to the 2nd grain and proceeds to the 8th etc., moving gradually through the whole infile in this way. The code gfedcba:b does the same thing, but places the grains in reverse order. Musical applications; So you can see that you can play with the shaping possibilities to your heart's content, possibly making subtle correlations between the shapes made by the grains and patterns made elsewhere in the music." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "code", input = "string", def = "adb:c", tip = "a string such as adb:c indicating how grains are to be reordered" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Repitch 1 - Repitch grains in a grainy sound based on transposition text file"] = {
  cmds = { exe = "grain", mode = "repitch 1", channels = "any", tip = "1 Repitch each grain in turn, without repeating any grains; on reaching the end of the transposition list, cycle back to its start. This function is like GRAIN REMOTIF but only deals with transposition of the grains. Again, the transpositions are handled in terms of positive or negative numbers of semitones, so it is very simple to create the required transpfile. The transposition pattern / contour in the transpfile is transferred to the grains, such as narrow and wavy, or wide and jagged. This pattern can also serve as an important gestural or formal unifier in the composition. The textfile can be written on a single line, with the values separated by spaces. Newlines are ignored. For example: 0 1 2 3 2 1 0 -1 -2 -3 -2 -1 0. There may be some slight change in duration, proceeding from the source to the outfile, due to the effect of transposition on the final grain. Otherwise the grain onset timings should remain exactly as in the source sound. The role of the two Modes is the same as in GRAIN REMOTIF, either taking each grain in turn, or applying all the transpositions in the textfile to each grain, repeating each grain as many times as there are transpositions. Be careful with Mode 2, because an overly long textfile will create a very long outfile (and be very repetitive). However, the pattern is very clear, and a part of it may be especially interesting and able to be CUT and used elsewhere. Musical applications; An ideal tool for creating staccato pitch figurations with clear contour shapes or even gestural potential" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transpfile", input = "txt", tip = "a file listing transpositions given as positive or negative semitone shifts. The maximum transposition is 4 octaves up or down" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Repitch 2 - Repitch grains in a grainy sound based on transposition text file"] = {
  cmds = { exe = "grain", mode = "repitch 2", channels = "any", tip = "2 Play grain at each transposed pitch, before proceeding to the next grain. This function is like GRAIN REMOTIF but only deals with transposition of the grains. Again, the transpositions are handled in terms of positive or negative numbers of semitones, so it is very simple to create the required transpfile. The transposition pattern / contour in the transpfile is transferred to the grains, such as narrow and wavy, or wide and jagged. This pattern can also serve as an important gestural or formal unifier in the composition. The textfile can be written on a single line, with the values separated by spaces. Newlines are ignored. For example: 0 1 2 3 2 1 0 -1 -2 -3 -2 -1 0. There may be some slight change in duration, proceeding from the source to the outfile, due to the effect of transposition on the final grain. Otherwise the grain onset timings should remain exactly as in the source sound. The role of the two Modes is the same as in GRAIN REMOTIF, either taking each grain in turn, or applying all the transpositions in the textfile to each grain, repeating each grain as many times as there are transpositions. Be careful with Mode 2, because an overly long textfile will create a very long outfile (and be very repetitive). However, the pattern is very clear, and a part of it may be especially interesting and able to be CUT and used elsewhere. Musical applications; An ideal tool for creating staccato pitch figurations with clear contour shapes or even gestural potential" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transpfile", input = "txt", tip = "a file listing transpositions given as positive or negative semitone shifts. The maximum transposition is 4 octaves up or down" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Reposition - Reposition grain onsets in a grainy sound based upon grain onsets listed in a text file "] = {
  cmds = { exe = "grain", mode = "reposition", channels = "any", tip = "This process re-times the start-times of the grains in the soundfile according to the pattern of times specified in timefile. The gaps between grain onsets can become larger or smaller in the timefile, but all onset times must increase – otherwise you would be trying to go backwards: e.g., 2.3 2.5 is OK, but 2.3 1.8 is not. The offset parameter determines when after the start of the soundfile the timefile pattern is applied. If, for example, the offset is 2 (sec.) and there are grains before the 2 second point, you will hear these sound as normal, and then you will hear grains after the 2 second point patterned by the timefile. It is useful to run GRAIN FIND first, because it finds any grains in the soundfile and writes their start-times to a textfile. You can then edit this textfile to create a timefile with your own timing pattern. Musical applications; GRAIN REPOSITION enables you to impose a rhythmic structure on the grains in a soundfile. Considerably more subtle effects are also possible. Trevor (Wishart) writes that it provides a way to create a correlation between the fine attack structure and some other musical events. For example, the repositioning data can be taken from another grainy sound (using GRAIN FIND) or any other timing data, allowing the fine attack structure of a grainy sound to be synchronised or coordinated with another sound or sound sequence. Here's a simple procedure: A timefile for the grains in a soundfile is quickly made with GRAIN FIND. Then GRAIN REPOSITION is run twice, e.g., with an offset of 0.25 the first time, and 0.33 the second time. The two files can be mixed with SUBMIX MERGE and You can then audition the result, hearing the double occurrence of the grains. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "timefile", input = "txt", tip = "must contain a list of grain-onset times in seconds. If any inter-grain time is reduced the minimum time allowed for a grain (0.032 sec), it will be reset to this minimum grain time." },
  arg4 = { name = "offset", min = 0, max = length/1000, def = 0, tip = "add this value in seconds to all grain timings – in effect, begin to apply the timefile pattern at this point in the infile." },
  arg5 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg6 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg7 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg8 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg9 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Rerhythm 1 - Change rhythm of grains in a grainy sound based upon time-multipliers list in text file  "] = {
  cmds = { exe = "grain", mode = "rerhythm 1", channels = "any", tip = "1 Lengthen or shorten each grain in turn, without repeating any grains; on reaching the end of the time-multipliers list, cycle back to its start. GRAIN RERHYTHM works in the same way as REMOTIF AND REPITCH, except applied to the timing of the grains only. Thus they retain their original pitch but begin at different times, according to the data in multfile. It may be that the grain lengths appear to change, because they begin to overlap one another: i.e., the distance between grain onsets changes, but the lengths of the grains are not meant to change. TW: do not change... Also see GRAIN TIMEWARP. The two Modes also work in the same way, either applying the data in the textfile to the grains sequentially, or repeating grains, applying all the data in the textfile to each grain in turn. Musical applications; As pitch is not altered, the sense of movement is more directly perceived. The flow of the grains can be made to flex in supple or in strongly contrasting ways. If there is a stuttering or rapidfire quality, it may be caused by the process not adequately separating the grains. If a group of grains are not seen as separate, they will behave as a single grain. It is essential with grain processes to ensure that all the grains are clearly separated (i.e., set the appropriate gate level). In some cases it will not be possible to get a totally satisfactory result (everything depends on the nature of the source). A little cosmetic editing may be required. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multfile", input = "txt", tip = "a file of listing duration-multipliers to change the duration between one grain onset and the next grain onset." },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Rerhythm 2 - Change rhythm of grains in a grainy sound based upon time-multipliers list in text file "] = {
  cmds = { exe = "grain", mode = "rerhythm 2", channels = "any", tip = "2 Play grain at each specified retiming, before proceeding to the next grain. GRAIN RERHYTHM works in the same way as REMOTIF AND REPITCH, except applied to the timing of the grains only. Thus they retain their original pitch but begin at different times, according to the data in multfile. It may be that the grain lengths appear to change, because they begin to overlap one another: i.e., the distance between grain onsets changes, but the lengths of the grains are not meant to change. TW: do not change... Also see GRAIN TIMEWARP. The two Modes also work in the same way, either applying the data in the textfile to the grains sequentially, or repeating grains, applying all the data in the textfile to each grain in turn. Musical applications; As pitch is not altered, the sense of movement is more directly perceived. The flow of the grains can be made to flex in supple or in strongly contrasting ways. If there is a stuttering or rapidfire quality, it may be caused by the process not adequately separating the grains. If a group of grains are not seen as separate, they will behave as a single grain. It is essential with grain processes to ensure that all the grains are clearly separated (i.e., set the appropriate gate level). In some cases it will not be possible to get a totally satisfactory result (everything depends on the nature of the source). A little cosmetic editing may be required. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "multfile", input = "txt", tip = "a file of listing duration-multipliers to change the duration between one grain onset and the next grain onset." },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.032, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, def = 0, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Reverse - Reverse order of grains in a grainy sound without reversing the grains themselves "] = {
  cmds = { exe = "grain", mode = "reverse", channels = "any", tip = " Unlike MODIFY RADICAL Mode 1 (which plays the source sound itself backwards, and thus can radically modify it), this process plays each original grain in the normal way (not reversed in time), but reversed in order. This is much like the idea of retrograding a melodic motif in traditional music (where the sounds themselves are not reversed). The original source is thus always clearly recognisable in the output sound. Musical applications; The forwards orientation of each grain means that the attack transient of the grains is retained. Even through the grains are generally very TW: Even though (Rather than 'even through') short, it is a tribute to the acuteness of our hearing that their forwards orientation makes such a difference in the recognisability of the original. GRAIN REVERSE, therefore, provides a wonderfully subtle variant of the original." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg4 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1)" },
  arg5 = { name = "minhole", switch = "-h", min = 0.032, max = length, def = 0.6, tip = "minimum duration of holes between grains" },
  arg6 = { name = "winsize", switch = "-t", min = 0, max = length, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg7 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
dsp["Grain Timewarp - Stretch (or shrink) the duration of a grainy sound without stretching the grains themselves "] = {
  cmds = { exe = "grain", mode = "timewarp", channels = "any", tip = " T Using a numerical value for timestretch_ratio applies a constant multiplier to all the intergrain times. Thus the grains come constantly faster or slower, depending on the multiplier. In the process the onset times of the grains are timewarped, but not the grains themselves. This is akin, for example, to playing the same melody faster on the same instrument. It therefore differs from other timewarping processes, which also warp the internal architecture of the sounds themselves. Use of a breakpoint file adds considerable flexibility to the process because the function will interpolate gradual changes between different ratios at different times. For example, moving from a multiplier of 1.0 to 2.0 over the time of the infile will produce a gradually decelerando, or from 1.0 to 0.5, a gradual accelerando. Similarly, speeds can be stepped by applying different ratios at marginally different times (it isn't logical to apply two different ratios at precisely the same time). As an example of stepped changes, the following breakpoint file doubles the gap time for the first half of a 1.8 second infile, halves the (original) gap time for 0.6 seconds, returning to the longer gaps for the last part of the sound. Between these times, the timing of the grains remains constant. This is an important tool for creating all kinds of rhythmic patterns." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "timestretch_ratio", min = 0.001, max = 50, input = "brk", def = 2, tip = "the degree of stretching or shrinking of the intergrain time" },
  arg4 = { name = "len", switch = "-b", min = 0.1, max = length/1000, def = 0.1, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg5 = { name = "gate", switch = "-l", min = 0, max = 1, input = "brk", def = 0.3, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "minhole", switch = "-h", min = 0.032, max = 20, def = 0.06, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg7 = { name = "winsize", switch = "-t", min = 0, max = length, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg8 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}

dsp["Grain Assess - Estimate best gate value for grain extraction "] = { 
  cmds = { exe = "grain", mode = "assess", tip = " Understanding the GRAIN ASSESS Process. GRAIN ASSESS is a simple utility to provide useful information when using the GRAIN set of programs. You just provide an input soundfile and it scans the file, displaying a report such as: Maximum grains found = 21 at gate value 0.117100 and windowlen 50ms The program is therefore assessing which gate value and which windowlength value will result in the most grains. You can then use this information in other GRAIN programs. Musical applications; For example, even as simple a program as GRAIN COUNT gives you the option to specify a gate value. GRAIN ASSESS provides the value that will produce the most grains, and you can take it from there. Windowlength also affects the number of grains found. If it is too long, it may count several grains as one, or if too short, it may not find some of the grains. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
}

dsp["Grain Count - Count grains found in a sound (at given gate and minhole values)  "] = { 
  cmds = { exe = "grain", mode = "count", tip = " GRAIN COUNT displays the message: 'N grains found at this gate level.' Musical applications; GRAIN COUNT provides a check on the granularity of a file, and also on the gate level at which a given number of grains is operational." }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "len", switch = "-b", min = 0, max = length / 1000, tip = "maximum time between grains (Range: 1 to duration of infile)" },
  arg3 = { name = "gate", switch = "-l", min = 0.0, max = 1.0, def = 0.3, input = "brk", tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg4 = { name = "minhole", switch = "-h", min = 0.032, max = 1000, def = 0.5, tip = "minimum duration of holes between grains (Minimum allowed value is 0.032 – the Default)" },
  arg5 = { name = "winsize", switch = "-t", min = 0.0, max = length / 1000, tip = "gate level tracks the signal level, as found with a window size of winsize milliseconds (Range: 0.0 to duration of infile)" },
  arg6 = { name = "-x", switch = "-x", tip = "ignore the last grain in the source" },
}
 
------------------------------------------------
-- grainex
------------------------------------------------
 
dsp["Grainex Extend - Find grains in a sound, and extend the area that surrounds them"] = {
  cmds = { exe = "grainex", mode = "extend", tip = "As with the other GRAIN functions, you will need a reasonably grainy sound as an input, otherwise it won't be able to find any grains and you will probably see the message 'Insufficient valid troughs in the file'.GRAINEX extends an area containing grains, the start and end times being set by the user. The grains are found by envelope troughs and zero-crossings.When the above example command line was run with the input file count.wav, when the output sound got to the number 'three', it was repeated several times before continuing on with the rest of the count up to the number ten. The input sound was 8.066 seconds long, and the output was 18.866 seconds long, i.e., considerably more than the 2.5 seconds specified. NOTES; It is possible to set trof too low. Being relative to the peak level, if the signal doesn't drop that far in level, you may get the Error Message: 'Insufficient grains to proceed' or 'INSUFFICIENT PEAKS IN THE FILE AREA SPECIFIED'. The CDP Usage statement also gives the parameter gpcnt (before PLUS): 'number of grains to treat as a unit in the operations'. However, if set, the process reports too many parameters." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "wsiz", min = 1.81406, max = length/3, tip = "size of window in milliseconds, which determines the size of the grains to find (Range: 1.81406 to file-length ÷ 3ms)" },
  arg4 = { name = "trof", min = 0, max = 1, def = 0.5, tip = "acceptable trough depth, relative to adjacent peaks (Range: > 0 to < 1 - default: 0.5)" },
  arg5 = { name = "plus", min = 2e-06, max = 60, tip = "how much duration to add to the source" },
  arg6 = { name = "stt", min = 0, max = length/1000, tip = "time of start of grain material within the source (Range: 0.0 to file-length in seconds)" },
  arg7 = { name = "end", min = 0, max = length/1000, tip = "time of end of grain material within the source (Range: 0 to file-length in seconds and > stt)" },
}
 
------------------------------------------------
-- hilite
------------------------------------------------
 
dsp["Hilite Arpeg - 1 Play components inside arpeggiated band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 1", tip = " HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
  arg10 = { name = "k", switch = "-N", min = 0.02, max = 50, input = "brk", def = 1, tip = "nonlinear decay arpegtones; > 1 faster, < 1 slower; must be > 0 (range: 0.02 to 50 – take care with values higher than about 5, as can reduce output to silence)" },
  arg11 = { name = "S", switch = "-s", min = 0, max = 100, input = "brk", def = 3, tip = "number of windows over which arpegtones sustained: Default = 3 (high values, esp. with a long decay time, can cause amplitude overflow)" },
  arg12 = { name = "-T", switch = "-T", tip = " In sustains, TRACK changing frquency of source (Default = retain start frquency)" },
  arg13 = { name = "-K", switch = "-K", tip = "Let sustains run to zero before new arpegtone attack is accepted (Default: re-attack once sustains fall below current input level)" },
}
 
dsp["Hilite Arpeg - 2 BOOST - Amplify snds in band. Others play unamplified - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 2", tip = ".Amplify snds in band. Others play unamplified. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
  arg10 = { name = "k", switch = "-N", min = 0.02, max = 50, input = "brk", def = 1, tip = "nonlinear decay arpegtones; > 1 faster, < 1 slower; must be > 0 (range: 0.02 to 50 – take care with values higher than about 5, as can reduce output to silence)" },
  arg11 = { name = "S", switch = "-s", min = 0, max = 100, input = "brk", def = 3, tip = "number of windows over which arpegtones sustained: Default = 3 (high values, esp. with a long decay time, can cause amplitude overflow)" },
  arg12 = { name = "-T", switch = "-T", tip = " In sustains, TRACK changing frquency of source (Default = retain start frquency)" },
  arg13 = { name = "-K", switch = "-K", tip = "Let sustains run to zero before new arpegtone attack is accepted (Default: re-attack once sustains fall below current input level)" },
}
 
dsp["Hilite Arpeg - 3 BELOW BOOST -INITIALLY play components in & below band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 3", tip = "BELOW_BOOST.....INITIALLY play components in & below band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 2, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
  arg10 = { name = "k", switch = "-N", min = 0.02, max = 50, input = "brk", def = 1, tip = "nonlinear decay arpegtones; > 1 faster, < 1 slower; must be > 0 (range: 0.02 to 50 – take care with values higher than about 5, as can reduce output to silence)" },
  arg11 = { name = "S", switch = "-s", min = 0, max = 100, input = "brk", def = 3, tip = "number of windows over which arpegtones sustained: Default = 3 (high values, esp. with a long decay time, can cause amplitude overflow)" },
  arg12 = { name = "-T", switch = "-T", tip = " In sustains, TRACK changing frquency of source (Default = retain start frquency)" },
  arg13 = { name = "-K", switch = "-K", tip = "Let sustains run to zero before new arpegtone attack is accepted (Default: re-attack once sustains fall below current input level)" },
}
 
dsp["Hilite Arpeg - 4 ABOVE_BOOST - INITIALLY play components in & above band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 4", tip = "ABOVE_BOOST - INITIALLY play components in & above band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
  arg10 = { name = "k", switch = "-N", min = 0.02, max = 50, input = "brk", def = 1, tip = "nonlinear decay arpegtones; > 1 faster, < 1 slower; must be > 0 (range: 0.02 to 50 – take care with values higher than about 5, as can reduce output to silence)" },
  arg11 = { name = "S", switch = "-s", min = 0, max = 100, input = "brk", def = 3, tip = "number of windows over which arpegtones sustained: Default = 3 (high values, esp. with a long decay time, can cause amplitude overflow)" },
  arg12 = { name = "-T", switch = "-T", tip = " In sustains, TRACK changing frquency of source (Default = retain start frquency)" },
  arg13 = { name = "-K", switch = "-K", tip = "Let sustains run to zero before new arpegtone attack is accepted (Default: re-attack once sustains fall below current input level)" },
}
 
dsp["Hilite Arpeg - 5 BELOW - Play components in & below arpeggiated band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 5", tip = "ABOVE_BOOST - INITIALLY play components in & above band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
}
 
dsp["Hilite Arpeg - 6 ABOVE - Play components in & above arpeggiated band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 6", tip = "ABOVE - Play components in & above arpeggiated band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
}
 
dsp["Hilite Arpeg - 7 ONCE_BELOW - INITIALLY Play components in and below band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 7", tip = "ONCE_BELOW - INITIALLY Play components in and below band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 2, max = 4, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
}
 
dsp["Hilite Arpeg - 8 ONCE_ABOVE - INITIALLY Play components in and above arpeggiated band ONLY - Arpeggiate the spectrum "] = {
  cmds = { exe = "hilite", mode = "arpeg 8", tip = "ONCE_ABOVE - INITIALLY Play components in and above arpeggiated band ONLY. HILITE ARPEG means 'spectrum arpeggio' and causes a waveform at a specified frequency to sweep through the spectrum of a sound, selecting or emphasizing the partials it passes through (or which lie below it or above it), thus creating an 'arpeggiation' of the spectrum within the sound. Note the interaction of the -N and -s parameters. The one is controlling the envelope of the decay, and the other is sustaining across windows. When a slow decay envelope (e.g., 0.2) is combined with a large number of windows across which to sustain data (e.g., > 100), amplitude overflow can occur, sometimes massive amplitude overflow if the source is loud. If PVOC comments on amplitude, take care to to read what it says before playing the sound. Values for -a also affect amplitude output, so when near the limit with other factors, a lower value for -a can be helpful. High values for -N can muffle the sound, and very high values can reduce it to silence. You are recommended not to exceed a value of 7. Rule of Thumb: increase the decay rate (higher value for -N) as you increase the sustain windows (-s). The -T parameter causes arpeggios to follow (the moving) pitch content of the sound. The -K parameter places the attacks in relief by making sure that the sustains run to zero before a new attack begins. The appropriate rate parameter value can be imagined by likening the effect to a vibrato, i.e., a low frequency oscillator. Something about 10 to 20 cycles per second is what is expected, but slower than this can be used to create glissandos within the spectrum. A single sweep through the sound can be useful, such as to create a sense of upwards transposition. CDP user Peter Karkut came up with this idea as a way of preparing a sound for a morph. The rate parameter, we are told, must be less than the analysis rate. Note that this is NOT the value given for -N (now -c points when analyzing sound, but rather the analysis frame rate. As explained in the Phase Vocoder Manual, this is (SR/2)/(N/2), where SR is the sample rate and N is the value given for -N. The result of this calculation is then multiplied by 8, the number of window overlaps automatically made by the Phase Vocoder in order to improve the quality of the output. A typical frame rate is 344: (44100/2)/(1024/2) = 43.06;43.06 * 8 = 344. So in this case, rate must be < 344. In practice, a value of 100 is about the limit, but the software does not prevent exploring what happens beyond this point. Musical applications; HILITE ARPEG can be used to create an unusual attack for a sound object. Because the wave sweeps through all the analysis channels, it serves to bring out internal features inside the sound. Sometimes a pitch focus or harmonic colouration within the sound will result, particularly if the frequencies are being tracked (-T). This process seems to work most clearly with sounds possessing strong and slowly moving partials, such as bell sounds and sounds perceived mostly as pitch (harmonic partials predominate over inharmonic partials). Perception of the effect is also affected by the speed of the movement; it is more noticeable at slower speeds. This can be timed to give a single sweep over the duration of the sound because the rate can be a floating point value (> 0 but < the analysis rate). It might be interesting to try it on the result of a SPEC BARE process, because then the sweeping arpeggio will be emphasising the natural harmonic overtone series. High sharp sweeps can be achieved with Mode 6, e.g., by a high value for -h and none for -l: makes the sweeps occur above the -h frequency. Try pushing this higher still, and playing with rates < 1. There are many factors to play off against one another in HILITE ARPEG, so creating a series of batch files with which to explore the function would be time well spent. There are many serendipitous sounds hidden among the more obvious possibilities. It is more of the most powerful functions in the spectral set. (Ed.)" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "wave", min = 1, max = 3, def = 2, tip = "1 = downramp : 2 = sin : 3 = saw : 4 = upramp" },
  arg4 = { name = "rate", min = 0, max = length, input = "brk", tip = "number of sweeps per second (can be < 1)" },
  arg5 = { name = "U", switch = "-p", min = 0, max = 1, def = 0, tip = " start_phase: range 0-1 (limited range for some cases); may not affect the sound very much" },
  arg6 = { name = "X", switch = "-l", min = 0, max = 22050, input = "brk", def = 0, tip = " lowest frequency arpeg sweeps down to; Default = 0" },
  arg7 = { name = "Y", switch = "-h", min = 0, max = 22050, input = "brk", def = 22050, tip = "highest frequency arpeg sweeps up to; Default nyquist" },
  arg8 = { name = "Z", switch = "-b", min = 0, max = 22050, input = "brk", tip = "bandwidth of sweep band (in Hz); Default = nyquist/channel_cnt (i.e., sample rate/2/channel count)" },
  arg9 = { name = "A", switch = "-a", min = 0, max = 100, input = "brk", def = 10, tip = "amplification of arpegtones; Default = 10.0" },
}
 
dsp["Hilite Band - Split spectrum into bands and process these individually, specified in datafile "] = {
  cmds = { exe = "hilite", mode = "band", tip = " HILITE BAND used to be called SPECSPLIT, which means 'spectrum split': divide the spectrum into a number of (user-defined) frequency bands. It then carries out amplitude and/or pitch changes to the partials found in the bands. Each band may be treated differently. Typical use of HILITE BAND involves defining several data lines in order to create an arbitrary spectral filter envelope. Each data line is a single line contributing to a sort of frequency response breakpoint shape. Musical applications; When you set a frequency band to zero, it is filtered out (bit 1 set to 0). By retaining/emphasizing some frequency bands and zeroing others, you can focus on and enhance the bands you want to keep. This is also a way to tune or harmonize sound material. The data file for HILITE BAND can contain several lines, so several bands can be, and usually are, defined at one time. Filter slope can be applied within frequency bands (bit 2 set and values given for amp1 and amp2). In the line 3500 6000 1100 0.001 1.0, the slope moves from 3500 Hz upwards towards 6000 Hz, and in the line 2000 3000 1100 1.0 0.001, the slope moves downwards from 2000 Hz to 3000 Hz. The two lines as a pair give this shape: 2000 3000 1100 1.0   0.001 3500 6000 1100 0.001 1.0. Note the potential of the transposition functions (bit 3 and bit 4). Transposition by multiplication means that the harmonic relationships are maintained (logarithmic). Transposition by addition creates inharmonic relationships (linear). The superimposition of frequency bands made possible by bit 4 will have the effect of thickening the resultant sound in some way – which probably won't be very predictable. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "datafile", input = "txt", tip = "datafile contains the instructions on how to split the bands, written as (a number of) lines in the following format: 'lofrq hifrq bitflag [amp1 amp2 [+]transpose]'" },
}
 
dsp["Hilite Bltr - Time-average and TRACE the spectrum"] = {
  cmds = { exe = "hilite", mode = "bltr", tip = "In each window, all but the loudest (tracing) channels are discarded (see HILITE TRACE. Then a proportion of the windows are discarded and replaced by values interpolated between the remaining windows (see BLUR BLUR. The degree of the change will depend on the amplitude differential between the loudest and not so loud channels: the louder the latter are, the more their absence will be noticed. Musical applications; The dual functionality of HILITE BLTR enables the sound designer to reduce the data by removing all but the tracing loudest channels, and simultaneously to smooth/blur the remaining data by averaging the frequency and amplitude values contained in each set of blurring windows. This is a good way to get softly modulating sounds. However, we have seen with HILITE TRACE that a remarkable number of windows can go before the recognisability of the sound is seriously affected, so there is plenty of scope to soften the edges of a sound before recognisability is lost. A high blur factor combined with a low trace factor greatly reduces and smooths the sound: reduces it to a gentle murmur. You might also consider using FOCUS ACCU to alter/intensify the blurring effect. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "blurring", min = 2, max = 32768, input = "brk", tip = "the number of windows over which to average the spectrum" },
  arg4 = { name = "tracing", min = 0, max = 32768, input = "brk", tip = "the number of (loudest) channels to retain, in each window" },
}
 
dsp["Hilite Filter - 1 Highpass filter - Filter the spectrum"] = {
  cmds = { exe = "hilite", mode = "filter 1", tip = "Highpass filter - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 10 notch filter (normalised output)"] = {
  cmds = { exe = "hilite", mode = "filter 10", tip = "notch filter (normalised output) - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 11 band pass filter with gain"] = {
  cmds = { exe = "hilite", mode = "filter 11", tip = "11 band pass filter with gain - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
  arg6 = { name = "gain", min = 0, max = 100, def = 1, tip = "amplification of the resulting sound" },
}
 
dsp["Hilite Filter - 12 notch filter with gain"] = {
  cmds = { exe = "hilite", mode = "filter 12", tip = "12 notch filter with gain - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
  arg6 = { name = "gain", min = 0, max = 100, def = 1, tip = "amplification of the resulting sound" },
}
 
dsp["Hilite Filter - 2 Highpass filter (normalised output) - Filter the spectrum"] = {
  cmds = { exe = "hilite", mode = "filter 2", tip = "Highpass filter (normalised output) - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 3 low pass filter - Filter the spectrum"] = {
  cmds = { exe = "hilite", mode = "filter 3", tip = "low pass filter - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 4 low pass filter (normalised output) - Filter the spectrum"] = {
  cmds = { exe = "hilite", mode = "filter 4", tip = "low pass filter (normalised output) - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 5 high pass filter with gain) - Filter the spectrum"] = {
  cmds = { exe = "hilite", mode = "filter 5", tip = "high pass filter with gain - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
  arg5 = { name = "gain", min = 0, max = 100, def = 1, tip = "amplification of the resulting sound" },
}
 
dsp["Hilite Filter - 6 low pass filter with gain"] = {
  cmds = { exe = "hilite", mode = "filter 6", tip = "low pass filter with gain - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
  arg5 = { name = "gain", min = 0, max = 100, def = 1, tip = "amplification of the resulting sound" },
}
 
dsp["Hilite Filter - 7 band pass filter"] = {
  cmds = { exe = "hilite", mode = "filter 7", tip = "band pass filter - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 8 band pass filter (normalised output)"] = {
  cmds = { exe = "hilite", mode = "filter 8", tip = "band pass filter (normalised output) - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Filter - 9 notch filter)"] = {
  cmds = { exe = "hilite", mode = "filter 9", tip = "notch filter - These are standard filtering options, which can be applied directly in the spectral domain. Note that Q is entered as a frequency bandwidth, which hopefully will be a more intuitive way to use this parameter. Musical applications; The key facilities here are the time-varying frequency settings and Q. One can move the frequency placement about as the sound progresses, as well as change the sharpness (pitch focus) of the filter. Suppose, for example, you extracted a pitch trace and saved the data as a breakpoint file. This file could be used as a time-varying frq1 with a time-varying Q, so that the salient pitch content of the file can be made to fade in and out. Similarly, upward and downward sweeps which begin with a broad and end with a sharp pitch focus or v.vs. (Q), affecting changes over time, emerging or disappearing pitches etc. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq1", min = 0, max = 22050, input = "brk", def = 10000, tip = "filter cutoff frequency" },
  arg4 = { name = "frq2", min = 0, max = 22050, input = "brk", def = 5000, tip = "limits of filter band" },
  arg5 = { name = "Q", min = 0, max = 22050, input = "brk", tip = "width of filter skirts, in Hz (Range: > 0)" },
}
 
dsp["Hilite Greq - 1 single bandwidth for all filter bands - Graphic EQ type filter on the spectrum specified in filt text file"] = {
  cmds = { exe = "hilite", mode = "greq 1", tip = "single bandwidth for all filter bands. This is in concept a standard graphic equaliser, with a choice between bands of equal size or bands of varyingly specified sizes. A sonogram display may help to show how strong signal areas may be reduced (or enhanced by removing bands of frequencies which lie between them). Also see the spectral information gathering functions listed below. Musical applications; The graphic equaliser is typically used to boost certain selected parts of a sound, and reduce others, such as high level hiss or low booming. This 'EQ type' filter does not have a gain parameter as such and focuses on highlighting (or removing) the selected bands. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "filtfile", input = "txt", tip = "text file defining the graphic equaliser - see docs for syntax" },
  arg4 = { name = "-r", switch = "-r", tip = "Band reject (notch) filter. Default is a bandpass filter." },
}
 
dsp["Hilite Greq - 2 separate bandwidths for each filter band - Graphic EQ type filter on the spectrum specified in filt text file "] = {
  cmds = { exe = "hilite", mode = "greq 2", tip = "separate bandwidths for each filter band. This is in concept a standard graphic equaliser, with a choice between bands of equal size or bands of varyingly specified sizes. A sonogram display may help to show how strong signal areas may be reduced (or enhanced by removing bands of frequencies which lie between them). Also see the spectral information gathering functions listed below. Musical applications; The graphic equaliser is typically used to boost certain selected parts of a sound, and reduce others, such as high level hiss or low booming. This 'EQ type' filter does not have a gain parameter as such and focuses on highlighting (or removing) the selected bands. The following programs provide data which could be useful when deciding how to set the filter frequencies (use BACK in your Browser to return here to HILITE FILTER): SPECINFO PEAK and SPECINFO REPORT provide information on (time-varying) peaks in the spectral envelope. REPITCH GETPITCH can extract a pitch trace, saving it as either a binary pitch data file or a breakpoint file, and PITCHINFO WRITE can convert a binary pitch data file into a breakpoint file. PITCHINFO INFO gives an overview of the (main) pitch content of a binary pitch data file." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "filtfile", input = "txt", tip = "text file defining the graphic equaliser, see docs for correct syntax" },
  arg4 = { name = "-r", switch = "-r", tip = "Band reject (notch) filter. Default is a bandpass filter." },
}
 
dsp["Hilite Pluck - Emphasise spectral changes"] = {
  cmds = { exe = "hilite", mode = "pluck", tip = "In analysis files generated from complex soundfiles by functions such as HILITE TRACE and HILITE ARPEG, new partials will enter (and leave) the spectrum, window by window, creating a kind of 'resultant melody'. HILITE PLUCK emphasises the entry of these new partials by boosting their amplitude at the point of entry. If the amplitude boost is large, such as increased by a gain of 10 or more, the entries may suggest a 'plucking' of the entering partials. Note that the 'pluck' effect is added to the onset of partials. This is quite different from emphasising the attack transient of a sound in the amplitude envelope. See ENVEL PLUCK for this, where a noise component is introduced into the attack transient of a sound. Musical applications; This procedure will introduce into the sound a somewhat random rhythm of short pulses. These pulses will attract the ear to the entering partials and whatever harmonic relationship they may have ­ such as the harmonic partials retained after running SPEC BARE. SPEC BARE + HILITE TRACE + HILITE PLUCK form an interesting combination of functions to play with, and HILITE PLUCK can also be used to bring out the partials which HILITE ARPEG makes more audible. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", min = 1, max = 1000, input = "brk", def = 2, tip = " amplitude gain applied to newly prominent spectral components (Range: 1 to 1000)" },
}
 
dsp["Hilite Trace - Select loudest from above lofrq: Reject all spectral data below lofrq - Highlight n loudest partials, at each moment (window) in time"] = {
  cmds = { exe = "hilite", mode = "trace 2", tip = "Select loudest from above lofrq: Reject all spectral data below lofrq - it looks for and retains only the N loudest partials in the analysis data on a window-by-window basis. This reduces the data in the spectral dimension and produces an aural 'trace' of the original sound. With non-'noisy' sources it is necessary to reduce the number of channels quite considerably to make any appreciable aural change to the source sound. Even the 10 loudest channels will retain a surprising amount of the original sound. The flip-side of this is that HILITE TRACE can be used to 'clean' a sound, if a certain amount of data loss is not a problem (see below). My understanding of this procedure is that it deals with analysis channels, not directly with partials. It will retain whichever partial is contained in a given analysis channel window at any given moment in time, and this is likely to change. To find specific partials requires 'peak tracking', and this is not something which the CDP software is able to do. Thus a sonogram of 1 channel retained by HILITE TRACE will probably show a number of different frequencies. The frequency width of an analysis channel is obtained by dividing the sample rate by the number of channels: e.g., 44100 / 1024 = 43. This enables you to calculate the width of the frequency band that a given number of channels will cover. Remember that partials are more spread out in higher sounds (frequency is logarithmic), so low sounds are likely to contain more partials within a smaller band, while the same width might find relatively few in a high sound. For an alternative approach, see http://mpex.prosoniq.com.[AE] The HILITE TRACE filtering effect can be further controlled by using the lofrq and hifrq parameters. The default is to omit the data outside the selected area, but if the -r flag is used, this data is retained. For example, the frequency band can be focused into a narrower band by creating a breakpoint file for hifrq which starts high and moves lower, and creating a breakpoint file for lofrq which starts low and moves higher (but stays below the final frequency value in hifrq). Musical applications; Initially, this process can be used to clean up some of the ambient noise which is part of a sound source, if for example, only 50 or 100 channels are retained. However, some spectral information may be lost as well, when cleanup is done in this way. However, as one moves down to 50, 40, 30, 20, 10, 5 ..., less and less of the original remains, though it is surprising how low one has to go to get a really 'ghostly' result. At this stage, the original is being used simply as a source for another sound. Strongly and rapidly changing sounds can produce a burbling effect. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 0, max = 32768, input = "brk", def = 2, tip = "N is the trace index: the number of spectral components to retain" },
  arg4 = { name = "lofrq", min = 0, max = 22050, input = "brk", def = 60, tip = "low freq" },
  arg5 = { name = "-r", switch = "-r", input = "brk", tip = "If trace index N is greater than the number of channels in the filterband (defined by lofrq & hifrq): RETAIN the loudest channels OUTSIDE the filterband (Default: always omit channels outside the filterband)" },
}
 
dsp["Hilite Trace - Select loudest from below hifrq: Reject all spectral data above hifrq - Highlight n loudest partials, at each moment (window) in time"] = {
  cmds = { exe = "hilite", mode = "trace 3", tip = "Select loudest from below hifrq: Reject all spectral data above hifrq - it looks for and retains only the N loudest partials in the analysis data on a window-by-window basis. This reduces the data in the spectral dimension and produces an aural 'trace' of the original sound. With non-'noisy' sources it is necessary to reduce the number of channels quite considerably to make any appreciable aural change to the source sound. Even the 10 loudest channels will retain a surprising amount of the original sound. The flip-side of this is that HILITE TRACE can be used to 'clean' a sound, if a certain amount of data loss is not a problem (see below). My understanding of this procedure is that it deals with analysis channels, not directly with partials. It will retain whichever partial is contained in a given analysis channel window at any given moment in time, and this is likely to change. To find specific partials requires 'peak tracking', and this is not something which the CDP software is able to do. Thus a sonogram of 1 channel retained by HILITE TRACE will probably show a number of different frequencies. The frequency width of an analysis channel is obtained by dividing the sample rate by the number of channels: e.g., 44100 / 1024 = 43. This enables you to calculate the width of the frequency band that a given number of channels will cover. Remember that partials are more spread out in higher sounds (frequency is logarithmic), so low sounds are likely to contain more partials within a smaller band, while the same width might find relatively few in a high sound. For an alternative approach, see http://mpex.prosoniq.com.[AE] The HILITE TRACE filtering effect can be further controlled by using the lofrq and hifrq parameters. The default is to omit the data outside the selected area, but if the -r flag is used, this data is retained. For example, the frequency band can be focused into a narrower band by creating a breakpoint file for hifrq which starts high and moves lower, and creating a breakpoint file for lofrq which starts low and moves higher (but stays below the final frequency value in hifrq). Musical applications; Initially, this process can be used to clean up some of the ambient noise which is part of a sound source, if for example, only 50 or 100 channels are retained. However, some spectral information may be lost as well, when cleanup is done in this way. However, as one moves down to 50, 40, 30, 20, 10, 5 ..., less and less of the original remains, though it is surprising how low one has to go to get a really 'ghostly' result. At this stage, the original is being used simply as a source for another sound. Strongly and rapidly changing sounds can produce a burbling effect. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 0, max = 32768, input = "brk", def = 2, tip = "N is the trace index: the number of spectral components to retain" },
  arg4 = { name = "hifrq", min = 0, max = 22050, input = "brk", def = 60, tip = "high freq" },
  arg5 = { name = "-r", switch = "-r", input = "brk", tip = "If trace index N is greater than the number of channels in the filterband (defined by lofrq & hifrq): RETAIN the loudest channels OUTSIDE the filterband (Default: always omit channels outside the filterband)" },
}
 
dsp["Hilite Trace - Select loudest from between lofrq and hifrq: Reject data outside - Highlight n loudest partials, at each moment (window) in time"] = {
  cmds = { exe = "hilite", mode = "trace 4", tip = "Select loudest from between lofrq and hifrq: Reject data outside - it looks for and retains only the N loudest partials in the analysis data on a window-by-window basis. This reduces the data in the spectral dimension and produces an aural 'trace' of the original sound. With non-'noisy' sources it is necessary to reduce the number of channels quite considerably to make any appreciable aural change to the source sound. Even the 10 loudest channels will retain a surprising amount of the original sound. The flip-side of this is that HILITE TRACE can be used to 'clean' a sound, if a certain amount of data loss is not a problem (see below). My understanding of this procedure is that it deals with analysis channels, not directly with partials. It will retain whichever partial is contained in a given analysis channel window at any given moment in time, and this is likely to change. To find specific partials requires 'peak tracking', and this is not something which the CDP software is able to do. Thus a sonogram of 1 channel retained by HILITE TRACE will probably show a number of different frequencies. The frequency width of an analysis channel is obtained by dividing the sample rate by the number of channels: e.g., 44100 / 1024 = 43. This enables you to calculate the width of the frequency band that a given number of channels will cover. Remember that partials are more spread out in higher sounds (frequency is logarithmic), so low sounds are likely to contain more partials within a smaller band, while the same width might find relatively few in a high sound. For an alternative approach, see http://mpex.prosoniq.com.[AE] The HILITE TRACE filtering effect can be further controlled by using the lofrq and hifrq parameters. The default is to omit the data outside the selected area, but if the -r flag is used, this data is retained. For example, the frequency band can be focused into a narrower band by creating a breakpoint file for hifrq which starts high and moves lower, and creating a breakpoint file for lofrq which starts low and moves higher (but stays below the final frequency value in hifrq). Musical applications; Initially, this process can be used to clean up some of the ambient noise which is part of a sound source, if for example, only 50 or 100 channels are retained. However, some spectral information may be lost as well, when cleanup is done in this way. However, as one moves down to 50, 40, 30, 20, 10, 5 ..., less and less of the original remains, though it is surprising how low one has to go to get a really 'ghostly' result. At this stage, the original is being used simply as a source for another sound. Strongly and rapidly changing sounds can produce a burbling effect. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 0, max = 32768, input = "brk", def = 2, tip = "N is the trace index: the number of spectral components to retain" },
  arg4 = { name = "lofrq", min = 0, max = 22050, input = "brk", def = 60, tip = "low frq" },
  arg5 = { name = "hifrq", min = 0, max = 22050, input = "brk", def = 60, tip = "high frq" },
  arg6 = { name = "-r", switch = "-r", input = "brk", tip = "If trace index N is greater than the number of channels in the filterband (defined by lofrq & hifrq): RETAIN the loudest channels OUTSIDE the filterband (Default: always omit channels outside the filterband)" },
}
 
dsp["Hilite Trace - Select loudest spectral components - Highlight n loudest partials, at each moment (window) in time"] = {
  cmds = { exe = "hilite", mode = "trace 1", tip = " Select loudest spectral components - it looks for and retains only the N loudest partials in the analysis data on a window-by-window basis. This reduces the data in the spectral dimension and produces an aural 'trace' of the original sound. With non-'noisy' sources it is necessary to reduce the number of channels quite considerably to make any appreciable aural change to the source sound. Even the 10 loudest channels will retain a surprising amount of the original sound. The flip-side of this is that HILITE TRACE can be used to 'clean' a sound, if a certain amount of data loss is not a problem (see below). My understanding of this procedure is that it deals with analysis channels, not directly with partials. It will retain whichever partial is contained in a given analysis channel window at any given moment in time, and this is likely to change. To find specific partials requires 'peak tracking', and this is not something which the CDP software is able to do. Thus a sonogram of 1 channel retained by HILITE TRACE will probably show a number of different frequencies. The frequency width of an analysis channel is obtained by dividing the sample rate by the number of channels: e.g., 44100 / 1024 = 43. This enables you to calculate the width of the frequency band that a given number of channels will cover. Remember that partials are more spread out in higher sounds (frequency is logarithmic), so low sounds are likely to contain more partials within a smaller band, while the same width might find relatively few in a high sound. For an alternative approach, see http://mpex.prosoniq.com.[AE] The HILITE TRACE filtering effect can be further controlled by using the lofrq and hifrq parameters. The default is to omit the data outside the selected area, but if the -r flag is used, this data is retained. For example, the frequency band can be focused into a narrower band by creating a breakpoint file for hifrq which starts high and moves lower, and creating a breakpoint file for lofrq which starts low and moves higher (but stays below the final frequency value in hifrq). Musical applications; Initially, this process can be used to clean up some of the ambient noise which is part of a sound source, if for example, only 50 or 100 channels are retained. However, some spectral information may be lost as well, when cleanup is done in this way. However, as one moves down to 50, 40, 30, 20, 10, 5 ..., less and less of the original remains, though it is surprising how low one has to go to get a really 'ghostly' result. At this stage, the original is being used simply as a source for another sound. Strongly and rapidly changing sounds can produce a burbling effect. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", min = 0, max = 32768, input = "brk", def = 2, tip = "N is the trace index: the number of spectral components to retain" },
}
 
dsp["Hilite Vowels - Impose vowels on a sound specified in vowels text file"] = {
  cmds = { exe = "hilite", mode = "vowels", tip = "The vowelfile is a straightforward breakpoint file, with the times in the left column and the vowels as defined above in the right column: [times  vowels] 0.0    ai, 3.0    ee, 6.0    oa, 9.0    ar, 12.0    i, This process allows a sequence of vowels to be imposed over a spectrum derived from an input sound. The spectrum itself must have enough energy over all frequency ranges for the vowel formants to 'bite' e.g., a very pure pitch sound (a sine tone) only has energy at one place in the spectrum. The vowel formant has nothing much to shape, and the vowel sound will not be formed. Noisy sounds are best. If you want to impose vowel formants on strongly pitched sounds if may be best to extract the pitch and then use REPITCH VOWELS OVER PITCHFILE. The process glides between vowels specified at different times in a breakpont file. e.g. if you set the vowel 'a' at time 0 and also at time '1', the output will generate an 'a' sound. But if the vowel at time 1 is set to 'i', the output will do a vowel-glide (diphthong) from 'a' to 'i'. Speech characteristics can thus be imposed on pitch contours derived from pitched source. The sound onto which the vowels are imposed needs to a rich, noisy sound with fairly evenly spread energy. Otherwise, the vowel contours may find that they have nothing in the original sound to with which to work. Musical applications; This process can be used to create time-varying spectra or even the illusion of speech." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "vowelfile", min = 0, max = 0, input = "txt", def = 0, tip = "a text file containing paired times (must start at 0 and increase) and vowels" },
  arg4 = { name = "halfwidth", min = 0, max = 1, def = 0.25, tip = "half-width of formant peaks in Hz as a fraction of the formant centre frequency. Default: 0.25" },
  arg5 = { name = "steepness", min = 0.1, max = 10, def = 3, tip = "steepness of formant peaks, similar to 'Q'in filtering: steeper means more resonant. Range: 0.1 to 10. Default: 3" },
  arg6 = { name = "range", min = 0, max = 1, def = 0.95, tip = "the formant peaks stand above the signal floor. The range of the formant peaks is therefore a part of the total range of the signal. Range is a ratio of (maximum) peak height to (maximum) total range. Default: 0.95" },
  arg7 = { name = "threshold", min = 0, max = 10, def = 0.5, tip = "the spectral window's level is compared with the vowel envelope level. If it exceeds a certain proportion of that level, it is forced to the vowel envelope level. Otherwise, it remains where it is lest it amplify background noise artificially. Threshold defines this proportion. Default: 0.5" },
}
 
------------------------------------------------
-- housekeep
------------------------------------------------
 
dsp["Housekeep Bakup - Concatenate 2 soundfiles in one backup file, with silences inbetween"] = {
  cmds = { exe = "housekeep", mode = "bakup", channels = "any", tip = "The input sounds are assembled into a single file in the same order that they are listed, with a few seconds of silence inbetween each file. It is possible to have only one input soundfile to back up. This wouldn't normally happen, but you could be backing up groups of files to named backup files, and one of these groups might have only one member. Musical applications; HOUSEKEEP BAKUP is similar to SFEDIT JOIN except that silences are placed between the sounds. This makes it easy to transfer a list of soundfiles to tape or CD, to be listened to as separate soundfiles." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}

dsp["Housekeep Batchexpand - 1 Expand an existing batch file"] = {
  cmds = { exe = "housekeep", mode = "batchexpand 1", tip = "Mode 1: The existing batchfile must operate on only one soundfile as input to the process(es). The same sequence of operations is applied to each in-soundfile using each new parameter in turn. Thus a 3-line batchfile with 3 sound inputs becomes a 9-line file.Musical applications; Given the restrictions on matching formats, HOUSEKEEP BATCHEXPAND can help to speed up ringing the changes on exploring variant soundfile transformations. In Mode 1 you just have to alter the datafile and supply the names of the input soundfiles you want to process. It then processes each input soundfile with each of the parameter values in the datafile. In Mode 2, the first parameter is applied to the first process and soundfile, the second to the second process and soundfile (could be the same process), etc. Your soundfile inputs supply different soundfiles to the existing batchfile." },
  arg1 = { name = "batchfile", input = "data", tip = "an existing batchfile (.bat) that uses the same kind of parameter in one of its columns." },
  arg2 = { name = "sndfile1", input = "string", def = "", tip = "name of a soundfile in the batchfile, different to or identical with the names in the existing batchfile" },
  arg3 = { name = "sndfile2", input = "string", def = "", tip = "name of a soundfile in the batchfile, different to or identical with the names in the existing batchfile" },
  arg4 = { name = "datafile", input = "txt", tip = "text file of parameter values, where the 1st parameter applies to the 1st soundfile, the 2nd parameter applies to the 2nd soundfile, etc." },
  arg5 = { name = "ic", min = 1, max = 100, def = 2, tip = "the number of the column in the batchfile containing the input filename (usually '2')" },
  arg6 = { name = "oc", min = 1, max = 100, def = 3, tip = "the number of the column in the batchfile containing the output filename (usually '3')" },
  arg7 = { name = "pc", min = 1, max = 100, def = 4, tip = "the number of the column in the batchfile containing the parameter to be replaced (usually '4' ...)" },
}

dsp["Housekeep Batchexpand - 2 Expand an existing batch file"] = {
  cmds = { exe = "housekeep", mode = "batchexpand 2", tip = "Mode 2: The existing batchfile can have only a single line. The soundfile in the original batchfile is replaced by the chosen input soundfile(s), using new parameters. Thus the 1-line batchfile, when given 3 sound inputs, applies its process to each soundfile in turn, producing 3 outputs. This implies the presence of 3 different values in the datafile..Musical applications; Given the restrictions on matching formats, HOUSEKEEP BATCHEXPAND can help to speed up ringing the changes on exploring variant soundfile transformations. In Mode 1 you just have to alter the datafile and supply the names of the input soundfiles you want to process. It then processes each input soundfile with each of the parameter values in the datafile. In Mode 2, the first parameter is applied to the first process and soundfile, the second to the second process and soundfile (could be the same process), etc. Your soundfile inputs supply different soundfiles to the existing batchfile." },
  arg1 = { name = "batchfile", input = "data", tip = "an existing batchfile (.bat) that uses the same kind of parameter in one of its columns." },
  arg2 = { name = "sndfile1", input = "string", def = "", tip = "name of a soundfile in the batchfile, different to or identical with the names in the existing batchfile" },
  arg3 = { name = "sndfile2", input = "string", def = "", tip = "name of a soundfile in the batchfile, different to or identical with the names in the existing batchfile" },
  arg4 = { name = "datafile", input = "txt", tip = "text file of parameter values, where the 1st parameter applies to the 1st soundfile, the 2nd parameter applies to the 2nd soundfile, etc." },
  arg5 = { name = "ic", min = 1, max = 100, def = 2, tip = "the number of the column in the batchfile containing the input filename (usually '2')" },
  arg6 = { name = "oc", min = 1, max = 100, def = 3, tip = "the number of the column in the batchfile containing the output filename (usually '3')" },
  arg7 = { name = "pc", min = 1, max = 100, def = 4, tip = "the number of the column in the batchfile containing the parameter to be replaced (usually '4' ...)" },
}

dsp["Housekeep Bundle - 1 Bundle all files entered. List files into a textfile, For sorting, backup or creating a dummy mixfile"] = {
  cmds = { exe = "housekeep", mode = "bundle 1", tip = "1 Bundle all files entered. You enter your all your filenames on the command line, and the program groups them according to the Mode instructions and writes an appropriate text file. Musical applications; This facility could be used to document a project or to create the first stage of a mixfile. SUBMIX DUMMY can then be used to take a list of sounds a step further by creating a dummy mixfile – 'dummy' because it is presumed that some further editing will be required. When entering soundfile etc. names, you don't need to include the extension if the environment variable CDP_SOUND_EXT has been set. HOUSEKEEP BUNDLE and SUBMIX DUMMY make a great time-saving combination when creating mixfiles. SUBMIX DUMMY no longer takes a text file as an input: the files are listed on the command line (the graphic user interfaces have their own ways of doing this). The text file created in HOUSEKEEP DUMMY can therefore only be used as a reference guide when entering the filenames." },
  arg1 = { name = "infile", input = "data", tip = "any type of file with any extension, e.g., asound.wav, atext.txt, amix.mix" },
  arg2 = { name = "infile2", input = "data", tip = "continued series of file inputs" },
  arg3 = { name = "infile3", input = "data", tip = "continued series of file inputs" },
  arg4 = { name = "infile4", input = "data", tip = "continued series of file inputs" },
  arg5 = { name = "outtextfile", output = "txt", tip = "textfile containing a grouped list of all the infiles" },
}

dsp["Housekeep Bundle - 2 Bundle all non-text files entered, For sorting, backup or creating a dummy mixfile"] = {
  cmds = { exe = "housekeep", mode = "bundle 2", tip = "2 Bundle all non-text files entered. You enter your all your filenames on the command line, and the program groups them according to the Mode instructions and writes an appropriate text file. Musical applications; This facility could be used to document a project or to create the first stage of a mixfile. SUBMIX DUMMY can then be used to take a list of sounds a step further by creating a dummy mixfile – 'dummy' because it is presumed that some further editing will be required. When entering soundfile etc. names, you don't need to include the extension if the environment variable CDP_SOUND_EXT has been set. HOUSEKEEP BUNDLE and SUBMIX DUMMY make a great time-saving combination when creating mixfiles. SUBMIX DUMMY no longer takes a text file as an input: the files are listed on the command line (the graphic user interfaces have their own ways of doing this). The text file created in HOUSEKEEP DUMMY can therefore only be used as a reference guide when entering the filenames." },
  arg1 = { name = "infile", input = "data", tip = "any type of file with any extension, e.g., asound.wav, atext.txt, amix.mix" },
  arg2 = { name = "infile2", input = "data", tip = "continued series of file inputs" },
  arg3 = { name = "infile3", input = "data", tip = "continued series of file inputs" },
  arg4 = { name = "infile4", input = "data", tip = "continued series of file inputs" },
  arg5 = { name = "outtextfile", output = "txt", tip = "textfile containing a grouped list of all the infiles" },
}

dsp["Housekeep Bundle - 3 Bundle all non-text files of same type as first non-text file, For sorting, backup or creating a dummy mixfile"] = {
  cmds = { exe = "housekeep", mode = "bundle 3", tip = "3 Bundle all non-text files of same type as first non-text file. You enter your all your filenames on the command line, and the program groups them according to the Mode instructions and writes an appropriate text file. Musical applications; This facility could be used to document a project or to create the first stage of a mixfile. SUBMIX DUMMY can then be used to take a list of sounds a step further by creating a dummy mixfile – 'dummy' because it is presumed that some further editing will be required. When entering soundfile etc. names, you don't need to include the extension if the environment variable CDP_SOUND_EXT has been set. HOUSEKEEP BUNDLE and SUBMIX DUMMY make a great time-saving combination when creating mixfiles. SUBMIX DUMMY no longer takes a text file as an input: the files are listed on the command line (the graphic user interfaces have their own ways of doing this). The text file created in HOUSEKEEP DUMMY can therefore only be used as a reference guide when entering the filenames." },
  arg1 = { name = "infile", input = "data", tip = "any type of file with any extension, e.g., asound.wav, atext.txt, amix.mix" },
  arg2 = { name = "infile2", input = "data", tip = "continued series of file inputs" },
  arg3 = { name = "infile3", input = "data", tip = "continued series of file inputs" },
  arg4 = { name = "infile4", input = "data", tip = "continued series of file inputs" },
  arg5 = { name = "outtextfile", output = "txt", tip = "textfile containing a grouped list of all the infiles" },
}

dsp["Housekeep Bundle - 4 As 3 but only files with the same properties, For sorting, backup or creating a dummy mixfile"] = {
  cmds = { exe = "housekeep", mode = "bundle 4", tip = "4 As 3 but only files with the same properties. You enter your all your filenames on the command line, and the program groups them according to the Mode instructions and writes an appropriate text file. Musical applications; This facility could be used to document a project or to create the first stage of a mixfile. SUBMIX DUMMY can then be used to take a list of sounds a step further by creating a dummy mixfile – 'dummy' because it is presumed that some further editing will be required. When entering soundfile etc. names, you don't need to include the extension if the environment variable CDP_SOUND_EXT has been set. HOUSEKEEP BUNDLE and SUBMIX DUMMY make a great time-saving combination when creating mixfiles. SUBMIX DUMMY no longer takes a text file as an input: the files are listed on the command line (the graphic user interfaces have their own ways of doing this). The text file created in HOUSEKEEP DUMMY can therefore only be used as a reference guide when entering the filenames." },
  arg1 = { name = "infile", input = "data", tip = "any type of file with any extension, e.g., asound.wav, atext.txt, amix.mix" },
  arg2 = { name = "infile2", input = "data", tip = "continued series of file inputs" },
  arg3 = { name = "infile3", input = "data", tip = "continued series of file inputs" },
  arg4 = { name = "infile4", input = "data", tip = "continued series of file inputs" },
  arg5 = { name = "outtextfile", output = "txt", tip = "textfile containing a grouped list of all the infiles" },
}

dsp["Housekeep Bundle - 5 As 4 but if file1 is a soundfile, files with the same channel count only, For sorting, backup or creating a dummy mixfile"] = {
  cmds = { exe = "housekeep", mode = "bundle 5", tip = "5 As 4 but if file1 is a soundfile, files with the same channel count only. You enter your all your filenames on the command line, and the program groups them according to the Mode instructions and writes an appropriate text file. Musical applications; This facility could be used to document a project or to create the first stage of a mixfile. SUBMIX DUMMY can then be used to take a list of sounds a step further by creating a dummy mixfile – 'dummy' because it is presumed that some further editing will be required. When entering soundfile etc. names, you don't need to include the extension if the environment variable CDP_SOUND_EXT has been set. HOUSEKEEP BUNDLE and SUBMIX DUMMY make a great time-saving combination when creating mixfiles. SUBMIX DUMMY no longer takes a text file as an input: the files are listed on the command line (the graphic user interfaces have their own ways of doing this). The text file created in HOUSEKEEP DUMMY can therefore only be used as a reference guide when entering the filenames." },
  arg1 = { name = "infile", input = "data", tip = "any type of file with any extension, e.g., asound.wav, atext.txt, amix.mix" },
  arg2 = { name = "infile2", input = "data", tip = "continued series of file inputs" },
  arg3 = { name = "infile3", input = "data", tip = "continued series of file inputs" },
  arg4 = { name = "infile4", input = "data", tip = "continued series of file inputs" },
  arg5 = { name = "outtextfile", output = "txt", tip = "textfile containing a grouped list of all the infiles" },
}

--[[ doesn't work? No output?
dsp["Housekeep Chans 1 - Extract a channel - change channel format of a soundfile"] = {
  cmds = { exe = "housekeep", mode = "chans 1", channels = "any", tip = "Mode 1 will take out, for example, all the ch2's and stream them together into a single channel (mono) soundfile – whichever channel is specified by channo.. Soundfiles may have one channel (mono), two channels (stereo), four channels (quad) and possible 6 or 8 channels. Each channel is a different stream of data. The streams may duplicate each other or have different contents. The data for the various channels is interleaved in a multichannel soundfile; for example, the samples of a quad file will be ordered: ch1-ch2-ch3-ch4 ch1-ch2-ch3-ch4 etc. HOUSEKEEP CHANS can take apart this data, or merge a stereo file to form a mono file. You can also piece together single channels of data to make real multi-channel files using SUBMIX INTERLEAVE.  Musical applications; Besides the obvious 'housekeeping' chores of Modes 4 & 5, the other Modes make it possible to dissect and rearrange channel data from different soundfiles. Rejoining separated channels is done with SUBMIX INTERLEAVE. The primary application involves processing the extracted channels separately. The logic of using HOUSEKEEP CHANS and SUBMIX INTERLEAVE rather than a mixing routine would seem to be that the intended output is conceived of as a single, more complex sound. For example, it is possible to create pulsations of different frequencies with DISTORT ENVEL. An effective way to do this is to split a stereo file into separate channels (HOUSEKEEP CHANS Mode 2), process each with different rates of pulsation (DISTORT ENVEL), and then reunite them as a single stereo file with phased pulsations (SUBMIX INTERLEAVE)." },
  arg1 = { name = "infile", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "channo", min = 1, max = 2, def = 1, tip = "channel to extract" },
}

--]]

--[[ doesn't work - multiple outputs not supported.
dsp["Housekeep Chans 2 - Extract all channels - change channel format of a soundfile"] = {
  cmds = { exe = "housekeep", mode = "chans 2", channels = "any", tip = "Mode 2 will take apart all the channels present and create as many separate single channel (mono) soundfiles as there are channels.. Soundfiles may have one channel (mono), two channels (stereo), four channels (quad) and possible 6 or 8 channels. Each channel is a different stream of data. The streams may duplicate each other or have different contents. The data for the various channels is interleaved in a multichannel soundfile; for example, the samples of a quad file will be ordered: ch1-ch2-ch3-ch4 ch1-ch2-ch3-ch4 etc. HOUSEKEEP CHANS can take apart this data, or merge a stereo file to form a mono file. You can also piece together single channels of data to make real multi-channel files using SUBMIX INTERLEAVE.  Musical applications; Besides the obvious 'housekeeping' chores of Modes 4 & 5, the other Modes make it possible to dissect and rearrange channel data from different soundfiles. Rejoining separated channels is done with SUBMIX INTERLEAVE. The primary application involves processing the extracted channels separately. The logic of using HOUSEKEEP CHANS and SUBMIX INTERLEAVE rather than a mixing routine would seem to be that the intended output is conceived of as a single, more complex sound. For example, it is possible to create pulsations of different frequencies with DISTORT ENVEL. An effective way to do this is to split a stereo file into separate channels (HOUSEKEEP CHANS Mode 2), process each with different rates of pulsation (DISTORT ENVEL), and then reunite them as a single stereo file with phased pulsations (SUBMIX INTERLEAVE)." },
  arg1 = { name = "infile", input = "wav", tip = "Select the input sound to the process" },
}
--]]

dsp["Housekeep Chans 3 - Zero one channel - change channel format of a soundfile"] = {
  cmds = { exe = "housekeep", mode = "chans 3", channels = "any", tip = "Mode 3 will create a stereo soundfile with one empty channel. With a mono input, channo will be empty in the stereo output and the other channel will contain the original mono input. With a stereo input, the channo side will be zeroed. Soundfiles may have one channel (mono), two channels (stereo), four channels (quad) and possible 6 or 8 channels. Each channel is a different stream of data. The streams may duplicate each other or have different contents. The data for the various channels is interleaved in a multichannel soundfile; for example, the samples of a quad file will be ordered: ch1-ch2-ch3-ch4 ch1-ch2-ch3-ch4 etc. HOUSEKEEP CHANS can take apart this data, or merge a stereo file to form a mono file. You can also piece together single channels of data to make real multi-channel files using SUBMIX INTERLEAVE.  Musical applications; Besides the obvious 'housekeeping' chores of Modes 4 & 5, the other Modes make it possible to dissect and rearrange channel data from different soundfiles. Rejoining separated channels is done with SUBMIX INTERLEAVE. The primary application involves processing the extracted channels separately. The logic of using HOUSEKEEP CHANS and SUBMIX INTERLEAVE rather than a mixing routine would seem to be that the intended output is conceived of as a single, more complex sound. For example, it is possible to create pulsations of different frequencies with DISTORT ENVEL. An effective way to do this is to split a stereo file into separate channels (HOUSEKEEP CHANS Mode 2), process each with different rates of pulsation (DISTORT ENVEL), and then reunite them as a single stereo file with phased pulsations (SUBMIX INTERLEAVE)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "channo", min = 1, max = 2, def = 1, tip = "channel to zero" },
}
 
dsp["Housekeep Chans 4 - Stereo to mono - change channel format of a soundfile"] = {
  cmds = { exe = "housekeep", mode = "chans 4", channels = "any", tip = "Mode 4 will change a stereo file into a mono file – often invoked when it is found that a certain function will only accept mono inputs. Soundfiles may have one channel (mono), two channels (stereo), four channels (quad) and possible 6 or 8 channels. Each channel is a different stream of data. The streams may duplicate each other or have different contents. The data for the various channels is interleaved in a multichannel soundfile; for example, the samples of a quad file will be ordered: ch1-ch2-ch3-ch4 ch1-ch2-ch3-ch4 etc. HOUSEKEEP CHANS can take apart this data, or merge a stereo file to form a mono file. You can also piece together single channels of data to make real multi-channel files using SUBMIX INTERLEAVE.  Musical applications; Besides the obvious 'housekeeping' chores of Modes 4 & 5, the other Modes make it possible to dissect and rearrange channel data from different soundfiles. Rejoining separated channels is done with SUBMIX INTERLEAVE. The primary application involves processing the extracted channels separately. The logic of using HOUSEKEEP CHANS and SUBMIX INTERLEAVE rather than a mixing routine would seem to be that the intended output is conceived of as a single, more complex sound. For example, it is possible to create pulsations of different frequencies with DISTORT ENVEL. An effective way to do this is to split a stereo file into separate channels (HOUSEKEEP CHANS Mode 2), process each with different rates of pulsation (DISTORT ENVEL), and then reunite them as a single stereo file with phased pulsations (SUBMIX INTERLEAVE)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "-p", switch = "-p", tip = "The -p option inverts the phase of the 2nd channel before mixing" },
}
 
dsp["Housekeep Chans 5 - Mono to stereo - change channel format of a soundfile"] = {
  cmds = { exe = "housekeep", mode = "chans 5", channels = "any", tip = "Creates a 2-channel equivalent of a mono infile. Soundfiles may have one channel (mono), two channels (stereo), four channels (quad) and possible 6 or 8 channels. Each channel is a different stream of data. The streams may duplicate each other or have different contents. The data for the various channels is interleaved in a multichannel soundfile; for example, the samples of a quad file will be ordered: ch1-ch2-ch3-ch4 ch1-ch2-ch3-ch4 etc. HOUSEKEEP CHANS can take apart this data, or merge a stereo file to form a mono file. You can also piece together single channels of data to make real multi-channel files using SUBMIX INTERLEAVE.  Musical applications; Besides the obvious 'housekeeping' chores of Modes 4 & 5, the other Modes make it possible to dissect and rearrange channel data from different soundfiles. Rejoining separated channels is done with SUBMIX INTERLEAVE. The primary application involves processing the extracted channels separately. The logic of using HOUSEKEEP CHANS and SUBMIX INTERLEAVE rather than a mixing routine would seem to be that the intended output is conceived of as a single, more complex sound. For example, it is possible to create pulsations of different frequencies with DISTORT ENVEL. An effective way to do this is to split a stereo file into separate channels (HOUSEKEEP CHANS Mode 2), process each with different rates of pulsation (DISTORT ENVEL), and then reunite them as a single stereo file with phased pulsations (SUBMIX INTERLEAVE)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Housekeep Deglitch - Attempt to deglitch a soundfile."] = {
  cmds = { exe = "housekeep", mode = "deglitch", channels = "any", tip = "A glitch is heard as a brief click in the soundfile. This is caused by a very rapid and large change in amplitude, usually from one sample to the next. If they also overmodulate, there is also a nasty noise component. These glitches appear in the Time Domain time amplitude sample display of a soundfile as vertical lines, and can comprise only a very small number of samples. HOUSEKEEP DEGLITCH provides a way to search for these anomalies and remove them. The length of a glitch might be able to be determined aurally, with PLAY FROM - TO used to pin it down further, though it may be too short for this to work. In a soundfile display, you could block one out and see its time in seconds (convert this to milliseconds by multiplying by 1000). Alternatively, the display might show sample positions. Then you would need to subtract the start from the end sample number and divide the result by the sample rate to get the time in seconds, and then multiply by 1000 to get the time in milliseconds. E.g., 10573 - 9455 = 1118 samples. 1118 ÷ 44100 (the sample rate) = 0.02535 seconds * 1000 = 25.35 milliseconds, rounding to 25. Note that extensive overmodulation after processing a sound is not a glitch problem. Rather the amplitude level of the input to sound was too high for that particular process. The solution in this case is to reduce the level of the input sound before processing (MODIFY LOUDNESS, Mode 1, Gain). HOUSEKEEP DEGLITCH works by identifying and removing the glitch, closing up the gap. The splice parameter smooths the join – but it doesn't want to be too long either, lest it cause an audible dip in amplitude. Musical applications; Certain processes involving extensive editing of small fragments of sound may lead to sharp amplitude changes audible as clicks. It is extremely useful to have a tool to search for and eliminate them amongst the many thousands of samples per second in a soundfile. Another situation where it might be useful is after a process that leaves only a handful of glitches, perhaps as overmodulations, but it is important to keep the overall amplitude level as high as possible." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "glitch", min = 0, max = length, def = 5, tip = "maximum duration in milliseconds of an glitch to find" },
  arg4 = { name = "sil", min = 0, max = length, def = 10, tip = "minimum duration in milliseconds of 'silence' on either side of the glitch" },
  arg5 = { name = "thresh", min = 0, max = 1, def = 0.8, tip = "maximum level of 'silence' on either side of the glitch" },
  arg6 = { name = "splice", min = 0, max = length, def = 5, tip = "splice length in milliseconds to cut out the glitch. It must be at least half the length of sil." },
  arg7 = { name = "window", min = 0, max = length, def = 2, tip = "window length in milliseconds in which to search for glitches and 'silence'." },
}
 
dsp["Housekeep Endclicks - Remove clicks from the start or end of a soundfile "] = {
  cmds = { exe = "housekeep", mode = "endclicks", channels = "any", tip = "Some CDP processes can produce output where the signal stops (or starts) abruptly (rather than falling, or rising, gradually from zero). This can happen with the output of transformed and resynthesized PVOC sounds, or at the end of mixes where the endtime of the mix process is not set at the end duration of the mixfile. Such sharp cutoffs (or cut-ins) produce clicks, which can be removed using this process. Note that if a sharp cutoff occurs before the very end of the outfile data, a simple TOPNTAIL may not shave off the endclick, whereas this process searches for (start or) endclicks within the file in order to shave them off. Dale Perkins has noticed that this program can fail if there are no edge clicks in the input soundfile. In Sound Loom, a red error message is displayed. In bulk process, however, Trevor Wishart explains, the bulk processor can lose the pipe-markers of the many processes it is running and display a message such as: cannot find channel named 'file 103'. These messages can be safely ignored – you just have to click on the 'OK' button at the bottom of the TK/Tcl error-message box(es) that come up. The bulk processor just carries on processing the files that do not 'fail'. Musical applications; This function provides a quick way to deal with clicks or even just overly sharp edges, without going as far as the, usually longer, slopes of a DOVETAIL. The gate parameter provides a way to retain a touch of 'edge' by raising the level, while a longer splicelen will add some extra smoothness to the attack and decay portions of the sound. The -b and -e parameter provide the option to trim only the start or the end. Both would be handled by default, i.e., by not using either." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gate", min = 0, max = 1, def = 0.5, tip = "level above which the signal is retained. (Range: 0 to 1)" },
  arg4 = { name = "splicelen", min = 0, max = length, def = 5, tip = "length of splice taper in milliseconds" },
  arg5 = { name = "-b", switch = "-b", tip = "trim start of sound." },
  arg6 = { name = "-e", switch = "-e", tip = "trim end of sound." },
}
 
dsp["Housekeep Extract - 4 Eliminate DC-offset"] = {
  cmds = { exe = "housekeep", mode = "extract 4", channels = "any", tip = "Remove: shift (offset) the entire signal to eliminate unwanted DC component (in reference to a 0 to 1 amplitude range)" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "shift", min = 0, max = 1, def = 0, tip = " amplitude value applied to the whole file (subtracted from amplitudes): in order to remove any DC (direct current) component which may be present." },
}
 
dsp["Housekeep Extract - 6 Find onset times: find the places where significant signal begins and write to a text file"] = {
  cmds = { exe = "housekeep", mode = "extract 6", tip = "6 Find onset times: find the places where significant signal begins and write to a text file" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "gate", min = 0, max = 1, def = 0, tip = "amplitude level above which signal is to be accepted. Range: 0 to 1 (Default: 0)" },
  arg4 = { name = "endcutoff", min = 0, max = 1, def = 0, tip = "end cut-off level below which the end of the sound is cut. If zero, defaults to gate." },
  arg5 = { name = "threshold", min = 0, max = 1, def = 0, tip = "amplitude level to be reached within the retained segments. If the level never exceeds threshold, the segment is not kept. (Default: 0)" },
  arg6 = { name = "baktrak", min = 0, max = 64, def = 0, tip = "go backwards in the soundfile and keep baktrak sectors prior to the current gate-on, but only if the level there is above the Initial level. (Range: 0 to 64 sectors) " },
  arg7 = { name = "Initlevel", min = 0, max = 1, def = 0, tip = "amplitude level at start of backtracked segment. Range: 0 to 1; Default = 0. Use with -bbaktrak" },
  arg8 = { name = "minlength", min = 0, max = length/1000, def = 0, tip = "minimum length in seconds of segment to keep (Range: 0 to length of input soundfile – but I have seen it fail at 1.0 (with an 8 second soundfile)" },
  arg9 = { name = "gate_window", min = 0, max = 64, tip = "length of the gate window in sectors (see Mode 2). Range: 0 to 64." },
}
 
dsp["Housekeep Extract - 3 top & tail"] = {
  cmds = { exe = "housekeep", mode = "extract 3", channels = "any", tip = "Top and tail: remove low level signal from start and end of sound. This creates outfile, a version of the infile without unwanted silence or low level signal at the beginning and/or end. Also see TOPANTAIL2 for gated extraction of a sound with 'top and tailing' and a backtracking facility." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "amplitude level above which signal is to be accepted. Range: 0 to 1 (Default: 0)" },
  arg4 = { name = "splice", switch = "-s", min = 0, max = length, def = 15, tip = "length of splice slope in milliseconds (Default: 15ms)" },
  arg5 = { name = "-b", switch = "-b", tip = "don't trim the beginning of infile" },
  arg6 = { name = "-e", switch = "-e", tip = "don't trim the end of infile" },
}
 
dsp["Housekeep Gate - Cut file at zero amplitude points"] = {
  cmds = { exe = "housekeep", mode = "gate 1", channels = "any", tip = "OUSEKEEP GATE finds the points of zero amplitude in a sound, and cuts it at those points. It is useful for extracting distinct features from a sound (or separate sounds from a sequence of recordings) provided the segments have true silence (zero amplitude) in between them. Musical applications; HOUSEKEEP GATE helps to isolate distinct areas of a sound, or distinct sounds in a sequence of recordings, or to get rid of unwanted silence." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "zerocount", switch = "-z", min = 0, max = 44100, tip = "the number of consecutive zero samples (per channel) to indicate a silent gap in the sound, where it can be cut" },
}

dsp["Housekeep Sort - 6 Find rogues (check for invalid files and report potential anomalies)"] = {
  cmds = { exe = "housekeep", mode = "sort 6", tip = "6 Find rogues (check for invalid files and report potential anomalies). Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
}
 
--[[ doesn't work? 
dsp["Housekeep Remove - Remove existing copies of a soundfile)"] = {
  cmds = { exe = "housekeep", mode = "remove", channels = "any", tip = "A series of soundfiles whose names end with '_001', '_002' etc. can be created with HOUSEKEEP COPY and HOUSEKEEP EXTRACT. HOUSEKEEP REMOVE is a handy way to remove these files after you have decided (and renamed) the ones you want to keep. Musical applications; This is just a housekeeping utility." },
  arg1 = { name = "filename", input = "wav", tip = "Deletes any copies of filename, having names filename_001, filename_002.. No checks are made that these are actually copies of filename!" },
  arg2 = { name = "-a", switch = "-a", tip = "The program checks all names in numbered sequence." },
}
--]]

dsp["Housekeep Respec 1 - resample at some different sample rate (rewrites sample data)"] = {
  cmds = { exe = "housekeep", mode = "respec 1", channels = "any", tip = "The key issue here is the difference between 'resampling' (Mode 1) and simply changing the information in the header of the soundfile (Mode 2). If the sample rate is 22050, resampling with a new_samplerate of 44100 will rewrite the file, doubling up every sample so that there will then be 44100 samples per second in the file. (The information in the header is also changed accordingly). This is what is meant by the phrase 'rewrites the sample data'. This is what happens in (Mode 1), and this is why the length of the file in bytes will be longer or shorter, but the duration and pitch of the sound will remain unchanged. When the PLAY program checks the header and finds e.g., 44100, there are indeed 44100 samples to play, etc. The resampling operation is relatively simple in nature, which is why it works only with the named sample rates. A fully flexible resampling algorithm is much more complex, requires extensive filtering procedures and has not been implemented in CDP. Resampling should be used sparingly for this reason, as some loss of quality is likely, especially in downsampling, as no anti-aliasing is applied to the sound file. In Mode 3 the information in the header is changed but the actual sample data is left untouched. Thus, when the PLAY program checks the header and finds e.g., 44100 when the file actually contains only 22050 samples, it will play it at a 44100 rate – which gets through the 22050 samples at double speed, takes half as long and raises the pitch by an octave (twice as fast). Mode 2 converts between 16-bit and floating-point soundfile data. At the moment, all soundfiles are normally 16-bit. The CDP System automatically converts to floating point, does the processing and then at the output stage the data is automatically converted from float back to short, thus losing the extra precision (= audio quality) which the floating point provides. It has to do this as there are at present no soundcards which can handle floating point soundfiles. If there is a floating point soundfile that you want to use, Mode 2 will convert it to shorts so you can use it with the CDP System. However, we are gradually extending the capacity of the System to handle floating point files, both as inputs, and to output a floating point result. HOUSEKEEP RESPEC will itself be updated in this way. Mode 2 will come into its own in the future as soundcards appear which can play floating point soundfiles, but will also be less needed as more CDP functions become capable of creating floating point soundfiles. Floating point output is also useful for DSP testing. Musical applications; Sometimes when mixing, one finds that the sample rates of the component soundfiles are not the same (they all must be the same for a mix). HOUSEKEEP RESPEC in Mode 1 provides a reasonably effective way to get the sample rates to conform. However, it shouldn't be regarded as a standard tool. You should really work at a consistent sample rate. However, if at some inconvenient moment you find you have a stray soundfile at an incompatible rate for mixing, you can quickly alter it – with some loss of quality. Some versions of COOLEDIT PRO will incorrectly write a floating point header on a 16-bit soundfile. The sampletype can be changed to SHORT with HOUSEKEEP RESPEC Mode 3." },
  arg1 = { name = "infile", input = "wav", tip = "soundfile to adjust" },
  arg2 = { name = "outfile", output = "wav", tip = "new soundfile with alterations made" },
  arg3 = { name = "new_samplerate", input = "string", def = "48000, 24000, 44100, 22050, 32000 or 16000", tip = "New sample rate must be one of: 48000, 24000, 44100, 22050, 32000 or 16000" },
}
 
dsp["Housekeep Respec 2 - convert from integer to float samples, or vice versa"] = {
  cmds = { exe = "housekeep", mode = "respec 2", channels = "any", tip = "The key issue here is the difference between 'resampling' (Mode 1) and simply changing the information in the header of the soundfile (Mode 2). If the sample rate is 22050, resampling with a new_samplerate of 44100 will rewrite the file, doubling up every sample so that there will then be 44100 samples per second in the file. (The information in the header is also changed accordingly). This is what is meant by the phrase 'rewrites the sample data'. This is what happens in (Mode 1), and this is why the length of the file in bytes will be longer or shorter, but the duration and pitch of the sound will remain unchanged. When the PLAY program checks the header and finds e.g., 44100, there are indeed 44100 samples to play, etc. The resampling operation is relatively simple in nature, which is why it works only with the named sample rates. A fully flexible resampling algorithm is much more complex, requires extensive filtering procedures and has not been implemented in CDP. Resampling should be used sparingly for this reason, as some loss of quality is likely, especially in downsampling, as no anti-aliasing is applied to the sound file. In Mode 3 the information in the header is changed but the actual sample data is left untouched. Thus, when the PLAY program checks the header and finds e.g., 44100 when the file actually contains only 22050 samples, it will play it at a 44100 rate – which gets through the 22050 samples at double speed, takes half as long and raises the pitch by an octave (twice as fast). Mode 2 converts between 16-bit and floating-point soundfile data. At the moment, all soundfiles are normally 16-bit. The CDP System automatically converts to floating point, does the processing and then at the output stage the data is automatically converted from float back to short, thus losing the extra precision (= audio quality) which the floating point provides. It has to do this as there are at present no soundcards which can handle floating point soundfiles. If there is a floating point soundfile that you want to use, Mode 2 will convert it to shorts so you can use it with the CDP System. However, we are gradually extending the capacity of the System to handle floating point files, both as inputs, and to output a floating point result. HOUSEKEEP RESPEC will itself be updated in this way. Mode 2 will come into its own in the future as soundcards appear which can play floating point soundfiles, but will also be less needed as more CDP functions become capable of creating floating point soundfiles. Floating point output is also useful for DSP testing. Musical applications; Sometimes when mixing, one finds that the sample rates of the component soundfiles are not the same (they all must be the same for a mix). HOUSEKEEP RESPEC in Mode 1 provides a reasonably effective way to get the sample rates to conform. However, it shouldn't be regarded as a standard tool. You should really work at a consistent sample rate. However, if at some inconvenient moment you find you have a stray soundfile at an incompatible rate for mixing, you can quickly alter it – with some loss of quality. Some versions of COOLEDIT PRO will incorrectly write a floating point header on a 16-bit soundfile. The sampletype can be changed to SHORT with HOUSEKEEP RESPEC Mode 3." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Housekeep Respec 3 - change properties of sound (affects header only – use with caution!) "] = {
  cmds = { exe = "housekeep", mode = "respec 3", channels = "any", tip = "The key issue here is the difference between 'resampling' (Mode 1) and simply changing the information in the header of the soundfile (Mode 2). If the sample rate is 22050, resampling with a new_samplerate of 44100 will rewrite the file, doubling up every sample so that there will then be 44100 samples per second in the file. (The information in the header is also changed accordingly). This is what is meant by the phrase 'rewrites the sample data'. This is what happens in (Mode 1), and this is why the length of the file in bytes will be longer or shorter, but the duration and pitch of the sound will remain unchanged. When the PLAY program checks the header and finds e.g., 44100, there are indeed 44100 samples to play, etc. The resampling operation is relatively simple in nature, which is why it works only with the named sample rates. A fully flexible resampling algorithm is much more complex, requires extensive filtering procedures and has not been implemented in CDP. Resampling should be used sparingly for this reason, as some loss of quality is likely, especially in downsampling, as no anti-aliasing is applied to the sound file. In Mode 3 the information in the header is changed but the actual sample data is left untouched. Thus, when the PLAY program checks the header and finds e.g., 44100 when the file actually contains only 22050 samples, it will play it at a 44100 rate – which gets through the 22050 samples at double speed, takes half as long and raises the pitch by an octave (twice as fast). Mode 2 converts between 16-bit and floating-point soundfile data. At the moment, all soundfiles are normally 16-bit. The CDP System automatically converts to floating point, does the processing and then at the output stage the data is automatically converted from float back to short, thus losing the extra precision (= audio quality) which the floating point provides. It has to do this as there are at present no soundcards which can handle floating point soundfiles. If there is a floating point soundfile that you want to use, Mode 2 will convert it to shorts so you can use it with the CDP System. However, we are gradually extending the capacity of the System to handle floating point files, both as inputs, and to output a floating point result. HOUSEKEEP RESPEC will itself be updated in this way. Mode 2 will come into its own in the future as soundcards appear which can play floating point soundfiles, but will also be less needed as more CDP functions become capable of creating floating point soundfiles. Floating point output is also useful for DSP testing. Musical applications; Sometimes when mixing, one finds that the sample rates of the component soundfiles are not the same (they all must be the same for a mix). HOUSEKEEP RESPEC in Mode 1 provides a reasonably effective way to get the sample rates to conform. However, it shouldn't be regarded as a standard tool. You should really work at a consistent sample rate. However, if at some inconvenient moment you find you have a stray soundfile at an incompatible rate for mixing, you can quickly alter it – with some loss of quality. Some versions of COOLEDIT PRO will incorrectly write a floating point header on a 16-bit soundfile. The sampletype can be changed to SHORT with HOUSEKEEP RESPEC Mode 3." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "srate", switch = "-s", min = 16000, max = 48000, tip = "new sample rate to impose; New sample rate must be one of: 48000, 24000, 44100, 22050, 32000 or 16000" },
  arg4 = { name = "channels", switch = "-c", min = 1, max = 2, tip = "the new channel count to impose" },
  arg5 = { name = "samptype", switch = "-t", min = 0, max = 1, def = 1, tip = "0 = integers, 1 = floats,Warning: use this only to restore an incorrect header!" },
}

dsp["Housekeep Sort - 1 Sort by filetype. Sort files listed in a textfile"] = {
  cmds = { exe = "housekeep", mode = "sort 1", tip = "1 Sort by filetype, listing in textfile(s) with 3-letter file extension(s) to identify the groups of files (see table below). Mode 1 reads a textfile containing a list of files (with their extensions) and groups them into one or more text files containing the names of all the files of a given type. These output textfiles are given extensions by which you can easily identify their contents. Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
}

dsp["Housekeep Sort - 2 Sort by sample rate, listing in textfile(s) with extension(s) indicating SR. Sort files listed in a textfile"] = {
  cmds = { exe = "housekeep", mode = "sort 2", tip = "2 Sort by sample rate, listing in textfile(s) with extension(s) indicating SR. In Mode 2 you get a list of all the soundfiles in your current directory grouped by sample rate. The sample rate of the sounds in a given textfile generated by SORT Mode 2 is indicated by a numerical extension: .24 for 24000, .44 for 44100 etc. Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
  arg2 = { name = "small", min = 0, max = 100, def = 1, tip = "maximum size of the smallest soundfiles, in seconds" },
  arg3 = { name = "large", min = 0, max = 100, def = 10, tip = "minimum size of the largest soundfiles, in seconds" },
  arg4 = { name = "step", min = 0, max = 100, def = 0.1, tip = "groups the files according to duration increment" },
  arg5 = { name = "-l", switch = "-l", tip = "causes file durations not to be written to the output text file" },
}

dsp["Housekeep Sort - 3 Sort by duration, listing in textfile named 'your_listfile.len' (in step duration groups if step was specified)"] = {
  cmds = { exe = "housekeep", mode = "sort 3", tip = "3 Sort by duration, listing in textfile named 'your_listfile.len' (in step duration groups if step was specified). Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
  arg2 = { name = "small", min = 0, max = 100, def = 1, tip = "maximum size of the smallest soundfiles, in seconds" },
  arg3 = { name = "large", min = 0, max = 100, def = 10, tip = "minimum size of the largest soundfiles, in seconds" },
  arg4 = { name = "step", min = 0, max = 100, def = 0.1, tip = "groups the files according to duration increment" },
  arg5 = { name = "-l", switch = "-l", tip = "causes file durations not to be written to the output text file" },
}

dsp["Housekeep Sort - 4 Sort by log duration, relating file lengths by a duration ratio, and listing in textfile named 'your_listfile.len'"] = {
  cmds = { exe = "housekeep", mode = "sort 4", tip = "4 Sort by log duration, relating file lengths by a duration ratio, and listing in textfile named 'your_listfile.len'. Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
  arg2 = { name = "-l", switch = "-l", tip = "causes file durations not to be written to the output text file" },
}

dsp["Housekeep Sort - 5 Sort into duration order, listing in 'your_listfile.len'"] = {
  cmds = { exe = "housekeep", mode = "sort 5", tip = "5 Sort into duration order, listing in 'your_listfile.len'. Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
}

dsp["Housekeep Sort - 6 Find rogues (check for invalid files and report potential anomalies)"] = {
  cmds = { exe = "housekeep", mode = "sort 6", tip = "6 Find rogues (check for invalid files and report potential anomalies). Musical applications; As a directory for a given project starts to fill up, confusion about what is what may begin to set in, especially if the naming conventions vary for one reason or another. HOUSEKEEP SORT Mode 1 looks at the headers of all the files and automatically groups them correctly, writing a series of files for future reference. This is also a great way to document a project. The other modes tend to be useful when preparing mixes. You may need a quick check on whether all the files you intend to mix have the same sample rate (required by the mixing operation), or you might want a listing of all the soundfiles with durations within a certain range, or you might want to get an ordered list of all your soundfiles from the shortest to the longest. HOUSEKEEP SORT handles all these various operations. The resulting files can, if appropriate, be used as source textfiles for your actual mixfile, with SUBMIX DUMMY automatically turning them into a prototype mixfile. All of these operations are simple and quick, and the larger your mixes, the more useful these facilities become. SUBMIX MIX can handle more than 50 soundfiles at once, so it may be worth exploring the potential speed and power of a non-graphic approach." },
  arg1 = { name = "listfile", input = "txt", tip = " a text file containing a list of the files to be sorted. The normal extension of the file must be included: e.g., .wav or .aif for a soundfile." },
}


--[[ 
dsp["[No use, yet?] Housekeep Copy 2 - Make multiple copies of a sound"] = {
  cmds = { exe = "housekeep", mode = "copy 2", tip = "Copy many: produce count copies of infile" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "count", min = 1, max = 16, def = 2, tip = "number of copies to make." },
  arg4 = { name = "-i", switch = "-i", tip = "option to ignore existing duplicates: i.e., don't overwrite them. The Default is to stop the copying process upon finding a pre-existing soundfile, i.e., one already named like the copies being made." },
}
--]]

--[[ 
dsp["[No use] Housekeep Copy 1 - Make copies of a sound"] = {
  cmds = { exe = "housekeep", mode = "copy 1", tip = "1 Copy once: make one copy of infile named outfile" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
--]]

dsp["Housekeep Disk - Show available space on disk "] = { 
  cmds = { exe = "housekeep", mode = "disk", tip = "The purpose of HOUSEKEEP DISK is to show you how much free space you have available on your hard disk. The display show available space in bytes, in samples (2 bytes per sample), and available time in hours, minutes and seconds for Mono and Stereo. If a soundfile is used as the anyinfile, the Mono and Stereo will relate to the sample rate of that soundfile (its header is interrogated). If your input is a mixfile, it will list time available for all valid sample rates, and will also tell you if a soundfile listed in the mixfile is no longer present on your disk (its name could be mis-spelled or it could be deleted.) Musical applications; HOUSEKEEP DISK is very handy when about to make a very long recording, or when you know that you are starting to get low on disk space. 'Disk' in this context means a partition of a hard disk (such as the c: or d: 'drive') if there is more than one on the physical hard disk unit. Hard disk software normally protects against problems occuring when the disk reaches full capacity. Computer systems have been known in the past to 'wrap' round to the beginning of the disk and overwrite the FAT table (File Allocation Table: system index of all the files on the disk!). To be on the safe side, it's best to stop writing to the disk when it's nearly full. Stop and do some housework: clean out redundant files to make more space on the disk. Disks are happiest when they have some headroom in which to store their little bits and pieces of system information. Recently, my Internet Explorer refused to load. It would start loading, and then I would get a system error message about an illegal operation. Richard Dobson acutely worked out that I didn't have enough space left on my disk for it to run, meaning places to which to copy temporary information." },
   arg1 = { name = "anyinfile", input = "data", tip = "any cdp compatible input file" },
}

--[[ doesn't work, audio input, multiple audio results with changed names
dsp["Housekeep Extract - 1 Cut out and keep significant events from infile, creating a series of outfiles."] = {
cmds = { exe = "housekeep", mode = "extract 1", tip = "1 Cut out and keep significant events from infile, creating a series of outfiles. With an infile named nn, the soundfiles produced from the segments of significant data will be nn_x001, nn_x002, nn_x003, etc." },
arg1 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "amplitude level above which signal is to be accepted. Range: 0 to 1 (Default: 0)" },
arg2 = { name = "splice", switch = "-s", min = 0, max = 1000, def = 15, tip = "length of splice slope in milliseconds (Default: 15ms)" },
arg3 = { name = "endcutoff", switch = "-e", min = 0, max = 2500, def = 0, tip = "end cut-off level below which the end of the sound is cut. If zero, defaults to gate." },
arg4 = { name = "threshold", switch = "-t", min = 0, max = 1, def = 0, tip = "amplitude level to be reached within the retained segments. If the level never exceeds threshold, the segment is not kept. (Default: 0)" },
arg5 = { name = "hold", switch = "-h", min = 0, max = 1000, def = 0, tip = "hold the sound for hold sectors before the start of the next segment. (Default: 0)" },
arg6 = { name = "baktrak", switch = "-b", min = 0, max = 64, def = 0, tip = "go backwards in the soundfile and keep baktrak sectors prior to the current gate-on, but only if the level there is above the Initial level. (Range: 0 to 64 sectors) " },
arg7 = { name = "Initial", switch = "-i", min = 0, max = 1, def = 0, tip = "amplitude level at start of backtracked segment. Range: 0 to 1; Default = 0. Use with -bbaktrak" },
arg8 = { name = "minlength", switch = "-l", min = 0, max = length/1000, def = 0, tip = "minimum length in seconds of segment to keep (Range: 0 to length of input soundfile – but I have seen it fail at 1.0 (with an 8 second soundfile)" },
arg9 = { name = "gate_window", switch = "-w", min = 0, max = 64,  tip = "length of the gate window in sectors (see Mode 2). Range: 0 to 64." },
arg10 = { name = "n", switch = "-n", tip = "the process stops if one of the soundfiles generated duplicates the name of an existing soundfile. The Default is to not generate that particular soundfile and continue processing." },
}
--]]

dsp["Housekeep Extract - 2 Extraction preview, showing envelope of infile as a pseudo-soundfile, sector by sector."] = { 
  cmds = { exe = "housekeep", mode = "extract 2", channels = "any", tip = "2 Extraction preview, showing envelope of infile as a pseudo-soundfile, sector by sector. It will often be useful to run this first and view the result in order to help choose parameter values for Mode 1" },
  arg1 = { name = "infile", input = "wav", tip = "original sound from which to extract significant data" },
  arg2 = { name = "outfileview", output = "txt", tip = "a pseudo-soundfile which shows amplitude levels for each sector (rather than for each sample). It provides an overview which shows at what levels significant data occurs. " },
}


------------------------------------------------
-- hover
------------------------------------------------

dsp["Hover - Move through a file, zig-zag reading it at a given frequency"] = {
  cmds = { exe = "hover", mode = "hover", url = "http://www.ensemble-software.net/CDPDocs/html/cgroextd.htm#HOVER", tip = "HOVER is a variant of ZIGZAG, but instead of jumping about in the file, it hovers around a given time-point (loc), reading forwards and backwards from this point at a given speed, which also determines the width of the reading. Note that the location point is time-variable, so the pointer can move through the file over time or indeed move to any time-point you wish. You can also randomly vary the frequency and the location point. Musical Applications: HOVER gives considerable scope for prolonging a sound, by reading the file in a controlled zig-zag fashion. It might be used for extending short-lived percussive sounds of an inharmonic timbre; producing a series of ebb-and-flow shapes (each like BAKTOBAK); or prolonging a highly textured sound which is difficult to loop. A number of different HOVERings of the same sound mixed together should also produce an interesting texture out of the one source." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "frq", min = 0, max = 44100, input = "brk", def = length/1000 + 1,  tip = "rate of reading source-samples (in Hz)." },
  arg4 = { name = "loc", min = 0, max = length/1000, input = "brk", def = 0.2, tip = "time in infile from which samples are read." },
  arg5 = { name = "frqrand", min = 0, max = 1, input = "brk", tip = "degree of random variation of frequency (range 0-1)" },
  arg6 = { name = "locrand", min = 0, max = 1, input = "brk", tip = "degree of random variation of location (range 0-1)." },
  arg7 = { name = "splice", min = 0, max = 100, def = 5, tip = "length of the splice (range: 0-100 milliseconds). splice length must be less than 1 over twice the maximum frq used, e.g. <5 ms for 100 Hz." },
  arg8 = { name = "dur", min = 0, max = length/1000 + 60, tip = "total output duration." },
}

------------------------------------------------
-- isolate
------------------------------------------------

--[[
dsp["[Doesn't work - multiple outputs] Isolate Disjunct portions of soundfile are specified by dB loudness and saved to separate files - multiple sound outputs"] = { 
  cmds = { exe = "isolate", mode = "isolate 3", tip = "ISOLATE is similar to PARTITION in that the primary operation is to cut a soundfile into disjunct pieces and assign these to different output soundfiles. It does this in a special way so that the time-position of these pieces in the original soundfile is retained: silence is inserted between the cut pieces in the outputs to achieve this ('silent surrounds'). The result is that the disjunct pieces can be reassembled in their original positions (remixed with everything synchronised at time zero): One difference between ISOLATE and PARTITION is that the number of output files is user-defined in PARTITION and worked out by the program, depending on the Mode selected, in ISOLATE. Another different is that Modes 1, 2 and 3 of ISOLATE generate a file containing all the materials left over after cutting. This is also used in reconstructing the original (now-treated) soundfile. However, the main difference between ISOLATE and PARTITION is how the disjunct pieces are identified. In PARTITION there are automatic processes for this, but the number of output files is user-defined by the outcnt parameter, and the durations are controlled by the groupcnt or dur parameters. In ISOLATE the user specifies start end times for the pieces in a textfile: the cutsfile (Modes 1-2) or specifies a list of (increasing) times in splicefile Modes 4-5, or they are picked up by level thresholds (dBon and dBoff (Mode 3). You are advised to familiarise yourself with the way the different Modes create different numbers of output soundfiles containing different numbers of segments." },
  arg1 = { name = "outnam", min = -60, max = 0, def = 0, tip = "generic root name for the output soundfiles. Numerals starting at 0 are appended to distinguish the outputs." },
  arg2 = { name = "dBon", min = -96.0, max = 0.0, tip = "the dB level at which a segment is recognised" },
  arg3 = { name = "dBoff", min = -96.0, max = 0.0, tip = "the dB level at which a recognised segment is triggered to end" },
  arg4 = { name = "splice", switch = "-s", min = 0, max = 500, def = 15, tip = " the length of the splice in milliseconds (Range: 0 to 500. Default: 15)" },
  arg5 = { name = "min", switch = "-m", min = 2, max = length, tip = "the minimum duration in milliseconds of segments to accept (Range: > 2 * splice)"},
}
--]]

--[[

dsp["[Doesn't work - multiple outputs] Isolate 1 - Create several output soundfiles each of which contains one segment of source (cutsfile)"] = { 
  cmds = { exe = "isolate", mode = "isolate 1", tip = "1 - Create several output soundfiles each of which contains one segment of source (cutsfile). ISOLATE is similar to PARTITION in that the primary operation is to cut a soundfile into disjunct pieces and assign these to different output soundfiles. It does this in a special way so that the time-position of these pieces in the original soundfile is retained: silence is inserted between the cut pieces in the outputs to achieve this ('silent surrounds'). The result is that the disjunct pieces can be reassembled in their original positions (remixed with everything synchronised at time zero): One difference between ISOLATE and PARTITION is that the number of output files is user-defined in PARTITION and worked out by the program, depending on the Mode selected, in ISOLATE. Another different is that Modes 1, 2 and 3 of ISOLATE generate a file containing all the materials left over after cutting. This is also used in reconstructing the original (now-treated) soundfile. However, the main difference between ISOLATE and PARTITION is how the disjunct pieces are identified. In PARTITION there are automatic processes for this, but the number of output files is user-defined by the outcnt parameter, and the durations are controlled by the groupcnt or dur parameters. In ISOLATE the user specifies start end times for the pieces in a textfile: the cutsfile (Modes 1-2) or specifies a list of (increasing) times in splicefile Modes 4-5, or they are picked up by level thresholds (dBon and dBoff (Mode 3). You are advised to familiarise yourself with the way the different Modes create different numbers of output soundfiles containing different numbers of segments." },
  arg1 = { name = "outname", input = "string", def = "generic", tip = "generic root name for the output soundfiles. Numerals starting at 0 are appended to distinguish the outputs." },
  arg2 = { name = "cutsfile", input = "txt", tip = "a textfile containing start end time-pairs in (increasing) time order at which to cut segments. " },
  arg3 = { name = "splice", switch = "-s", min = 0, max = 500, def = 15, tip = " the length of the splice in milliseconds (Range: 0 to 500. Default: 15)" },
  arg4 = { name = "x", switch = "-x", tip = "add silence to the end of the segments files so they become the same length as the source"},
  arg5 = { name = "r", switch = "-r", tip = "reverse all the cut-segment files (but not the remnant file)" },
}

--]]

--[[

dsp["[Doesn't work - multiple outputs] Isolate 2 - Create several output soundfiles each of which contains several segments of source (cutsfile)"] = { 
  cmds = { exe = "isolate", mode = "isolate 2", tip = "2 - Create several output soundfiles each of which contains several segments of source (cutsfile). ISOLATE is similar to PARTITION in that the primary operation is to cut a soundfile into disjunct pieces and assign these to different output soundfiles. It does this in a special way so that the time-position of these pieces in the original soundfile is retained: silence is inserted between the cut pieces in the outputs to achieve this ('silent surrounds'). The result is that the disjunct pieces can be reassembled in their original positions (remixed with everything synchronised at time zero): One difference between ISOLATE and PARTITION is that the number of output files is user-defined in PARTITION and worked out by the program, depending on the Mode selected, in ISOLATE. Another different is that Modes 1, 2 and 3 of ISOLATE generate a file containing all the materials left over after cutting. This is also used in reconstructing the original (now-treated) soundfile. However, the main difference between ISOLATE and PARTITION is how the disjunct pieces are identified. In PARTITION there are automatic processes for this, but the number of output files is user-defined by the outcnt parameter, and the durations are controlled by the groupcnt or dur parameters. In ISOLATE the user specifies start end times for the pieces in a textfile: the cutsfile (Modes 1-2) or specifies a list of (increasing) times in splicefile Modes 4-5, or they are picked up by level thresholds (dBon and dBoff (Mode 3). You are advised to familiarise yourself with the way the different Modes create different numbers of output soundfiles containing different numbers of segments." },
  arg1 = { name = "outname", input = "string", def = "generic", tip = "generic root name for the output soundfiles. Numerals starting at 0 are appended to distinguish the outputs." },
  arg2 = { name = "cutsfile", input = "txt", tip = "a textfile containing start end time-pairs in (increasing) time order at which to cut segments. " },
  arg3 = { name = "splice", switch = "-s", min = 0, max = 500, def = 15, tip = " the length of the splice in milliseconds (Range: 0 to 500. Default: 15)" },
  arg4 = { name = "x", switch = "-x", tip = "add silence to the end of the segments files so they become the same length as the source"},
  arg5 = { name = "r", switch = "-r", tip = "reverse all the cut-segment files (but not the remnant file)" },
}

--]]

--[[

dsp["[Doesn't work - multiple outputs] Isolate 4 - Cut the entire soundfile into disjunct segments (slicefile)"] = { 
  cmds = { exe = "isolate", mode = "isolate 4", tip = "4 - Cut the entire soundfile into disjunct segments (slicefile). ISOLATE is similar to PARTITION in that the primary operation is to cut a soundfile into disjunct pieces and assign these to different output soundfiles. It does this in a special way so that the time-position of these pieces in the original soundfile is retained: silence is inserted between the cut pieces in the outputs to achieve this ('silent surrounds'). The result is that the disjunct pieces can be reassembled in their original positions (remixed with everything synchronised at time zero): One difference between ISOLATE and PARTITION is that the number of output files is user-defined in PARTITION and worked out by the program, depending on the Mode selected, in ISOLATE. Another different is that Modes 1, 2 and 3 of ISOLATE generate a file containing all the materials left over after cutting. This is also used in reconstructing the original (now-treated) soundfile. However, the main difference between ISOLATE and PARTITION is how the disjunct pieces are identified. In PARTITION there are automatic processes for this, but the number of output files is user-defined by the outcnt parameter, and the durations are controlled by the groupcnt or dur parameters. In ISOLATE the user specifies start end times for the pieces in a textfile: the cutsfile (Modes 1-2) or specifies a list of (increasing) times in splicefile Modes 4-5, or they are picked up by level thresholds (dBon and dBoff (Mode 3). You are advised to familiarise yourself with the way the different Modes create different numbers of output soundfiles containing different numbers of segments." },
  arg1 = { name = "outname", input = "string", def = "generic", tip = "generic root name for the output soundfiles. Numerals starting at 0 are appended to distinguish the outputs." },
  arg2 = { name = "slicefile", input = "txt", tip = "a textfile containing a list of increasing times at which the sound is to be cut." },
  arg3 = { name = "splice", switch = "-s", min = 0, max = 500, def = 15, tip = " the length of the splice in milliseconds (Range: 0 to 500. Default: 15)" },
  arg4 = { name = "x", switch = "-x", tip = "add silence to the end of the segments files so they become the same length as the source"},
  arg5 = { name = "r", switch = "-r", tip = "reverse all the cut-segment files (but not the remnant file)" },
}

--]]

--[[

dsp["[Doesn't work - multiple outputs] Isolate 5 - Cut as in Mode 4 but also overlap the segments slightly: separates speech syllables (slicefile)"] = { 
  cmds = { exe = "isolate", mode = "isolate 5", tip = "5 - Cut as in Mode 4 but also overlap the segments slightly: separates speech syllables (slicefile) . ISOLATE is similar to PARTITION in that the primary operation is to cut a soundfile into disjunct pieces and assign these to different output soundfiles. It does this in a special way so that the time-position of these pieces in the original soundfile is retained: silence is inserted between the cut pieces in the outputs to achieve this ('silent surrounds'). The result is that the disjunct pieces can be reassembled in their original positions (remixed with everything synchronised at time zero): One difference between ISOLATE and PARTITION is that the number of output files is user-defined in PARTITION and worked out by the program, depending on the Mode selected, in ISOLATE. Another different is that Modes 1, 2 and 3 of ISOLATE generate a file containing all the materials left over after cutting. This is also used in reconstructing the original (now-treated) soundfile. However, the main difference between ISOLATE and PARTITION is how the disjunct pieces are identified. In PARTITION there are automatic processes for this, but the number of output files is user-defined by the outcnt parameter, and the durations are controlled by the groupcnt or dur parameters. In ISOLATE the user specifies start end times for the pieces in a textfile: the cutsfile (Modes 1-2) or specifies a list of (increasing) times in splicefile Modes 4-5, or they are picked up by level thresholds (dBon and dBoff (Mode 3). You are advised to familiarise yourself with the way the different Modes create different numbers of output soundfiles containing different numbers of segments." },
  arg1 = { name = "outname", input = "string", def = "generic", tip = "generic root name for the output soundfiles. Numerals starting at 0 are appended to distinguish the outputs." },
  arg2 = { name = "slicefile", input = "txt", tip = "a textfile containing a list of increasing times at which the sound is to be cut." },
  arg3 = { name = "splice", switch = "-s", min = 0, max = 500, def = 15, tip = "the length of the splice in milliseconds (Range: 0 to 500. Default: 15)" },
  arg4 = { name = "dovetail", switch = "-d", min = 0, max = 20, def = 5, tip = "the overlap of cut segments in milliseconds (Range: 0 to 20. Default 5)" },
  arg5 = { name = "x", switch = "-x", tip = "add silence to the end of the segments files so they become the same length as the source"},
  arg6 = { name = "r", switch = "-r", tip = "reverse all the cut-segment files (but not the remnant file)" },
}

--]]
 
------------------------------------------------
-- iterline
------------------------------------------------
 
dsp["Iterline 1 - Gissandi - iterate an input sound, following a transposition line - needs datafile"] = {
  cmds = { exe = "iterline", mode = "iterline 1", channels = "any", tip = "1 - Interpolate between transpositions (glissandi). EXTEND ITERATE repeats a sound over and over until outduration is reached, possibly with an amplitude reduction with each iteration. ITERLINE adds to this the facility to have these iterations follow a time-varying linear contour, either with glissandi or with discrete steps between the iterations." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tdata", input = "txt", tip = "text file of time transposition pairs, with the transpositions given in (possibly factional) semitones" },
  arg4 = { name = "outduration", min = 0, max = 60, def = 10, tip = "duration of the output soundfile" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, def = 0, tip = "the (average) delay betwen iterations" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "randomisation of the delay time. Range: 0 to 1. Default: 0 (no randomisation)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "the maximum value for the random pitch shift of each iteration. Range: 0 to 12 semitones." },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "the maximum value for a random amplitude reduction on each iteration. Range: 0 to 1. Default 0 (no randomisation)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall gain. Range: 0 to 1" },
  arg10 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on each rerun. Default: 0 (random sequence is different every time)" },
  arg11 = { name = "-n", switch = "-n", tip = "normalise the output: the maximum output level is the same as the maximum input level." },
}
 
dsp["Iterline 2 - Step between transpositions (discrete pitch changes) - needs datafile"] = {
  cmds = { exe = "iterline", mode = "iterline 2", channels = "any", tip = "2 - Step between transpositions (discrete pitch changes). EXTEND ITERATE repeats a sound over and over until outduration is reached, possibly with an amplitude reduction with each iteration. ITERLINE adds to this the facility to have these iterations follow a time-varying linear contour, either with glissandi or with discrete steps between the iterations." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tdata", input = "txt", tip = "text file of time transposition pairs, with the transpositions given in (possibly factional) semitones" },
  arg4 = { name = "outduration", min = 0, max = 60, def = 10, tip = "duration of the output soundfile" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, def = 0, tip = "the (average) delay betwen iterations" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "randomisation of the delay time. Range: 0 to 1. Default: 0 (no randomisation)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "the maximum value for the random pitch shift of each iteration. Range: 0 to 12 semitones." },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "the maximum value for a random amplitude reduction on each iteration. Range: 0 to 1. Default 0 (no randomisation)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall gain. Range: 0 to 1" },
  arg10 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on each rerun. Default: 0 (random sequence is different every time)" },
  arg11 = { name = "-n", switch = "-n", tip = "normalise the output: the maximum output level is the same as the maximum input level." },
}
 
------------------------------------------------
-- iterlinef
------------------------------------------------
 
dsp["Iterlinef 1 - Gissandi - iterate an input sound set, following a transposition line  - needs datafile"] = {
  cmds = { exe = "iterlinef", mode = "iterlinef 1", channels = "any", tip = "Here we need to understand what an 'input sound set' is. It must consist of 25 transpositions of a source at intervals of one semitone, in ascending order. The input sounds must be of approximately equal duration." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tdata", input = "txt", tip = "text file of time transposition pairs, with the transpositions given in (possibly factional) semitones" },
  arg4 = { name = "outduration", min = 0, max = 60, def = 10, tip = "duration of the output soundfile" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, def = 0, tip = "the (average) delay betwen iterations" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "randomisation of the delay time. Range: 0 to 1. Default: 0 (no randomisation)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "the maximum value for the random pitch shift of each iteration. Range: 0 to 12 semitones." },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "the maximum value for a random amplitude reduction on each iteration. Range: 0 to 1. Default 0 (no randomisation)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall gain. Range: 0 to 1" },
  arg10 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on each rerun. Default: 0 (random sequence is different every time)" },
  arg11 = { name = "-n", switch = "-n", tip = "normalise the output: the maximum output level is the same as the maximum input level." },
}
 
dsp["Iterlinef 2 - Iterate an input sound set, step between transpositions (discrete pitch changes) - needs datafile"] = {
  cmds = { exe = "iterlinef", mode = "iterlinef 2", channels = "any", tip = "Here we need to understand what an 'input sound set' is. It must consist of 25 transpositions of a source at intervals of one semitone, in ascending order. The input sounds must be of approximately equal duration." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tdata", input = "txt", tip = "text file of time transposition pairs, with the transpositions given in (possibly factional) semitones" },
  arg4 = { name = "outduration", min = 0, max = 60, def = 10, tip = "duration of the output soundfile" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, def = 0, tip = "the (average) delay betwen iterations" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0, tip = "randomisation of the delay time. Range: 0 to 1. Default: 0 (no randomisation)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "the maximum value for the random pitch shift of each iteration. Range: 0 to 12 semitones." },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "the maximum value for a random amplitude reduction on each iteration. Range: 0 to 1. Default 0 (no randomisation)" },
  arg9 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall gain. Range: 0 to 1" },
  arg10 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on each rerun. Default: 0 (random sequence is different every time)" },
  arg11 = { name = "-n", switch = "-n", tip = "normalise the output: the maximum output level is the same as the maximum input level." },
}

------------------------------------------------
-- iterlx
------------------------------------------------
 
dsp["Interlx - Mixpaste - Interleave mono or stereo files"] = { 
  cmds = { exe = "interlx", mode = "", channels = "any", tip = "Here are a few more important things to remember: To create a silent channel, for infile2 onwards, use 0 (zero) as the filename. Infile1 must be a soundfile. Speaker-positions WAVE-EX infiles are ignored. Note that the same infile can be listed multiple times, for example, to write a mono file as stereo, quad, etc. The .amb B-Format extension is supported. The program will issue a warning if the channel count is anomalous. Known B-Format channel counts are: 3, 4, 5, 6, 7, 8, 9, 11, 16. This program complements CHANNELX, and is the primary workhorse for assembling multi-channel files from multiple input files. As the usage message shows, it serves to key tasks: to create WAVE_EX files in particular formats, and to create AMB B-Format files from mono intput files comprising individual B-Format signals. For the latter task, use -t1 to make a generic WAVE_EX file – no speaker positions are defined for the AMB format." }, 
  arg1 = { name = "N", switch = "-t", min = 0, max = 1, def = 0, tip = "write outfile format as type N; 0 - (default) standard soundfile (.wav, .aif, .afc, .aifc), 1 - generic WAVE_EX (no speaker assignments)" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg4 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },  
}
  
------------------------------------------------
-- madrid
------------------------------------------------
 
dsp["Madrid - Spatially syncopate repetitions of the source soundfile(s) - random output file"] = {
  cmds = { exe = "madrid", mode = "madrid 1", channels = "1in2out", tip = "MADRID achieves its syncopated repetitions by randomly deleting items from the spatially-separated repetition streams. The program sets up several sound streams. In each sound stream the same source sounds are repeated at the same time-interval. By randomly deleting repetitions from the various streams, the output appears to be spatially syncopated, as stress is transferred from one stream to another, or to some combination of streams, changing the apparent spatial location of the source." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "dur", min = length/1000, max = 60, def = length/1000, tip = "duration of the output sound" },
  arg5 = { name = "ochans", min = 2, max = 2, def = 2, tip = "number of channels in the output sound (Range 2 to 16)" },
  arg6 = { name = "strmcnt", min = 2, max = 64, tip = "number of spatially distinct streams (2 to 64)" },
  arg7 = { name = "delfact", min = 0, max = 10, input = "brk", def = 2, tip = "proportion of items to (randomly) delete. Values between 0 and 1 delete that proportion of events in the various streams. For values greater than 1, the proportion of events at a single location increases." },
  arg8 = { name = "step", min = 0, max = 60, input = "brk", def = 0.5, tip = "time between event repetitions (Range: 0 to 60 sec.)" },
  arg9 = { name = "rand", min = 0, max = 1, input = "brk", tip = " randomisation of step size (Range: 0 to 1)" },
  arg10 = { name = "seed", switch = "-s", min = 0, max = 1, tip = "value to initialise the randomisation of the delfact deletions. With a non-zero value, rerunning the process with the same parameters will produce the same output. Otherwise, the deletions are always different." },
  arg11 = { name = "-l", switch = "-l", tip = "for ochans > 2, the loudspeaker array is assumed to be circular. The -l flag forces the array to be linear, with defined left and right ends." },
  arg12 = { name = "-e", switch = "-e-", tip = "allow empty events: i.e., sound is absent at some of the repeat-steps." },
  arg13 = { name = "-r", switch = "-r", tip = "randomly permutate the order of input sounds used in the output. (ALL input sounds are used ONCE before the next order permutation is generated.)" },
  arg14 = { name = "-R", switch = "-R", tip = "randomly select the next input sound: the selection is unrelated to the previous selection" },
}
 
dsp["Madrid 2 - Spatially syncopate repetitions of the source soundfile(s) - Use segfile to determine the order of output files"] = {
  cmds = { exe = "madrid", mode = "madrid 2", channels = "1in2out", tip = "2 Use segfile to determine the order of output files. MADRID achieves its syncopated repetitions by randomly deleting items from the spatially-separated repetition streams. The program sets up several sound streams. In each sound stream the same source sounds are repeated at the same time-interval. By randomly deleting repetitions from the various streams, the output appears to be spatially syncopated, as stress is transferred from one stream to another, or to some combination of streams, changing the apparent spatial location of the source." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "segfile", input = "txt", tip = "textfile containing a list of numbers in the range 1 to count-of-infiles which determines the sequence in which the infiles are used in the output" },
  arg5 = { name = "dur", min = 0, max = 60, def = 10, tip = "duration of the output sound" },
  arg6 = { name = "ochans", min = 2, max = 2, def = 2, tip = "number of channels in the output sound (Range 2 to 16)" },
  arg7 = { name = "strmcnt", min = 2, max = 64, tip = "number of spatially distinct streams (2 to 64)" },
  arg8 = { name = "delfact", min = 0, max = 10, input = "brk", def = 2, tip = "proportion of items to (randomly) delete. Values between 0 and 1 delete that proportion of events in the various streams. For values greater than 1, the proportion of events at a single location increases." },
  arg9 = { name = "step", min = 0, max = 60, input = "brk", def = 0.5, tip = "time between event repetitions (Range: 0 to 60 sec.)" },
  arg10 = { name = "rand", min = 0, max = 1, input = "brk", tip = " randomisation of step size (Range: 0 to 1)" },
  arg11 = { name = "seed", switch = "-s", min = 0, max = 1, tip = "value to initialise the randomisation of the delfact deletions. With a non-zero value, rerunning the process with the same parameters will produce the same output. Otherwise, the deletions are always different." },
  arg12 = { name = "-l", switch = "-l", tip = "for ochans > 2, the loudspeaker array is assumed to be circular. The -l flag forces the array to be linear, with defined left and right ends." },
  arg13 = { name = "-e", switch = "-e-", tip = "allow empty events: i.e., sound is absent at some of the repeat-steps." },
  arg14 = { name = "-r", switch = "-r", tip = "randomly permutate the order of input sounds used in the output. (ALL input sounds are used ONCE before the next order permutation is generated.)" },
  arg15 = { name = "-R", switch = "-R", tip = "randomly select the next input sound: the selection is unrelated to the previous selection" },
}
 
------------------------------------------------
-- manysil
------------------------------------------------
 
dsp["Manysil - Insert many silences into a soundfile through time duration pairs from a text file"] = {
  cmds = { exe = "manysil", mode = "manysil", channels = "any", tip = "MANYSIL extends idea of inserting silence, to many silences. Using MANYSIL is straightforward, focusing on setting the times and durations of the silences in the data file. MANYSIL enables us to set up a number of silences all at once. This might be simply to separate data, or one may want to introduce a duration pattern of silences into the sound." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "silencedata", input = "txt", tip = "a textfile with time duration pairs: time is the insertion time of the silence, duration is the duration of the inserted silence." },
  arg4 = { name = "splicelen", min = 0, max = length, def = 5, tip = "length in milliseconds of the splices used " },
}

------------------------------------------------
-- mchanpan
------------------------------------------------

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 1 Move a mono sound around a multi-channel space (timed move)"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 1", channels = "1in2out", tip = "1 - Move a mono sound around a multi-channel space (timed move). We are used to panning sounds within a Left-Right stereo field. MCHANPAN enables us to pan sounds within a much larger speaker array.Mode 1 is the basic mode of operation of the program. In Mode 1 the path of the multi-channel pan motion is set forth in the panfile.The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "panfile", input = "txt", tip = "(Mode 1) Text file with multi-channel output pan-data in value triples: time pan-position pantype" },
  arg4 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg5 = { name = "focus", switch = "-f", min = 0, max = 1, def = 0.5, tip = "focus" },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 2 Switch (silence-separated) mono events in a file from one channel to another"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 2", channels = "1in2out", tip = "2 - Switch (silence-separated) mono events in a file from one channel to another. Mode 2 handles 'events'. Events are defined as sounds in the input file separated by silences. This implies that the input soundfile needs to be a soundfile containing events actually separated by silence. A switch to a new output channel occurs at the entry of a new event in the input file, meaning that each subsequent sound event just 'turns up' at another speaker location without an actual pan movement between the speakers – the illusion of a pan movement may take place if the time between events is very fast. The list of channels is given in the switchdata textfile, such as: 1  5  2  6  8  4  3  7  1. The channel numbers can be separated by a space, a tab or a newline. The channel hopping finishes when there are no more events, so the number of events in the source soundfile should match the number of speaker locations in the switchdata file. If there are more events than speaker positions in the switchdata file, as there are likely to be if a granulated sound is used, the output wraps repeatedly around the defined pattern of the speaker positions. There are a variety of ways to create a soundfile with actual silences between events. This is an important consideration, as a number of the multi-channel effects are dependent on soundfiles of this kind..The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "switchdata", input = "txt", tip = "Textfile containing a listed sequence of output channelsThe sound switches from one output channel to the next." },
  arg4 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg5 = { name = "focus", switch = "-f", min = 0, max = 1, def = 0.5, tip = "focus" },
  arg6 = { name = "minsyl", switch = "-m", min = 0, max = 1000, def = 50, tip = "The minimum duration (ms) of zero-level signal between events to signify that one event has ended and another has begun." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 3 Spread (silence-separated) mono events stepwise from one set of channels, to another set"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 3", channels = "1in2out", tip = "3 - Spread (silence-separated) mono events stepwise from one set of channels, to another set. Mode 3 spreads events left and right from a defined centre. This mode is a little hard to understand. I think Trevor gave me the key when he said that it does not produce a gradual panning movement from the defined centre to the other speakers. Rather, each new sound finds that it needs to be at such and such speakers. 'Each new sound' means that the input soundfile is made up of a number of sound events. Events are defined as sounds in the input soundfile separated by silences. An increase in the spread only occurs at the entry of a new event in the input file. Note the range of parameters that need to be set in this mode: centre, spread, depth, rolloff, and the optional -s flag. The depth parameter is particularly important in that it can be used to suppress sounds at the defined centre (see below). The centre is defined as the principal location of the sound being panned (i.e., on which channel, or on which loudspeaker, it is centred). The centre itself can be changing with time, so that, for example, the sound image may circle around the loudspeakers. The spread defines how wide the sound image is, around that centre (i.e. how much subsequent events spill onto loudspeakers adjacent to the defined centre). The spread can also change over time, so that, for example, a sound image clearly focused in a loudspeaker gradually bleeds onto the adjacent loudspeakers. Imagine that the image is centred in loudspeaker 4, and the spread is 5: the sound image will spread out over the five loudspeakers 2,3,4,5 and 6 (centred on loudspeaker 4). We can now create a 'hole in the middle' effect, so that the centre of the sound image (at loudspeaker 4) is suppressed. We can achieve this effect using the depth parameter. If the depth is sufficiently large (in this case, at least 2.5, i.e. 5 ÷ 2), then we will hear the full image we expect over the five loudspeakers. This is because the depth of the image on each side of the centre is 2.5, so that the image will cover 2.5 + 2.5 = 5 loudspeakers). However, if the depth is less than this (say 2), then the image will cover only four (2 + 2) loudspeakers, and there will be no signal on the central loudspeaker (number 4), even though it is the (now empty) centre of the sound image..The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg4 = { name = "centre", min = 0, max = length/1000, def = 0, input = "brk", tip = "Start-position of the spreading. This position may vary over time." },
  arg5 = { name = "spread", min = 1, max = 10, def = 1, input = "brk", tip = "Integer value defining the channel spread of the output (far Left to far Right of centre). This parameter may also vary over time. The minimum spread is 1" },
  arg6 = { name = "depth", min = 0, max = 32, def = 2, tip = "Integer value defining the maximum number of channels (to Left, and to Right) used, behind leading edges. The signal always reaches the maximum spread, but may have a hole in the middle." },
  arg7 = { name = "rolloff", min = 0, max = 1, def = 1, tip = "The decrease in the amplitude level as the signal spreads over several channels. (Range: 0 to 1)." },
  arg8 = { name = "minsyl", min = 0, max = 1000, def = 50, tip = "The minimum duration (ms) of zero-level signal between events to signify that one event has ended and another has begun." },
  arg9 = { name = "-s", switch = "-s", tip = "Optional parameter to force the output to step wider by 1 channel (to both Left and Right) on every event, as far as the value given for spread." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 4 Spread source gradually from a centre, across several channels)"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 4", channels = "1in2out", tip = "4 - Spread source gradually from a centre, across several channels. Mode 4 makes use of an input source that can be a continuous sound, and the sound spreads gradually over time, interpolating between one spread value and the next. The parameters are almost the same as Mode 3 except that there is no minsil parameter and no -s option. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg4 = { name = "centre", min = 0, max = length/1000, def = 0, input = "brk", tip = "Start-position of the spreading. This position may vary over time." },
  arg5 = { name = "spread", min = 1, max = 10, def = 1, input = "brk", tip = "Integer value defining the channel spread of the output (far Left to far Right of centre). This parameter may also vary over time. The minimum spread is 1" },
  arg6 = { name = "depth", min = 0, max = 32, def = 2, tip = "Integer value defining the maximum number of channels (to Left, and to Right) used, behind leading edges. The signal always reaches the maximum spread, but may have a hole in the middle." },
  arg7 = { name = "rolloff", min = 0, max = 1, def = 1, tip = "The decrease in the amplitude level as the signal spreads over several channels. (Range: 0 to 1)." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 5 Switch (silence-separated) events antiphonally between 2 specified sets of channels"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 5", channels = "1in2out", tip = "5 - Switch (silence-separated) events antiphonally between 2 specified sets of channels. Mode 5 makes use of an input source that contains silences. Here the material being antiphonated is events separated by silence within a single soundfile. In this case, the set of outchannels in use changes only with the entry of a new event in the input file. Note that, because silence is involved, the minsil parameter is used. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "antiphon", input = "string", def = "abcd-efgh", tip = "This parameter consists of two strings of letters, with a hyphen separator ('-'), representing the antiphonal channels." },
  arg4 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg5 = { name = "minsyl", min = 0, max = 1000, def = 50, tip = "The minimum duration (ms) of zero-level signal between events to signify that one event has ended and another has begun." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 6 Switch sounds (in 1 or more files) antiphonally between 2 specified sets of channels"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 6", channels = "1in2out", tip = "6 - Switch sounds (in 1 or more files) antiphonally between 2 specified sets of channels. Mode 6 is similar to Mode 5, but the input sounds need not contain silences, and nore than one input sound can be used. The set of outchans in use changes after the time specified by eventdur. If there is more than 1 input file, the next file is selected at each such antiphonal switch, and if the end of the input files is reached, the first file in the list is chosen again, and processing continues as before. Silence can also be inserted prior to an antiphonal switch by using the gap parameter. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Infile2", input = "wav", tip = "second input soundfile, should be mono" },
  arg3 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg4 = { name = "antiphon", input = "string", def = "abcd-efgh", tip = "This parameter consists of two strings of letters, with a hyphen separator ('-'), representing the antiphonal channels." },
  arg5 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg6 = { name = "eventdur", min = 0, max = length/1000, def = 0.1, input = "brk", tip = "The time (duration) in seconds before switching to the next outchans set. This parameter can vary over time by using a time duration breakpoint file instead of a numerical constant." },
  arg7 = { name = "gap", min = 0, max = length/1000, def = 0, input = "brk", tip = "The time gap (silence) switching between event sets. This parameter can vary over time by using a time duration breakpoint file instead of a numerical constant. NB: Gap cannot be equal to or greater than eventdur." },
  arg8 = { name = "splice", min = 0, max = 1000, def = 20, tip = "The duration (in ms) of the splices used when cutting events." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 7 Pan from one channel configuration to another, passing through surround-mono"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 7", channels = "1in2out", tip = "7 - Pan from one channel configuration to another, passing through surround-mono. Mode 7 is the nearest we can get, outside Ambisonics, to making all the sounds (on all channels) move into the (listener) centre of the space, and then back out (to possibly some other orientation of the channels). In reality, on a ring of loudspeakers, all the sounds expand from the channel they are in to fill all the available channels, then contract onto a (possibly new) channel, as specified in the data file. Suppose we had a pandata file containing the following times and pan-positions: time  loudspeaker 0.0  1 4.0  4 8.0  6 12.0  2. We start with the sound concentrated on loudspeaker 1. It then expands outwards to all channels, but then contracts again so that at time 4.0 seconds, it is concentrated on loudspeaker 4. From here it expands outwards again to all speakers, then focusing at time 8.0 seconds on speaker 6, etc. coming to rest at time 12.0 at speaker 2. The psycho-acoustics of this process are still being researched.. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pandata", input = "txt", tip = "Textfile with output pan-data in lines, each having time pan-positions value pairs.See CDP docs for details" },
  arg4 = { name = "roll_off", min = 3.1e-05, max = 1, def = 1, tip = "Rate of loss of level with successive echoes (Range: 0.000031 to 1.0, Default: 1)" },
}
--]]

--[[ mchanpan mchanpan 8 only available in soundloom --]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 9 Rotate a mono soundfile around a multi-channel space"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 9", channels = "1in2out", tip = "9 - Rotate a mono soundfile around a multi-channel space. Mode 9 In Mode 1, the pan data file allows you to move the source sound clockwise, anticlockwise, or directly between non-adjacent output channels, and you can change the type or direction of motion at any time, by specifying a change of motion type in the panning data. Mode 9 allows only clockwise or anti-clockwise rotation (but not both), and the only parameter is the speed of rotation, which may vary over time. Mode 9 is simpler to use if only (uni-directional) rotation is desired. Use Mode 1 to get to a specific channel at a specified time. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg4 = { name = "startchan", min = 0, max = 16, def = 2, tip = "startchan" },
  arg5 = { name = "speed", min = 0, max = 64, def = 0, input = "brk", tip = "The speed of rotation in revolutions per second. This parameter can vary over time, and can be zero. Range: Range 0 - 64" },
  arg6 = { name = "focus", min = 0, max = 1, def = 0.5, tip = "focus" },
  arg7 = { name = "-a", switch = "-a", tip = "With the this flag set, stepping to an adjacent output channel is avoided. Using this option makes the scattering of events in the multi-channel space more pronounced. Note that this option is only possible with more than 4 output channels." },
}
--]]

--[[ doesn't work, minimum amount of channel output = 3, and a max of 16 channels 
dsp["Mchanpan Mchanpan - 10 Switch (silence-separated) mono events randomly from one output channel to another"] = {
  cmds = { exe = "mchanpan", mode = "mchanpan 10", channels = "1in2out", tip = "10 - Switch (silence-separated) mono events randomly from one output channel to another. Mode 10 Similar to Mode 2, but the events are switched randomly between output channels. All the output channels are used up before a new random-permutation of the output channels is begun. It is possible to group the events at each channel: for example, 3 events occur at each (randomly selected) output channel. Also see parameter grouping. It is possible to randomise this grouping of events: see the -r flag. It is also possible to avoid stepping to an adjacent output channel; see the -a flag. The musical results that can be achieved with MCHANPAN can be reviewed by looking through the list of modes for the program. Panning in a multi-channel context is a fundamental procedure. Please refer to Panning and multi-channel Mixfiles below for more information about how to set up a multi-channel mixfile while using (already) panned multi-channel soundfiles. Any takers? At least one (comprehensive) worked example (with written out breakpoint files) still seems essential, though the new Appendix on MULTIMIX mixfiles provides more information. Perhaps a full multi-channel tutorial taking the user through all the stages, from diagram to breakpoint files for PAN, to the way the mixfile is created and then used, and perhaps a discrete use of REVERB for additional spatial effects ... [Ed.]" },
  arg1 = { name = "Infile", input = "wav", tip = "input soundfile: soundfiles should be Mono" },
  arg2 = { name = "Outfile", output = "wav", tip = "multi-channel output soundfile" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output file" },
  arg4 = { name = "focus", switch = "-f", min = 0, max = 1, def = 0.5, tip = "focus" },
  arg5 = { name = "minsyl", switch = "-m", min = 0, max = 1000, def = 10, tip = "The minimum duration (ms) of zero-level signal between events to signify that one event has ended and another has begun." },
  arg6 = { name = "grouping", min = 0, max = 100, def = 1, tip = "With a grouping of e.g. 3, three events occur at the first (randomly selected) output channel, then three events at the second (randomly selected) output channel, and so on. Range 1 - 100 (Default 1)" },
  arg7 = { name = "-a", switch = "-a", tip = "With the this flag set, stepping to an adjacent output channel is avoided. Using this option makes the scattering of events in the multi-channel space more pronounced. Note that this option is only possible with more than 4 output channels." },
  arg8 = { name = "-r", switch = "-r", tip = "With this flag set, the grouping of events at output channels is randomised. For example, with a grouping of 3, either 3, 2 or 1 events may occur at the next (randomly selected) output channel. For the -r flag to take effect the grouping must be more than 1." },
}
--]]

------------------------------------------------
-- mchanrev
------------------------------------------------
 
dsp["Mchanrev Mchanrev - Create multi-channel Echoes or Reverb from a mono soundfile"] = {
  cmds = { exe = "mchanrev", mode = "mchanrev", channels = "1in2out", tip = "MCHANREV is the multi-channel equivalent of MODIFY REVECHO Mode 3, i.e., STADIUM ECHO. The size parameter controls the amount of time there is between 'echoes'. With size of 1 (sec.) or more, individual echoes in the output are likely to be heard, depending on the length of the input soundfile. If the value for size is longer than the duration of the input soundfile, you will hear the input sound repeating (in the different speaker range defined by spread, gradually fading away to nothing. (I have not been able to find an upper limit for size. With smaller values for size, and large values of count (the number of echoes), reverberation will be produced. Larger values for size in connection with larger values for count means more overlap, and this will increase the amplitude, which may not be fully compensated for by the program. You may therefore need to apply a gain reduction (values increasingly less than 1.) Note, therefore, the interplay between size, count and gain, which may require some trial and error to get a good signal level in the output without producing distortion. The Default roll-off value is 1.0. This is the maximum decrease in amplitude from one echo to the next. As the value for roll-off decreases, there are higher amplitudes in the successive echoes / reverberations, meaning more chance of amplitude overload. This therefore another parameter to handle carefully to get the best signal level without amplitude overload. The echoes or reverberant output can be centred at a particular channel in the multi-channel space, using the parameters centre, and spread. These will determine where the centre of the sound image is located and how wide the reverberant image is: how many adjacent output channels it spreads across. If the image is spread over all the output channels, the reverberant image will fill the multi-channel space. The wider the output spread, the more echoes (larger count) and higher echo density (smaller size) are needed to produce a similar reverberation result." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", min = 0, max = 1, def = 0.645654, tip = "Gain to apply to input signal before processing (0-1, Default: 0.645654)" },
  arg4 = { name = "roll_off", min = 0, max = 1, def = 1, tip = "Rate of loss of level with successive echoes (Range: 0.000031 to 1.0, Default: 1)" },
  arg5 = { name = "size", min = 0, max = 10, def = 0.1, tip = "Multiplies average time between echoes: (Default time 0.1 secs)" },
  arg6 = { name = "count", min = 1, max = 1000, def = 10, tip = "Number of stadium echoes (1- 1000, Default (max): 1000)" },
  arg7 = { name = "outchans", min = 2, max = 2, def = 2, tip = "Number of channels in the output sound - only 2 supported in Renoise" },
  arg8 = { name = "centre", min = 1, max = 2, tip = "Centre position of reverberated image, amongst the output channels" },
  arg9 = { name = "spread", min = 2, max = 2, def = 2, tip = "Number of output channels over which echoes are spread. (Range: 2 to N) - only 2 supported in Renoise" },
}
 
------------------------------------------------
-- mchiter
------------------------------------------------
 
dsp["Mchiter Iter 1 - Iterate the input sound in a fluid manner, scattering around a multi-channel space"] = {
  cmds = { exe = "mchiter", mode = "iter 1", channels = "1in2out", tip = "This is a 'scattering' algorithm, similar to EXTEND ITERATE, but with the addition of multi-channel spatial scattering. Notice that the amplitude of the sound is diminishing with each repetition. Sound is distributed randomly among the speakers of the multi-channel rig for a given length of time (outduration). The various parameter options make it possible to vary the repetitions in a number of ways. MCHITER ITER extends a sound by iterating it. This can be a simple repetition of (a part of) the source, as in EXTEND LOOP, but more typically, each iteration can be (slightly) transposed or varied in level, to produce a more naturalistic iterated output. In addition the iterated copies of the sound are distributed at random to the output channels of a multi-channel space. Each channel of the output space is visited before a new random permutation of the ouput channels is begun. Musical Applications; MCHITER ITER can be used to distribute a small sound over an entire multi-channel space. See EXTEND ITERATE for more information about the iteration process. See also MCHZIG ZAG and MCHSHRED SHRED. For a more controlled spatial distributon (e.g. over less channels of the space, or in a way which varies through time), see TEXMCHAN TEXMCHAN." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "number of channels in the output file - only 2 supported in Renoise" },
  arg4 = { name = "outduration", min = 0, max = 60, def = 10, tip = "duration of the output sound. (Must be greater than that of the input)" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, tip = "(average) delay between the iterations: must be less than or equal to the duration of the input sound" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0.5, tip = "randomisation of the delay time. Range: 0 to 1 (Default 0)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "maximum of the random pitchshift of each iteration, in semitones. Range: 0 to 12 (Default 0)" },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "maximum of the random amplitude fluctuation (reduction) at each iteration. Range: 0 to 1 (Default 0)" },
  arg9 = { name = "fade", switch = "-f", min = 0, max = 1, def = 0, tip = "(average) amplitude fade from one iteration to the next. Range: 0 to 1 (Default 0)" },
  arg10 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall Gain. Range: 0 to 1 (Default 0). Using the special value 0 (default), gives the best guess for no output distortion" },
  arg11 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on rerunning the process. The special value 0 produces a different random sequence every time the process is run. (Default 0)." },
}
 
dsp["Mchiter Iter 2 - Iterate the input sound in a fluid manner, scattering around a multi-channel space"] = {
  cmds = { exe = "mchiter", mode = "iter 2", channels = "1in2out", tip = "This is a 'scattering' algorithm, similar to EXTEND ITERATE, but with the addition of multi-channel spatial scattering. Notice that the amplitude of the sound is diminishing with each repetition. Sound is distributed randomly among the speakers of the multi-channel rig for a given length of time (outduration). The various parameter options make it possible to vary the repetitions in a number of ways. MCHITER ITER extends a sound by iterating it. This can be a simple repetition of (a part of) the source, as in EXTEND LOOP, but more typically, each iteration can be (slightly) transposed or varied in level, to produce a more naturalistic iterated output. In addition the iterated copies of the sound are distributed at random to the output channels of a multi-channel space. Each channel of the output space is visited before a new random permutation of the ouput channels is begun. Musical Applications; MCHITER ITER can be used to distribute a small sound over an entire multi-channel space. See EXTEND ITERATE for more information about the iteration process. See also MCHZIG ZAG and MCHSHRED SHRED. For a more controlled spatial distributon (e.g. over less channels of the space, or in a way which varies through time), see TEXMCHAN TEXMCHAN." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "number of channels in the output file" },
  arg4 = { name = "repetitions", min = 0, max = 60, def = 10, tip = "number of iterations of the input sound in the output" },
  arg5 = { name = "delay", switch = "-d", min = 0, max = length/1000, tip = "(average) delay between the iterations: must be less than or equal to the duration of the input sound" },
  arg6 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0.5, tip = "randomisation of the delay time. Range: 0 to 1 (Default 0)" },
  arg7 = { name = "pshift", switch = "-p", min = 0, max = 12, def = 0, tip = "maximum of the random pitchshift of each iteration, in semitones. Range: 0 to 12 (Default 0)" },
  arg8 = { name = "ampcut", switch = "-a", min = 0, max = 1, def = 0, tip = "maximum of the random amplitude fluctuation (reduction) at each iteration. Range: 0 to 1 (Default 0)" },
  arg9 = { name = "fade", switch = "-f", min = 0, max = 1, def = 0, tip = "(average) amplitude fade from one iteration to the next. Range: 0 to 1 (Default 0)" },
  arg10 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0, tip = "overall Gain. Range: 0 to 1 (Default 0). Using the special value 0 (default), gives the best guess for no output distortion" },
  arg11 = { name = "seed", switch = "-s", min = 0, max = 10, def = 0, tip = "the same seed-number will produce identical output on rerunning the process. The special value 0 produces a different random sequence every time the process is run. (Default 0)." },
}

------------------------------------------------
-- mchshred
------------------------------------------------

dsp["Mchshred Shred - 1 Sound is cut into random segments which are then reassembled in random order within the original duration"] = {
  cmds = { exe = "mchshred", mode = "shred 1", channels = "1in2out", tip = "1 - Shreds a mono input file to a multi-channel output. MCHSHRED SHRED shreds a sound in the same manner as MODIFY RADICAL, Mode 2. In addition: In mode 1, the shredded elements are randomly distributed over a multi-channel space. In mode 2, each of the shredded elements has its frame randomly reoriented in the multi-channel space. For example, input channels 1-2-3-4-5-6-7-8 may end up in output channels 6-7-8-1-2-3-4 respectively. Musical Applications; See MODIFY RADICAL, Mode 2 for more information about the shredding process. MCHSHRED can be used to distribute the fragments of a sound over an entire multi-channel space. For a more controlled spatial distributon, such as over less channels of the space, or in a way which varies through time, see TEXMCHAN TEXMCHAN." },
  arg1 = { name = "infile", input = "wav", tip = "input soundfile, Mono in Mode 1" },
  arg2 = { name = "outmcfile", output = "wav", tip = "output multi-channel soundfile - stereo in Renoise" },
  arg3 = { name = "repeats", min = 0, max = 1000, def = 10, tip = "number of repetitions of the shredding" },
  arg4 = { name = "chunklen", min = 0, max = length, def = 50, tip = "average length of the chunks to cut and permute" },
  arg5 = { name = "scatter", min = 0, max = 1, def = 1, tip = "randomisation of the cuts Range: 0 to K (Default = 1)" },
  arg6 = { name = "outchans", min = 2, max = 2, def = 2, tip = "number of channels in the output sound. Mode 1 shreds a mono input into a multi-channel output." },
}

dsp["Mchshred Shred - 2 Sound is cut into random segments which are then reassembled in random order within the original duration"] = {
  cmds = { exe = "mchshred", mode = "shred 2", channels = "2in2out", tip = "2 - Shreds a multi-channel input file, reorienting shredded units in the multi-channel space. MCHSHRED SHRED shreds a sound in the same manner as MODIFY RADICAL, Mode 2. In addition: In mode 1, the shredded elements are randomly distributed over a multi-channel space. In mode 2, each of the shredded elements has its frame randomly reoriented in the multi-channel space. For example, input channels 1-2-3-4-5-6-7-8 may end up in output channels 6-7-8-1-2-3-4 respectively. Musical Applications; See MODIFY RADICAL, Mode 2 for more information about the shredding process. MCHSHRED can be used to distribute the fragments of a sound over an entire multi-channel space. For a more controlled spatial distributon, such as over less channels of the space, or in a way which varies through time, see TEXMCHAN TEXMCHAN." },
  arg1 = { name = "infile", input = "wav", tip = "input soundfile, Multi-channel in Mode 2" },
  arg2 = { name = "outmcfile", output = "wav", tip = "output multi-channel soundfile - stereo in Renoise" },
  arg3 = { name = "repeats", min = 0, max = 1000, def = 10, tip = "number of repetitions of the shredding" },
  arg4 = { name = "chunklen", min = 0, max = length, def = 50, tip = "average length of the chunks to cut and permute" },
  arg5 = { name = "scatter", min = 0, max = 1, def = 1, tip = "randomisation of the cuts Range: 0 to K (Default = 1)" },
}

------------------------------------------------
-- multimix create
------------------------------------------------

dsp["Multimix Create - 1 Create a mix where all files start at time zero. (Sound design amalgamation of sounds.)"] = {
  cmds = { exe = "multimix", mode = "create 1", tip = "1 - Create a mix where all files start at time zero. (Sound design amalgamation of sounds.) Input files can have any number of channels, including a mixture of different numbers of channels. For example the inputs for the example command line above are mono, stereo and 4-channel.Outchannel count is set to the maximum channel-count among the input files. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
}

dsp["Multimix Create - 2 Create a mix where each file enters where the last ended. (End-to-end mixing.)"] = {
  cmds = { exe = "multimix", mode = "create 2", tip = "2 - Create a mix where each file enters where the last ended. (End-to-end mixing.) Input files can have any number of channels. Outchannel count is set to the maximum channel-count among the input files. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
}

dsp["Multimix Create - 3 Create a mix where each file enters a given timestep after the start of the previous sound"] = {
  cmds = { exe = "multimix", mode = "create 3", tip = "3 - Create a mix where each file enters a given timestep after the start of the previous sound (Constant time interval between entries.) Input files can have any number of channels. Outchannel count is set to the maximum channel-count among the input files.. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
  arg6 = { name = "timestep", min = 0, max = 60, def = 1, tip = "time in seconds between entry of each input file in the mix" },
}

dsp["Multimix Create - 4 Distributes a mono or stereo input to two stereo pairs, one narrow and one wide, in an 8-channel output frame, with balance parameter."] = {
  cmds = { exe = "multimix", mode = "create 4", tip = "4 - Distributes a mono or stereo input to two stereo pairs, one narrow and one wide, in an 8-channel output frame, with balance parameter. (Stereo assignments.) Input Stereo-Right (or mono) goes to 1 and 2. Input Stereo-Left (or mono) goes to 8 and 7. All input files are sent to the same set of output channels.. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
  arg6 = { name = "balance", min = 0, max = 10, def = 1, tip = "proportion of the stereo signal assigned to the wider of the 2 stereo pairs" },
}

dsp["Multimix Create - 5 Distributes mono or stereo input over 8 loudspeakers, with extra level-control parameters. (8-wide distribution.)"] = {
  cmds = { exe = "multimix", mode = "create 5", tip = "5 - Distributes mono or stereo input over 8 loudspeakers, with extra level-control parameters. (8-wide distribution.) Input Stereo-Right (or mono) goes to 1, 2, 3 & 4. Input Stereo-Left (or mono) goes to 8, 7, 6 & 5. All input files are sent to the same set of output channels.. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
  arg6 = { name = "stage", min = 0, max = 10, def = 1, tip = "level of signal assigned to front loudspeaker pair (8,1)" },
  arg7 = { name = "wide", min = 0, max = 10, def = 1, tip = "level of signal assigned to front-wide loudspeaker pair (7,2)" },
  arg8 = { name = "rearwide", min = 0, max = 10, def = 1, tip = "level of signal assigned to rear-wide loudspeaker pair (6,3)" },
  arg9 = { name = "rear", min = 0, max = 10, def = 1, tip = "level of signal assigned to rear loudspeaker pair (5,4)." },
}

dsp["Multimix Create - 6 Distributes N mono files, in order, to N successive output channels. (Distribution in ascending order.)"] = {
  cmds = { exe = "multimix", mode = "create 6", tip = "6 – Distributes N mono files, in order, to N successive output channels. (Distribution in ascending order.) Output Channel count = number of input files. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
}

dsp["Multimix Create - 7 Distributes N mono files, in order, to K successive output channels. (Channels may exceed or be less than inputs.)"] = {
  cmds = { exe = "multimix", mode = "create 7", tip = "7 - Distributes N mono files, in order, to K successive output channels. (Channels may exceed or be less than inputs.) Output Channel count (ochans) can be greater than the number of input files, in which case the remaining channels are assigned no signal. Output Channel count (ochans) can be less than the number of input files, in which case some signals will be assigned to the same channel For example, 6 files into a 4-channel output file gives inmonosnd1-to-1, inmonosnd2-to-2, inmonosnd3-to-3, inmonosnd4-to-4, inmonosnd5-to-1, inmonosnd6-to-2. Also note startch, skip and timestep parameters. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
  arg6 = { name = "ochans", min = 2, max = 16, def = 2, tip = "number of channels in the output soundfile generated by the mixoutfile. Range: 2 to 16." },
  arg7 = { name = "startch", min = 1, max = 32, def = 2, tip = "this parameter defines which of the multi-channel file's channels will get the first sound in the list." },
  arg8 = { name = "skip", switch = "-s", min = 1, max = 32, def = 1, tip = "this parameter defines the channel-step between channel assignment in the output multi-channel soundfile." },
  arg9 = { name = "timestep", switch = "-t", min = 0, max = 32, def = 1, tip = "In the mode where a mixfile is created with a timestep between the entry of each input soundfile, this parameter defines that timestep." },
}

dsp["Multimix Create - 8 Each file in the mix starts at zero, leftmost channel to outchan 1."] = {
  cmds = { exe = "multimix", mode = "create 8", tip = "8 - Each file in the mix starts at zero, leftmost channel to outchan 1. In Mode 1, The number of output channels in the mixfile output is determined by the maximum number of channels in any of the input files. In Mode 8, The number of output channels in the mixfile output is explicitly specified.. This process is similar to the existing SUBMIX DUMMY (but with additional options for creating the multi-channel mixfile). The multi-channel mixfile has a slightly different format to standard CDP mixfiles – see Files & Formats or NEWMIX MULTICHAN below, or the Appendix in this document for a fuller explanation. The mixfile for multi-channel work is the same as standard CDP mixfiles except for the following: There is an extra initial line that states the number of output channels On ensuing lines, input and output channels are numbered 1, 2, 3, etc. Routing of input to output is indicated by inchan colon outchan, e.g., 1:4 (no spaces) sends input channel 1 to output channel 4. The levels on these channels are in the range -1 to 1. An input channel must be routed to an output channel, with a level: e.g., 1:1 1.0 2:4 0.5, meaning: input channel 1 to output channel 1 with level 1.0, input channel 2 to output channel 4 with level 0.5. You can route an input channel to several output channels: e.g., 1:1 0.5  1:2 0.3   1:4 0.7 etc. You must take care with levels, where more than one input goes to the same output. As a rule of thumb, the sum of the levels should not exceed 1.0. Using MULTIMIX is the easiest way to produce a multi-channel mixfile of the correct format. The file can then be edited, either as a textfile or, on the Sound Loom, using the QikEdit facilties, where many new functions have been added to automatically manipulate the data lines in the mixfile. We have added several examples of multi-channel mixfiles in the MULTIMIX Mixfiles Appendix below, along with a discussion about moving on to use the mixfile with NEWMIX MULTICHAN, and some notes about facilities available on the Sound Loom GUI. Musical Applications; Generating multi-channel mixfiles in the correct format is not always straightforward. Even when MULTIMIX will not generate exactly the file you need it is usually easier to create a dummy file here and subsequently edit it (in a text editor or on the Sound Loom's QikEdit facility) than to write out a multi-channel mixfile from scratch in the correct format." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "mixoutfile", output = "data", tip = "output multi-channel mixfile, with the extension .mmx" },
  arg6 = { name = "ochans", min = 2, max = 16, def = 2, tip = "number of channels in the output soundfile generated by the mixoutfile. Range: 2 to 16." },
}

dsp["Newmix Multichan - Mix from a multi-channel mixfile to give a multi-channel soundfile output."] = {
  cmds = { exe = "newmix", mode = "multichan", tip = "Multi-channel mixfiles can also be written as text files, and are similar to the Standard CDP mixfiles. However, it is recommended that these files be generated using MULTIMIX CREATE (and subsequently edited if necessary) to ensure that they are in the correct format. For an overview of the format of .mmx files, see Files & Formats. As with standard mixfiles, The first entry in a line is the name of an input sound (with a path unless it is in the current directory). The second entry in the line is the time in the mix at which it starts. The third entry in the line is the number of channels in the input sound. The format for the channels for NEWMIX MULTICHAN has been changed to accommodate both a larger number of channels and channel group routing. You can examine the output mixfiles of MULTIMIX CREATE to get more examples of multi-channel mixfiles.  The main differences from standard mixfiles are: There is an extra initial line that gives the number of output channels that there will be in the resultant multi-channel soundfile. On ensuing lines, input and output channels are numbered 1,2,3 etc. ...16. Routing of input to output is indicated by 'inchan:outchan' (no spaces) e.g. 1:4 sends input channel 1 to output channel 4. An input channel must be routed to an output channel, with a level: e.g. 2:4 0.5 (input 2 is sent to output 4 with level 0.5). The levels on these channel-routings are in the range 0 to 1. You can route an input to many outs as with sound4.wav in the mixfile above. You must take care with levels where more than 1 input goes to the same output. Panning and multi-channel Mixfiles: We need to consider how these channel assignments in the mixfile relate to a multi-channel input soundfile that has already been panned with MCHANPAN. T Wishart writes: If a sound has already been panned around the multi-channel space (using MCHANPAN) the panned-sound output consists of a set of discrete signals assigned to the N channel outputs of the multi-channel sound. (The movement of the sound is the result of the changing levels in the individual channels of the multi-channel output). Routing – To preserve the 'sense' of the panning, when mixing, routing should be N:N i.e., 1:1 2:2 3:3 4:4 5:5 etc. so that the orientation of the signal is not changed. It is very important to remember this when using panned multi-channel soundfiles in a mixfile. Shifting the whole pattern – The entire panning pattern could be shifted to a different position. For example, shifted along by N loudspeaker positions, by making the assignment 1:1+N. Thus, to shift the pattern by 4 loudspeakers, use the assignment 1:5 2:6 3:7 4:8 5:1 6:2 7:3 8:4 (with the output channel values wrapping back to 1 when the maximum output channel is reached). This transformation preserves the original rotation, but starts it in a different place (i.e., at output channel 5). Mirroring – Alternatively, the panning data can be mirrored (e.g. a clockwise rotation can be made into an anticlockwise rotation) by reversing the output channel order e.g., (for 8 channels) you would put 1:1 2:8 3:7 4:6 5:5 6:4 7:3 8:2. (Note that this transformation will also convert an anticlockwise rotation into a clockwise rotation – this needs a little thought - these channel assignments all take place at the same time, before the sound begins to play. We are not 'panning' (actively moving) the sound here, merely reassigning the channels on which the sound will by played). Reorientation – This reorientation or mirroring can also be achieved directly on the multi-channel sound itself, using the Reorient option in FRAME SHIFT. Musical Applications; Note that the program MULTIMIX CREATE can be used to automatically generate multi-channel mixfiles of the correct format for use by NEWMIX, and this is the recommended approach to creating these files. For more information and various examples of MULTIMIX mixfiles, please see the section MULTIMIX Mixfiles Appendix. It may be useful to summarise the various roles of NEWMIX MULTICHAN, MULTIMIX CREATE and MCCHANPAN: NEWMIX MULTICHAN (this function) does the mixing, using a multi-channel mixfile (.mmx). This .mmx file can be written by hand with a text editor, or your could use the facilities of MULTIMIX CREATE to make it (recommended). For its format, see Files & Formats, as well as the Appendix in this document. The distinguishing feature of a multi-channel mixfile is that it includes routing information for each channel, expressed in the format IN-CHANNEL:OUT-CHANNEL, as in 2:7, i.e., channel-2 of the input soundfile becomes channel-7 in the output multi-channel mixfile. For example, where the input to MULTIMIX CREATE is a mono file, 'IN-CHANNEL' will be '1' and refer to that particular sound. It can be routed to more than one 'OUT-CHANNEL'. Modes 4 & 5 also accept stereo input, so 'IN-CHANNEL' can be '1' or '2'. Note that all this is simply a speaker assignment – it does not imply any pan movement. MCHANPAN is the function that produces pan and must be applied to the input MONO soundfile(s) before mixing (only Mode 4 & 5 allows a Stereo infile). Because panning is applied before mixing, the level changes that create the illusion of movement are already in place in any panned-soundfile before it is used in the mix. The data in the panfile used to generate the panning motion has time position direction information (example) . When using these already panned soundfiles as inputs to NEWMIX MULTICHAN, remember to do a straight routing for all channels, i.e., 1:1, 2:2, 3:3 ... 8:8. This preserves the movement pattern of the already panned input (multi-channel) soundfile. It would seem that the crucial aspect of this is keeping track of what sound material is on each channel before starting to mix. NEWMIX MULTICHAN can accept mono, stereo or 2+ channel inputs, and the latter could consist of several mono sounds that have been panned into a multi-channel soundfile. You are therefore advised to find a way to chart out what is where in the multi-channel mixfile. If any of the above is either ambiguous or incorrect, or could use further explanation, please let me know so I can revise the documentation. (Ed.) Also see Appendix: M-C MIXFILES T Wishart writes: MULTIMIX is concerned with the mixing of sounds of any number of channels. Those sounds may themselves be moving around the multi-channel space (see MCHANPAN). This is equivalent to mixing stereo signals containing sound-motion, as in a normal CDP mix. MULTIMIX cannot itself be used to pan sounds (make them move around the multi-channel space – this is done with MCHANPAN) or to change an existing movement. Assigning input channel N to output channel N will ensure that any motion within an input sound in the mix is preserved. But (as in the examples above) an existing movement can be affected by how the signal is routed - it can be shifted or mirrored or in fact distorted in any desired way, according to how the input channels are assigned to the output channels." },
  arg1 = { name = "mixfile", input = "data", tip = "input multi-channel mixfile, with the extension .mmx" },
  arg2 = { name = "mchoutsndfile", output = "data", tip = "output multi-channel soundfile" },
  arg3 = { name = "start", switch = "-s", min = 0.0, max = 30, def = 0.0, tip = "time in seconds at which to start mixing within the mixfile. Defaults to 0.0." },
  arg4 = { name = "end", switch = "-e", min = 0.0, max = 60, tip = "time in seconds at which to stop mixing within the mixfile. Defaults to the end of the complete mix." },
  arg5 = { name = "gate", switch = "-g", min = 0, max = 1.0, def = 1.0, tip = "the level of the output mix. Defaults to 1.0 (no attenuation)." },
}

------------------------------------------------
-- panorama
------------------------------------------------

dsp["Panorama - 1 Loudspeakers are assumed to be equally spaced. Distribute > 1 mono files in a spatial panorama across a specified angle of a sound-surround loudspeaker array)"] = {
  cmds = { exe = "panorama", mode = "panorama 1", tip = "1 - Loudspeakers are assumed to be equally spaced. PANORAMA is new in Release 7. It distributes two or more mono soundfiles in a spatial panorama, with a multi-channel mixfile as the output. Loudspeakers are assumed to be effectively surrounding the listening area from the front outwards. The input sounds are distributed in order from the leftmost to the rightmost position: with a 360° spread, 180° is rightmost. Assuming the listener is in the middle of the auditorium, facing the front, the centre-line passes through the listener in the direction s/he is facing. So in an 8-channel ring, with loudspeaker 1 placed centrally at the front, it passes through loudspeakers 1 and 5. However, the loudspeakers do not have to encircle the listener. The parameter ang_width refers to the total spread angle of the loudspeakers. The minimum is 190° (just greater than 180°) with loudspeakers from (just behind) left of the central-listener, through to (just behind) the right of the central listener ... and none in the rest of the space behind the listener. The maximum is 360°, with the loudspeakers completely encircling the listener. In Mode 1 the loudspeakers are assumed to be equidistant (except for the gap between the leftmost and rightmost speakers in the non-encircling cases). In a 190° total spread the leftmost position is at -95° and the rightmost position at +95°. In a 360° total spread, the leftmost position is at -180°, and rightmost at +180° (i.e. both in loudspeaker 5 in a centred octagonal arrangement). In Mode 2 the loudspeaker positions can be specified. NB: There is an ambiguity in the usage, as the Mode 2 loudspeaker position information specifies position from 0 (front centre) through to 360, rather than from -180 to +180." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "outmixfile", output = "data", tip = "output multi-channel mixfile for use with the multi-channel mixer NEWMIX. NB: The suffix produced is .mmx whatever is specified." },
  arg6 = { name = "lspk_cnt", min = 0, max = 100, def = 2, tip = "the number of loudspeakers. In Mode 1 they are assumed to be equally spaced, but this is not necessarily the case for Mode 2." },
  arg7 = { name = "lspk_aw", min = -360, max = 360, def = 0, tip = "the angular width of the loudspeaker array (Range: 190° - 360°), front centre at 0. The loudspeaker array is assumed to be symmetrical around a centre-line, running through front-centre of the loudspeaker array and centre of the auditorium. Thus, if front centre is at 0°, the a 190° spread is from -95° to (+)95°. A 360° spread is from -180° to (+)180°." },
  arg8 = { name = "sounds_aw", min = -360, max = 360, def = 0, tip = "the angular width of the output sounds, equal to or less than lspk_aw." },
  arg9 = { name = "sounds_ao", min = -360, max = 360, def = 0, tip = "the angular offset of the output sounds. This is only possible if sounds_aw is less than lspk_aw: the angle between the centre-line of the sounds and the centre-line of the loudspeakers." },
  arg10 = { name = "config", min = 1, max = 360, def = 1, tip = "the distribution of the output sounds within the output angle, config must be a divisor of the number of input sounds.." },
  arg11 = { name = "rand", switch = "-r", min = 0, max = 1, def = 1, tip = "a randomisation of sound positions (Range: 0 to 1)" },
  arg12 = { name = "-p", switch = "-p", tip = "set this flag to give a pair of loudspeakers at the front. ??NB: if lspk_aw is less than 360°, this flag will be ignored ??interpreted differently. Therefore, If the angular width of the loudspeakers (lspk_aw) < 360°, an odd number of loudspeakers gives one speaker at centre- front (the Default), and an even number of loudspeakers gives a pair of loudspeakers centred on the front. If lspk_aw = 360°, the speaker orientation is ambiguous." },
  arg13 = { name = "-q", switch = "-q", tip = "the same logic as for the -p flag operates here, but applied to sound positions: ??ignored if sounds_aw < 360°." },
}

dsp["Panorama - 2 Loudspeaker positions are defined in a textfile. Distribute > 1 mono files in a spatial panorama across a specified angle of a sound-surround loudspeaker array)"] = {
  cmds = { exe = "panorama", mode = "panorama 2", tip = "2 - Loudspeaker positions are defined in a textfile. PANORAMA is new in Release 7. It distributes two or more mono soundfiles in a spatial panorama, with a multi-channel mixfile as the output. Loudspeakers are assumed to be effectively surrounding the listening area from the front outwards. The input sounds are distributed in order from the leftmost to the rightmost position: with a 360° spread, 180° is rightmost. Assuming the listener is in the middle of the auditorium, facing the front, the centre-line passes through the listener in the direction s/he is facing. So in an 8-channel ring, with loudspeaker 1 placed centrally at the front, it passes through loudspeakers 1 and 5. However, the loudspeakers do not have to encircle the listener. The parameter ang_width refers to the total spread angle of the loudspeakers. The minimum is 190° (just greater than 180°) with loudspeakers from (just behind) left of the central-listener, through to (just behind) the right of the central listener ... and none in the rest of the space behind the listener. The maximum is 360°, with the loudspeakers completely encircling the listener. In Mode 1 the loudspeakers are assumed to be equidistant (except for the gap between the leftmost and rightmost speakers in the non-encircling cases). In a 190° total spread the leftmost position is at -95° and the rightmost position at +95°. In a 360° total spread, the leftmost position is at -180°, and rightmost at +180° (i.e. both in loudspeaker 5 in a centred octagonal arrangement). In Mode 2 the loudspeaker positions can be specified. NB: There is an ambiguity in the usage, as the Mode 2 loudspeaker position information specifies position from 0 (front centre) through to 360, rather than from -180 to +180." },
  arg1 = { name = "infile", input = "wav", tip = "name of first mono or stereo soundfile to be mixed" },
  arg2 = { name = "infile2", input = "wav", tip = "name of second mono or stereo soundfile to be mixed" },
  arg3 = { name = "infile3", input = "wav", tip = "name of the third mono or stereo soundfile to be mixed" },
  arg4 = { name = "infile4", input = "wav", tip = "name of the fourth mono or stereo soundfile to be mixed" },
  arg5 = { name = "outmixfile", output = "data", tip = "output multi-channel mixfile for use with the multi-channel mixer NEWMIX. NB: The suffix produced is .mmx whatever is specified." },
  arg6 = { name = "lspk_positions", input = "txt", tip = "speaker positions are specified in a textfile list of angular positions of (3 - 16) loudspeakers.See CDP docs for details" },
  arg7 = { name = "lspk_aw", min = -360, max = 360, def = 0, tip = "the angular width of the loudspeaker array (Range: 190° - 360°), front centre at 0. The loudspeaker array is assumed to be symmetrical around a centre-line, running through front-centre of the loudspeaker array and centre of the auditorium. Thus, if front centre is at 0°, the a 190° spread is from -95° to (+)95°. A 360° spread is from -180° to (+)180°." },
  arg8 = { name = "sounds_aw", min = -360, max = 360, def = 0, tip = "the angular width of the output sounds, equal to or less than lspk_aw." },
  arg9 = { name = "sounds_ao", min = -360, max = 360, def = 0, tip = "the angular offset of the output sounds. This is only possible if sounds_aw is less than lspk_aw: the angle between the centre-line of the sounds and the centre-line of the loudspeakers." },
  arg10 = { name = "config", min = 1, max = 360, def = 1, tip = "the distribution of the output sounds within the output angle, config must be a divisor of the number of input sounds.." },
  arg11 = { name = "rand", switch = "-r", min = 0, max = 1, def = 1, tip = "a randomisation of sound positions (Range: 0 to 1)" },
  arg12 = { name = "-p", switch = "-p", tip = "set this flag to give a pair of loudspeakers at the front. ??NB: if lspk_aw is less than 360°, this flag will be ignored ??interpreted differently. Therefore, If the angular width of the loudspeakers (lspk_aw) < 360°, an odd number of loudspeakers gives one speaker at centre- front (the Default), and an even number of loudspeakers gives a pair of loudspeakers centred on the front. If lspk_aw = 360°, the speaker orientation is ambiguous." },
  arg13 = { name = "-q", switch = "-q", tip = "the same logic as for the -p flag operates here, but applied to sound positions: ??ignored if sounds_aw < 360°." },
}

------------------------------------------------
-- modify
------------------------------------------------
 
dsp["Modify Brassage - 6 Brassage. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 6", channels = "any", tip = "BRASSAGE: powerful segmentation/fragmentation procedures using constants or time-varying breakpoint files" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "velocity", min = 0, max = 1, input = "brk", def = 0.1, tip = "stretch or compress the infile in time, while retaining the same pitch" },
  arg4 = { name = "density", min = 0, max = 2, input = "brk", def = 1, tip = "amount of grain overlap" },
  arg5 = { name = "grainsize", min = 2, max = 1997.12, input = "brk", def = 5, tip = "size of the grains in milliseconds" },
  arg6 = { name = "pitchshift", min = -0.33, max = 0.33, input = "brk", tip = "shift the pitch of infile while retaining (more or less) the same duration" },
  arg7 = { name = "amp", min = 0, max = 1, input = "brk", tip = "gain applied to the grains (Range: 0 to 1; Default: 1.0)" },
  arg8 = { name = "space", min = 0, max = 1, input = "brk", tip = "spatial position – set stereo position" },
  arg9 = { name = "bsplice", min = 0, max = 100, input = "brk", def = 5, tip = "length of start-splices on grains in ms (Default: 5)" },
  arg10 = { name = "esplice", min = 0, max = 100, input = "brk", def = 5, tip = "length of end-splices on grains in ms (Default: 5)" },
  arg11 = { name = "range", switch = "-r", min = 0, max = 50, input = "brk", def = 20, },
  arg12 = { name = "jitter", switch = "-j", min = 0, max = 1, input = "brk", tip = "randomisation of grain position" },
  arg13 = { name = "outlength", switch = "-l", min = 0, max = 60, def = 15, tip = "maximum outfile length (if end of data is not reached)" },
  arg14 = { name = "channel", switch = "-c", min = 0, max = 2, tip = " extract and work on just one channel of a stereo input (Range: 1 or 2) Set channel to 0 (the Default) for this parameter to be ignored." },
  arg15 = { name = "-d", switch = "-d", },
  arg16 = { name = "-x", switch = "-x", tip = "do exponential splices (Default: linear)" },
  arg17 = { name = "-n", switch = "-n", tip = "no interpolation for pitch values (quick but dirty)" },
}
 
dsp["Modify Brassage - 7 Full Monty. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 7", channels = "any", tip = "FULL MONTY: finely tuned granular textures using parameter ranges with upper and lower limits, which can be set as constants or as time-varying breakpoint files, or mixtures of the two. You are recommended to use the graphic program GrainMill for this, if it is available." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "velocity", min = 0, max = 1, input = "brk", def = 0.2, tip = "stretch or compress the infile in time, while retaining the same pitch" },
  arg4 = { name = "density", min = 0, max = 2, input = "brk", def = 1, tip = "amount of grain overlap" },
  arg5 = { name = "hvelocity", min = 0, max = 5000, input = "brk", },
  arg6 = { name = "hdensity", min = 0, max = 5000, input = "brk", },
  arg7 = { name = "grainsize", min = 2, max = 1997.12, input = "brk", def = 5, tip = "size of the grains in milliseconds" },
  arg8 = { name = "pitchshift", min = -0.33, max = 0.33, input = "brk", tip = "shift the pitch of infile while retaining (more or less) the same duration" },
  arg9 = { name = "amp", min = 0, max = 1, input = "brk", tip = "gain applied to the grains (Range: 0 to 1; Default: 1.0)" },
  arg10 = { name = "space", min = 0, max = 1, input = "brk", tip = "spatial position – set stereo position" },
  arg11 = { name = "bsplice", min = 0, max = 1000, input = "brk", def = 5, tip = "length of start-splices on grains in ms (Default: 5)" },
  arg12 = { name = "esplice", min = 0, max = 1000, input = "brk", def = 5, tip = "length of end-splices on grains in ms (Default: 5)" },
  arg13 = { name = "hgrainsize", min = 0, max = 5000, input = "brk", def = 10 },
  arg14 = { name = "hpitchshift", min = -0.33, max = 0.33, input = "brk", def = 0 },
  arg15 = { name = "hamp", min = 0, max = 1, input = "brk", def = 0.5 },
  arg16 = { name = "hspace", min = 0, max = 2, },
  arg17 = { name = "hbsplice", min = 0, max = 1000, input = "brk", def = 10 },
  arg18 = { name = "hesplice", min = 0, max = 1000, input = "brk", def = 30 },
  arg19 = { name = "range", switch = "-r", min = 0, max = 50, input = "brk", def = 20 },
  arg20 = { name = "jitter", switch = "-j", min = 0, max = 1, input = "brk", tip = "randomisation of grain position" },
  arg21 = { name = "outlength", switch = "-l", min = 0, max = 60, def = 15, tip = "maximum outfile length (if end of data is not reached)" },
  arg22 = { name = "channel", switch = "-c", min = 0, max = 2, tip = " extract and work on just one channel of a stereo input (Range: 1 or 2) Set channel to 0 (the Default) for this parameter to be ignored." },
  arg23 = { name = "-d", switch = "-d", },
  arg24 = { name = "-x", switch = "-x", tip = "do exponential splices (Default: linear)" },
  arg25 = { name = "-n", switch = "-n", tip = "no interpolation for pitch values (quick but dirty)" },
}
 
dsp["Modify Brassage - 5 Granulate. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 5", channels = "any", tip = "GRANULATE: 'granulate' (put a grainy surface on) a source. A density of 1.0 will achieve this; < 1 will introduce gaps, and values > 1.1 (out of a range which ends at 2.0) begin to sound smooth again." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "density", min = 0, max = 2, input = "brk", def = 1, tip = "amount of grain overlap" },
  arg4 = { name = "-d", switch = "-d", },
}
 
dsp["Modify Brassage - 1 Pitchshift. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 1", channels = "any", tip = "PITCHSHIFT: shift the pitch of infile while retaining (more or less) the same duration" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitchshift", min = -0.12, max = 12, input = "brk", tip = "shift the pitch of infile while retaining (more or less) the same duration" },
}
 
dsp["Modify Brassage - 3 Reverb. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 3", channels = "any", tip = "REVERB: use 3 parameters to create a kind of reverberant effect. Pitch – transposition factor (Range: -0.33 to 0.33)" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "density", min = 0, max = 1000, input = "brk", def = 55, tip = "amount of grain overlap (Range: > 0)" },
  arg4 = { name = "pitch", min = -0.33, max = 0.33, input = "brk", tip = "transposition factor (Range: -0.33 to 0.33)" },
  arg5 = { name = "amp", min = 0, max = 1, input = "brk", tip = "gain applied to the grains (Range: 0 to 1; Default: 1.0)" },
  arg6 = { name = "range", switch = "-r", min = 0, max = 50, input = "brk", def = 20, },
}
 
dsp["Modify Brassage - 4 Scramble. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 4", channels = "any", tip = "SCRAMBLE: random reordering of grains within a timeframe. NB: To get a scramble effect, you do need to provide a timeframe (in ms) with -rrange to overcome the default setting of 0." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "grainsize", min = 2, max = 1997.12, input = "brk", tip = "size of the grains in milliseconds" },
  arg4 = { name = "range", switch = "-r", min = 0, max = 1000, input = "brk", },
}
 
dsp["Modify Brassage - 2 Timestretch. Granular reconstitution of soundfile"] = {
  cmds = { exe = "modify", mode = "brassage 2", channels = "any", tip = "TIMESTRETCH: stretch or compress the infile in time, while retaining the same pitch" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "velocity", min = 0, max = 1, input = "brk", tip = "stretch or compress the infile in time, while retaining the same pitch" },
}
 
dsp["Modify Convolve 1 - Convolve the first sound with the second "] = {
  cmds = { exe = "modify", mode = "convolve 1", channels = "any", tip = "Convolution is based on complex mathematical operations between two source soundfiles. There is a great deal of information about convolution available in Wikipedia, but your editor does not know just which ones Trevor Wishart uses in his program. Musical applications; The result of convolution is unpredictable but fascinating. The operation of this particular sub-module can take rather a long time, so you are recommended also to make use of FASTCONV." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Convolve 2 - Convolve the first sound with the second based upon time semitone-transposition pairs in textfile "] = {
  cmds = { exe = "modify", mode = "convolve 2", channels = "any", tip = "Convolution is based on complex mathematical operations between two source soundfiles. There is a great deal of information about convolution available in Wikipedia, but your editor does not know just which ones Trevor Wishart uses in his program. Musical applications; The result of convolution is unpredictable but fascinating. The operation of this particular sub-module can take rather a long time, so you are recommended also to make use of FASTCONV." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "transposfile", input = "txt", tip = "textfile of time semitone-transposition transposition pairs" },
}
 
dsp["Modify Loudness 1 - Alter gain of sound"] = {
  cmds = { exe = "modify", mode = "loudness 1", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", min = 0, max = 100, def = 1, tip = "adjust level by factor gain" },
}
 
dsp["Modify Loudness 2 - Alter dB-gain of sound"] = {
  cmds = { exe = "modify", mode = "loudness 2", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", min = -96, max = 96, tip = "level expressed in dB (Range: -96dB to 96dB). NB: every drop of -6dB halves the previous level." },
}
 
dsp["Modify Loudness 3 - Normalise"] = {
  cmds = { exe = "modify", mode = "loudness 3", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "level", switch = "-l", min = 0, max = 1, tip = "force level (if necessary) to the maximum possible, or to the level given (Range: 0 to 1)." },
}
 
dsp["Modify Loudness 4 - Force level"] = {
  cmds = { exe = "modify", mode = "loudness 4", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "level", switch = "-l", min = 0, max = 1, tip = "force level to maximum possible, or to the level given (Range: 0 to 1)" },
}
 
dsp["Modify Loudness 5 - Balance, force the maxmimum level of infile to the maximum level of infile2 "] = {
  cmds = { exe = "modify", mode = "loudness 5", channels = "any", tip = "BALANCE: force the maxmimum level of infile to the maximum level of infile2" },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Loudness 6 - Invert Phase"] = {
  cmds = { exe = "modify", mode = "loudness 6", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Loudness 8 - Equalise: force all of 2 files to level of loudest file. "] = {
  cmds = { exe = "modify", mode = "loudness 8", channels = "any", tip = "EQUALISE: force all of 2 files to level of loudest file." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Radical - 6 Cross Modulate"] = {
  cmds = { exe = "modify", mode = "radical 6", channels = "any", tip = "CROSS MODULATE: Two input soundfiles are multiplied, creating complex sidebands. CROSS MODULATION: multiplies two different soundfiles, producing a very strange mixture of the two." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Radical - 4 Lose resolution"] = {
  cmds = { exe = "modify", mode = "radical 4", channels = "any", tip = "LOSE RES: Reducing the sample rate reduces the level of the Nyquist frequency (sample_rate/2), thereby lowering the frequency level which can be safely handling during processing. Lowering bit-resolution reduces the precision of the numerical expression of the data, making the digital 'quantisation' of the sonic material coarser. This means that time-varying information is lost." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "bit_resolution", min = 0, max = 16, },
  arg4 = { name = "srate_division", min = 1, max = 256, def = 1, tip = "Range: 1 to 256 Default 1 (normal)" },
}
 
dsp["Modify Radical - 1 Reverse"] = {
  cmds = { exe = "modify", mode = "radical 1", channels = "any", tip = "Sound plays backwards, the soundfile is re-written back to front: starting at the end and ending at the beginning." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Radical - 5 Ring Modulate"] = {
  cmds = { exe = "modify", mode = "radical 5", channels = "any", tip = "RING MODULATION: multiplies two (bipolar) signals. In this case, one signal is a soundfile and the other is a modulating_frq. This creates two 'sidebands' which are the sum and the difference of the two signals, while the carrier signal disappears. The result is a timbrally 'hollow' sound. (See Curtis Roads, The Computer Music Tutorial, pp. 215-220)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "modulating-frq", min = 0, max = 22050, tip = "number of cycles per second" },
}
 
dsp["Modify Radical - 3 Scrub back and forth"] = {
  cmds = { exe = "modify", mode = "radical 3", channels = "any", tip = "This is an acceleration/deceleration process which models an editing procedure used in the 'classical' tape studio: the desired edit point was found by turning the tape spools by hand so that the tape moved (very slowly!) across the tape head. You could hear locate exactly where silence began or ended, where clicks came etc., although the sound was very low because of the slow speed.The SCRUB function could also be used creatively, to create extreme speed modifications of the source sound on the tape, e.g. an abrupt acceleration-deceleration as the tape went from not-moving, to fast-moving, to stopped as the hands jerked the tape across the heads." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "dur", min = 0, max = 20, tip = "minimum length of outfile required" },
  arg4 = { name = "down", switch = "-l", min = 0, max = 12, tip = "lowest downward tranposition in semitones" },
  arg5 = { name = "up", switch = "-h", min = 0, max = 12, tip = " highest upward transposition in semitones" },
  arg6 = { name = "start", switch = "-s", tip = "scrubs start before time start seconds " },
  arg7 = { name = "end", switch = "-e", tip = "scrubs end after time end" },
}
 
dsp["Modify Radical - 2 Shred"] = {
  cmds = { exe = "modify", mode = "radical 2", channels = "any", tip = "The soundfile is (randomly) segmented, and these segments reordered by means of a permutation process. As the number of repeats increases, it gets more and more jumbled, literally 'reducing it to shreds'. The -n adds splicing that results in a smoother output." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "repeats", min = 0, max = 1000, tip = "number of repeats of shredding process" },
  arg4 = { name = "chunklen", min = 0, max = length/1000, tip = "average length of chunks to cut and permutate" },
  arg5 = { name = "scatter", switch = "-s", min = 0, max = 10, tip = "randomisation of cuts (Range: 0 to K, where K = duration of infile/chunklen Default = 1)" },
  arg6 = { name = "-n", switch = "-n", tip = "use this flag for a smoother output" },
}
 
dsp["Modify Revecho - 3 Stadium Echo"] = {
  cmds = { exe = "modify", mode = "revecho 3", channels = "any", tip = "This creates a stereo outfile, bouncing the signal between speakers with a very prominent delay factor. The idea is that you hear the signal bouncing around the 'stadium'. The defaults work quite nicely, but you can intensify the effect by multiplying the delay time with the size parameter. Note that this is a multiple, so values less than 0 (multiplied with the default time of 0.1 sec) will decrease the delay time. Beyond size = 2, you are likely to run into insufficient buffer space, so if you really want longer delay times, you will have to increase the buffer size with 'set CDP_MEMORY_BBSIZE=...' (bear in mind the RAM capacity of your machine). (The default buffer size is 1 Mbyte and the units are in 'k': e.g., a BBSIZE of 3000 units is 3 Mbyte)" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", switch = "-g", min = 0, max = 1, def = 0.645654, tip = "to apply to input signal (Default: 0.645654)" },
  arg4 = { name = "roll_off", switch = "-r", min = 0, max = 10, def = 1, tip = "rate of loss of level across stadium (Default: 1))" },
  arg5 = { name = "size", switch = "-s", min = 0, max = 10, tip = "multiplies average time between echoes (the Default time between echoes of 0.1 sec)" },
  arg6 = { name = "count", switch = "-e", min = 0, max = 23, tip = "number of stadium echoes (Default and max: 23)" },
}
 
dsp["Modify Revecho - 1 Standard delay"] = {
  cmds = { exe = "modify", mode = "revecho 1", channels = "any", tip = "Mode 1 provides a standard delay. If a smooth echoey effect is wanted, be careful to keep mix on the low side. Remember that delay is given in milliseconds; after 60ms or so, one begins to hear a strong reverberation; after 100 ms the reverberation begins to 'bounce', and by 200 ms one begins to hear distinct echoes. Delay times greater than 1000 will cause repetitions of (all or much of) the sound. Don't forget to lengthen the tail to match the delay time.The mix parameter, in adding in the delayed signal, increases the reverberant effect. With a short delay time, this can sound like the reverberation which occurs in an enclosed space. Feedback has a similar effect, becoming very pronounced towards a value of 1.0 – rather like the 'feedback' which occurs when a microphone is placed facing a loudspeaker. The operation of a standard delay line in Mode 1 ranges from modest reverberation to echo effects. The reverberations, however, tend easily towards rough edges, so for smoother reverbs, look to the programs listed below." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "delay", min = 0, max = 5000, def = 100, tip = "delay time, in milliseconds" },
  arg4 = { name = "mix", min = 0, max = 1, tip = " amount of delayed signal in final mix: 0 gives 'dry' result (Range: 0 to 1)" },
  arg5 = { name = "feedback", min = -1, max = 1, tip = "produces resonances related to delay time (with short times) (Range: -1.0 to 1.0)" },
  arg6 = { name = "tail", min = -1, max = 1, tip = "time to allow decayed signal to decay to zero (Range: -1.0 to 1.0)" },
  arg7 = { name = "prescale", switch = "-p", tip = "prescales input level, to avoid overload" },
  arg8 = { name = "-i", switch = "-i", tip = "inverts the dry signal (for phasing effects)" },
}
 
dsp["Modify Revecho - 2 Varying delay"] = {
  cmds = { exe = "modify", mode = "revecho 2", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "delay", min = 0, max = 5000, def = 100, tip = "delay time, in milliseconds" },
  arg4 = { name = "mix", min = 0, max = 1, def = 0.5, tip = " amount of delayed signal in final mix: 0 gives 'dry' result (Range: 0 to 1)" },
  arg5 = { name = "feedback", min = -1, max = 1, tip = "produces resonances related to delay time (with short times) (Range: -1.0 to 1.0)" },
  arg6 = { name = "lfomod", min = 0, max = 1, def = 0.3, tip = "depth of the delay-variation sweep (Range: 0 to 1)" },
  arg7 = { name = "lfofreq", min = -50, max = 50, tip = "frequency of the delay-variation sweep (negative values give random oscillations)" },
  arg8 = { name = "lfophase", min = 0, max = 1, tip = "start-phase of the delay-variation sweep (Range: 0 to 1)" },
  arg9 = { name = "lfodelay", min = 0, max = length/1000, def = 0, tip = "time in seconds before the delay-variation sweep begins" },
  arg10 = { name = "tail", min = -1, max = 1, tip = "time to allow decayed signal to decay to zero (Range: -1.0 to 1.0)" },
  arg11 = { name = "prescale", switch = "-p", tip = "prescales input level, to avoid overload" },
  arg12 = { name = "seed", switch = "-s", min = 0, max = 100, tip = "non-zero value gives reproducible output (with the same seed) where random oscillations are used " },
}
 
dsp["Modify Sausage - Brassage on several sources"] = {
  cmds = { exe = "modify", mode = "sausage", channels = "any", tip = "MODIFY SAUSAGE is in fact a slight variant on MODIFY BRASSAGE in which the latter is adapted to accept more than one infile. This means that the 'brassage' operations will cycle around the input soundfiles, producing a more complex sonic texture. For Reference manual details, see MODIFY BRASSAGE above.  NB: This program is not the same as the pre-Release 4 CDP program, SAUSAGE. The older SAUSAGE has the useful option to have the grains from the different sources appear in regular rotation or in random order. It can also cycle round a list of pitch (i.e., grain transposition) values. Because of these unique features, the older SAUSAGE program has been recompiled for Release 4 and included under 'CDP EXTRAS'. It is invoked on the command line simply by 'sausage' and has its own HTML manual of the same name. Musical applications; MODIFY SAUSAGE should be used when granular transformations need to work with several input soundfiles." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "velocity", min = 0, max = 10, input = "brk", def = 1, tip = "speed of advance in infile, relative to outfile (Range: >= 0)" },
  arg5 = { name = "density", min = 0, max = 2000, def = 1, tip = "amount of grain overlap (Range: > 0) " },
  arg6 = { name = "hvelocity", min = 0, max = 10000, input = "brk", def = 10, tip = "the width (from far Left to far Right) of spatialisation around centre" },
  arg7 = { name = "hdensity", min = 0, max = 20000, input = "brk", tip = "the number of channels with sound, behind the leading edges of spread." },
  arg8 = { name = "grainsize", min = 0, max = 1000, input = "brk", def = 50, tip = "size of the grains in milliseconds (Range: must be > 2 * the length of the splices; Default: 50ms)" },
  arg9 = { name = "pitchshift", min = -100, max = 100, input = "brk", def = 0, tip = "transposition of grains in + or - (fractions of) semitones" },
  arg10 = { name = "amp", min = 0, max = 1, input = "brk", tip = "gain applied to the grains (Range: 0 to 1; Default: 1.0)" },
  arg11 = { name = "space", min = 0, max = 1, input = "brk", tip = "set stereo position in outfile 0 = L, 1 = R (Range: 0 to N) " },
  arg12 = { name = "bsplice", min = 0, max = 2000, input = "brk", def = 5, tip = "length of start-splices on grains in ms (Default: 5)" },
  arg13 = { name = "esplice", min = 0, max = 2000, input = "brk", def = 5, tip = "length of end-splices on grains in ms (Default: 5)" },
  arg14 = { name = "hgrainsize", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitchshift in + or - (fractions of) semitones." },
  arg15 = { name = "hpitchshift", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitchshift in + or - (fractions of) semitones." },
  arg16 = { name = "hamp", min = 0, max = 1, input = "brk", def = 1, tip = "gain applied to the grains (Range: 0 to 1; Default: 1.0)" },
  arg17 = { name = "hspace", min = 0, max = 1, input = "brk", tip = "set stereo position in outfile 0 = L, 1 = R (Range: 0 to N)" },
  arg18 = { name = "hbsplice", min = 0, max = length, input = "brk", def = 5, tip = "length of start-splices on grains in ms (Default: 5)" },
  arg19 = { name = "hesplice", min = 0, max = length, input = "brk", def = 5, tip = "length of end-splices on grains in ms (Default: 5)" },
  arg20 = { name = "range", switch = "-r", min = 0, max = length/1000, input = "brk", def = 0, tip = " of search for next grain, before infile 'now' (Default: 0 ms)" },
  arg21 = { name = "jitter", switch = "-j", min = 0, max = 1, input = "brk", def = 0.5, tip = "randomisation of grain position; Range 0 to 1 (Default: 0.5)" },
  arg22 = { name = "outlength", switch = "-l", min = 0, max = 60, def = 0, tip = "maximum outfile length (if end of data is not reached)" },
  arg23 = { name = "channel", switch = "-c", min = 1, max = 2, def = 1, tip = "extract and work on just one channel of a multi-channel input (Range: 1 to N) " },
  arg24 = { name = "-d", switch = "-d", tip = "If the space flag is used on a stereo input, the program mixes it to mono before acting." },
  arg25 = { name = "-x", switch = "-x", tip = "do exponential splices (Default: linear)" },
  arg26 = { name = "-n", switch = "-n", tip = "no interpolation for pitch values (quick but dirty) " },
}
 
dsp["Modify Scaledpan - Distribute sound in stereo space, scaling pan data to soundfile duration through breakpoint file"] = {
  cmds = { exe = "modify", mode = "scaledpan", channels = "1in2out", tip = "Sometimes it is useful to apply a process to many sounds, slightly modifying that process to suit the different durations of the source files. This is especially useful when using BULK PROCESSING on the Sound Loom, so that new data files do not need to be written for every single input source. MODIFY SCALEDPAN enables you to take an existing pan file, for example with a panning pattern you use often, and apply it to a new sound. With bulk processing you could apply it to hundreds of sounds of slightly different duration. Musical applications; In breakpoint files, it can be important that the final value come at or near the end of the input soundfile. This can be particularly significant in PAN operations. Besides enabling you to reuse a favourite pan file with new sounds easily, the scaling also ensures (automatically) that the panning is properly timed with the duration of the infile." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pan", min = -1, max = 1, input = "brk", def = 0, tip = "pan data breakpoint file: positions sound in a stereo field, from -1 (Left) to 1 (Right), or beyond" },
  arg4 = { name = "prescale", switch = "-p", min = 0, max = 100, def = 0.7, tip = "gain reduction factor applied to input level in order to avoid clipping (Default: 0.7)" },
}
 
dsp["Modify Space - 3 MIRROWPAN: Invert stereo positions in a pan data file"] = {
  cmds = { exe = "modify", mode = "space 3", tip = "Mode 3 MIRRORPAN: this Mode acts on a breakpoint file, reversing the pan data. Thus, if the pan was from Left to Right, (-1 to 1), it would become Right to Left (1 to -1)." },
  arg1 = { name = "Input.txt", input = "txt", tip = "a text file containing time pan_position data" },
  arg2 = { name = "Output.txt", output = "txt", tip = "a text file containing time pan_position data produced by the program, with the pan positions reversed" },
}
 
dsp["Modify Space - 2 Mirrow: two channels swap sides "] = {
  cmds = { exe = "modify", mode = "space 2", channels = "2in2out", tip = "MIRROR: the data in the two channels swaps sides, a utility for a quick Left/Right swap" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Modify Space - 4 Narrow: Narrow the stereo image of a sound"] = {
  cmds = { exe = "modify", mode = "space 4", channels = "2in2out", tip = "A way to restrict the spatial location of a sound in the horizontal plane, this program allows you to narrow the spread of a stereo field" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "narrowing", min = -1, max = 1, tip = "make the stereo image less wide" },
}
 
dsp["Modify Space - 1 pan"] = {
  cmds = { exe = "modify", mode = "space 1", channels = "1in2out", tip = "Adjusts the amplitude levels of the two channels so as to create spatial illusions. A time pan breakpoint file is used to create movement. Mode1 PAN: control of sound in 3-D space has been a basic procedure when composing with 'sound' material, especially when these are drawn from nature/the environment. The assumption here is that you are thinking about the placement and movement of sound as an integral part of your compositional process. In this case, we are dealing with building these procedures into the sound itself. There is a further aspect external to the sound itself, achieved by 'mixing' (superimposing sounds and writing a new soundfile) and 'diffusion' (sound placement during playback achieved by manipulating the pan controls on a mixer)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pan", min = -1, max = 1, input = "brk", tip = "floating point value to specify location between the pair of stereo speakers, or breakpoint file of time pan pairs. Range: -1.0 to 1.0; 0.0 is centre." },
  arg4 = { name = "prescale", switch = "-p", min = 0, max = 1, tip = "gain factor multiplier with which to adjust the input level (Default: 0.7)" },
}
 
dsp["Modify Spaceform - Create a sinusoidal spatial distribution data file"] = {
  cmds = { exe = "modify", mode = "spaceform", tip = "MODIFY SPACEFORM produces a pan breakpoint file (.brk would be the correct extension) that can be used with MODIFY PAN or MODIFY SCALEDPAN. Note that the duration of the file, i.e., the last time in the file, can be specified. This function in effect swings the sound back and forth between the speakers in a stereo field. The speed at which it does so is controlled by cyclelen and the location between the speakers is handled by width. If width is: 1, the full stereo field is used, 0, no panning takes place and the sound is heard equally (without movement) from both Left and Right speakers. 0.5, the swing is from halfway between the Left speaker and Center to halfway between the Right speaker and Center, i.e., from -0.5 to + 0.5. The quantisation parameter determines the number of intermediate pan positions there will be during each swing around the loop. 0.1 appeared to produce a good result. The phase sets where in the stereo field the sound will be heard at the beginning of each repeat cycle, from whence the movement will emerge. Note the sophistication that can be brought to the spatial movement when breakpoint files for cyclelen and phase are employed. Musical applications; A slowly cycling panning motion can be created, generating a sense of an object rotating in space, e.g. a marble rolling around the rim of a huger glass bowl in front of you. This spatial image can be enhanced by making the level increase on the left-to-right motion, and decrease on the right-to-left motion (or vice cersa), suggesting that the sound moves towards, then away from you, as well as from right to left. This perception can be enhanced by adding treble-cut filtering and/or subtle reverb to the 'more distant' part of the motion. More generally speaking, sound streams in a piece can be differentiated and characterised by their spatial motion.Also see MODIFY SPACE (PAN is Mode 1) and MODIFY SCALEDPAN." },
  arg1 = { name = "Output.txt", output = "txt", tip = "output pan datafile produced by the program" },
  arg2 = { name = "cyclelen", min = 0, max = 60, input = "brk", def = 6, tip = "the duration of one complete sinusoidal pan cycle" },
  arg3 = { name = "width", min = 0, max = 1, tip = "the width of the pan (from 0 to 1, full width)" },
  arg4 = { name = "dur", min = 0, max = 60, def = 10, tip = "the duration of the output file" },
  arg5 = { name = "quantisation", min = 0, max = 60, def = 2, tip = "the time step between successive space-position specifications" },
  arg6 = { name = "phase", min = 0, max = 360, input = "brk", def = 180, tip = "the angular position at which the pan starts. 0 is full left and 360 is full right." },
}
 
dsp["Modify Speed - 5 Accelerate or decelerate a sound"] = {
  cmds = { exe = "modify", mode = "speed 5", channels = "any" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "accel", min = 0, max = 100, tip = "multiplication of speed to be reached by goaltime – i.e., a transposition ratio" },
  arg4 = { name = "goaltime", min = 0, max = length/1000, tip = "time in outfile at which the accelerated speed is to be reached." },
  arg5 = { name = "starttime", switch = "-s", min = 0, max = length/1000, tip = "time in infile / outfile at which the acceleration begins" },
}
 
dsp["Modify Speed - 6 Add vibrato to a sound"] = {
  cmds = { exe = "modify", mode = "speed 6", channels = "any", tip = "The vibrato created in Mode 6 is a frequency modulation. Given the very wide ranges allowed, this function is immensely powerful. A slow vibrate with a large vibdepth will swing the original sound wildly – increase vibrate and it really 'flaps in the breeze' (like a flag in the wind). A fast vibrate with a reasonably tight vibdepth, e.g., a minor 3rd, will produce a fluttering effect." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "vibrate", min = 0, max = 120, input = "brk", tip = "the rate of vibrato shaking in cyles-per-second (Range: 0.0 to 120.0)" },
  arg4 = { name = "vibdepth", min = 0, max = 96, input = "brk", tip = "vibrato depth (pitch shift from centre) in [possibly fractional] semitones (Range: 0.0 to 96.0)" },
}
 
dsp["Modify Speed - 2 Vary speed & pitch by (fractional) number of semitones)"] = {
  cmds = { exe = "modify", mode = "speed 2", channels = "any", tip = "Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch.MODIFY SPEED offers a range of functions which affect the speed of the soundfile. Perhaps it will be most often used for transposition. Modes 1 and 2 both accept either single values or the names of time-varying breakpoint files with time transposition pairs. The single values act as constants and transpose the whole soundfile up or down by the given amount. In the breakpoint files, transposition can be almost instantaneous (almost same time, different transposition value), or gradual, creating glissandi (different time, different transposition value). No transposition between times is a third possibility (different time, same transposition value). Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.)" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "semitone-transpos", min = -12, max = 12, input = "brk", tip = " transposition value in positive or negative number of semitones; e.g., 12 raises the sound by an octave, and -12 lowers it by an octave. NB: use 0 (semitones) for no transposition." },
}
 
dsp["Modify Speed - 1 Vary speed & pitch of a sound"] = {
  cmds = { exe = "modify", mode = "speed 1", channels = "any", tip = "Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch.MODIFY SPEED offers a range of functions which affect the speed of the soundfile. Perhaps it will be most often used for transposition. Modes 1 and 2 both accept either single values or the names of time-varying breakpoint files with time transposition pairs. The single values act as constants and transpose the whole soundfile up or down by the given amount. In the breakpoint files, transposition can be almost instantaneous (almost same time, different transposition value), or gradual, creating glissandi (different time, different transposition value). No transposition between times is a third possibility (different time, same transposition value). Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.)" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "speed", min = 0, max = 4, input = "brk", tip = "transposition value (ratio) expressed as a floating point multiplier" },
}
 
dsp["Modify Stack - Create a mix that stacks transposed versions of source on top of one another "] = {
  cmds = { exe = "modify", mode = "stack", channels = "any", tip = "New sounds can be developed from existing sources by stacking transposed copes of the original on top of each other. This is particularly successful where the source sound has a clearly defined attack at or near the start of the sound and then dies away gradually. If the copies are lined up in such a way that their attacks coincide exactly, the new sound will psycho-acoustically merge into a single sound event with harmonic content (rather than a chord made up of several sounds). The meaning of this is very precise: one is lining up the soundfiles at their perceived beginning, i.e., its attack, not the start of each soundfile. This is achieved by adjusting the start times of the transposed soundfiles. MODIFY STACK is a very important program. It enables you to construct 'larger' versions of sounds by superimposing transposed versions on top of one another. Furthermore, it does this in such a way as to maximise the fusion of the layered sound by synchronising the attacks. This relates to a basic technique of orchestral scoring (where it is called 'doubling'). It is a form of automatic mixing involving one soundfile.Because of the way the program functions, this is, however, quite a different process from transposing and mixing sounds to produce chords. (Doing this in the Spectral Domain, where transposition can be done without affecting duration, still has a place.) Here, if the attack points are made to coincide, the new sound-object is an integral whole, rather than a 'chord' made up of the transposed stack components: that is, the components fuse and cannot be separated in perception, as they can with a chord played, for example, on a piano keyboard.Also note that, as a Time Domain process, the higher transpositions end sooner than the lower ones. Use Spectral Domain transpositon along with SUBMIX MIX to assemble transposed versions of the same sound when equal length is important.Very low transpositions could produce an extremely long output. The dur parameter enables you to specify a much shorter outfile length. You can usually get an idea of the harmonic content of the output from the first few seconds." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transpos", min = 0, max = 12, input = "brk", def = 2, tip = "transpos is the transposition in semitones between successive copies" },
  arg4 = { name = "count", min = 2, max = 32, def = 10, tip = "the number of copies in the stack" },
  arg5 = { name = "lean", min = 0, max = 10, tip = "the loudness of the highest component, relative to the lowest (may be > 1)" },
  arg6 = { name = "attack-offset", min = 0, max = length/1000, def = 0, tip = "adjusts the time at which the attack of each sound occurs" },
  arg7 = { name = "gain", min = 0.1, max = 10, def = 1, tip = "an overall amplifying gain factor on the output sound; some reduction (less than 1) may be needed. (It may also be > 1.) Range: 0.1 to 10" },
  arg8 = { name = "dur", min = 0, max = 1, tip = "how much of the output to make (a proportion, from 0 to 1)" },
  arg9 = { name = "-s", switch = "-s", tip = " see the relative levels of the layers in the stack" },
}

--[[
dsp["Modify Shudder - Shudder a stereo file  "] = { 
  cmds = { exe = "modify", mode = "shudder", channels = "2in2out", tip = "MODIFY SHUDDER imposes tremulations on a stereo file, randomised both in time and space, so that it appears to shudder. It is used in the shuddering noise band which then 'speaks' in Trevor Wishart's Imago. Note the relationship between frq and width: the frequency per second of the shudders and the width of soundfile segment that is affected. Thus there could be one shudder per second involving 500ms (0.5 sec) of sound, or one shudder per second involving 100ms (0.1 sec), etc. Depth and width MIN and MAX can be the same value. Musical applications; Although not unlike tremolo and vibrato, these 'shudders' are designed to be more gestural, more dramatic in character. Suggestion: experiment with larger values for count (e.g., 10 or more), high values for (amplitude) depth (e.g., 0.9 - 1.0), and smaller values for width (e.g., 0.07 - 0.1). Then try slower shudders, such as count = 2 and width (both MIN and MAX) = 0.5." }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "starttime", min = 0, max = length / 1000, def = 0, tip = "time when the shuddering will begin" },
  arg4 = { name = "frq", min = 0.1, max = 100, def = 10, tip = "the average frequency of the shuddering" },
  arg5 = { name = "scatter", min = 0, max = 1, def = 0.5, tip = "randomises the shudder events in time. Range: 0 to 1" },
  arg6 = { name = "stereo_spread", min = 0, max = 1, def = 0.5, tip = "positions the shudder events in space. Range: 0 to 1" },
  arg7 = { name = "mindepth", min = 0, max = 1, def = 0, tip = "amplitude of the shudders – each gets a random value between MIN and MAX" },
  arg8 = { name = "maxdepth", min = 0, max = 1, def = 1, tip = "amplitude of the shudders – each gets a random value between MIN and MAX" },
  arg9 = { name = "minwidth", min = 0.02, max = length/1000, def = 0.02, tip = "the duration of the shudder events in milliseconds – each gets a random value between MIN and MAX" },
  arg10 = { name = "maxwidth", min = 0.02, max = length/1000, tip = "the duration of the shudder events in milliseconds – each gets a random value between MIN and MAX" },
}
--]]

dsp["Modify Loudness 7 - Find loudest: find loudest of 2 files "] = { 
  cmds = { exe = "modify", mode = "loudness 7", tip = "FIND LOUDEST: find loudest of 2 files" },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
}

--[[ Obsolete process
dsp["Modify Dshift - Adds Doppler shift to panning "] = { 
  cmds = { exe = "modify", mode = "dshift", input = "data", tip = "The Doppler effect is the phenomenon of pitch change that we hear when something emitting a loud noise, such as a horn or siren, goes quickly past us. This program was written by Rajmil Fischman. It has been implemented in Soundshaper, but not yet in Sound Loom, either on the PC or on the MAC. It can be run from the Command Line on the PC, but has not yet been ported to the MAC. Please note that as a standalone program, on the Command Line you would put just 'dshift ... '. There are 3 main steps to creating this effect with DSHIFT: We start with a pan breakpoint file and use it with PAN to create a sound that moves between the speakers in a stereo field. Then we use DSHIFT to use the pan breakpoint file again as a guide to creating the Doppler pitch change effect. The output of DSHIFT is a pitch transposition file in semitones. The pitch transposition file is used with MODIFY SPEED and the panned soundfile as inputs, to create the final, Doppler-shifted, soundfile. To operate DSHIFT in Soundshaper: Pan a mono sound as normal, using a breakpoint pan file. The mono sound input will be the primary opened soundfile. The resulting panned soundfile is the output of this process. Select FILE2 and load the pan breakpoint file. Run DSHIFT from the Data Menu: Doppler Shift Transposition. Name the output breakpoint transposition file (in semitones); the default name is 'dshift.brk'. Deselect FILE2 (the pan file) and run SPEED Mode 2 (semitones), using dshift.brk (or your own name) as the time-varying transposition file, and the panned soundfile you made in Step 1 as the soundfile input. The output of SPEED is therefore a panned soundfile that also changes in pitch. Musical applications; This is a 'real life' acoustic effect that can be used to enhance literal recreations of a natural environment. The case that most easily comes to mind is that of a vehicle passing by, especially if it is blasting a horn or siren as it moves. More abstract sounds can be used, with a 'real life' effect adding verisimilitude to a sound coming from a strange or unfamiliar source." },  
   arg1 = { name = "N", switch = "-d", min = 0, max = 10000, def = 50, tip = "a flag to set the time it takes for a sound to change direction. N is the time in milliseconds. Default: 10ms. This flag is useful in order to avoid clicks." },
   arg2 = { name = "Input.txt", input = "txt", tip = "input PAN breakpoint file to use when adding Doppler effect" },
   arg3 = { name = "Output.txt", output = "txt", tip = "output transposition file (in semitones) to use with MODIFY SPEED Mode 2 (on the soundfile panned with the PAN breakpoint file) to create the Doppler effect" },
}
--]]

dsp["Modify Findpan - Find stereo-pan position of a sound in a stereo file "] = { 
  cmds = { exe = "modify", mode = "findpan", channels = "2in2out", tip = "If you are carefully positioning sounds in stereo-space it might be useful to know exactly where on the stereo-stage a panning sound is at any particular time. This process assumes the input is an (originally mono) sound which you then panned so that it moves around the stereo space. The process uses the file loudness data on the two channels to calculate where in the stereo space the sound appears to be. It assumes that the infile contains a sound that has previously been panned to a position in the stereo field. If this is not the case, the results will be misleading. For example, it could be a stereo file which is not in fact moving across the spatial field. It is important that the input soundfile was originally mono. If it had always been a stereo file, you could not find out where anything is in the stereo stage by comparing the left and right channels – they could, for example, carry completely different sounds. If the file were originally mono and then panned, the left and right channels will have the same signal, but at different levels. Thus, by comparing those levels, you can tell where the sound is on the stereo stage. Musical applications; Suppose you have made a sound that moves about in space, such as across the horizontal field from left to right. Now you want to place another sound such that it is in exactly the same spatial location as the moving sound at a specific time point. The question is: where is that spatial location? MODIFY FINDPAN asks you to supply a stereo input soundfile and a time. It then returns the pan location of the input soundfile at this time-point. You then use this information to specify the placement of the sound you want to add to the mix." },  
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "time", min = 0, max = length/1000, tip = " the time-point in the sound at which you wish to query the pan position" },
}

dsp["Modify Speed - 3 Get information on varying speed in a time-changing manner"] = { 
  cmds = { exe = "modify", mode = "speed 3", tip ="3 Get information on varying speed in a time-changing manner" },
  arg1 = { name = "Input.txt", input = "txt" },
  arg2 = { name = "-o", switch = "-o", tip = "breakpoint times are read as times in the outfile. The Default is to read them as times in the infile" },
}

dsp["Modify Speed - 4 Get information on time-variable speed change in semitones"] = { 
  cmds = { exe = "modify", mode = "speed 4", input = "data", tip ="4 Get information on time-variable speed change in semitones" },
  arg1 = { name = "Input.txt", input = "txt" },
  arg2 = { name = "-o", switch = "-o", tip = "breakpoint times are read as times in the outfile. The Default is to read them as times in the infile" },
}
 
------------------------------------------------
-- morph
------------------------------------------------
 
dsp["Morph Bridge 1 - make a bridging-interpolation between two sound spectra - Output level is the direct result of interpolation"] = {
  cmds = { exe = "morph", mode = "bridge 1", tip = "1 Output level is the direct result of interpolation. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}
 
dsp["Morph Bridge 2 - make a bridging-interpolation between two sound spectra - Output level follows moment to moment minimum of the 2 infile amplitudes"] = {
  cmds = { exe = "morph", mode = "bridge 2", tip = "2 Output level follows moment to moment minimum of the 2 infile amplitudes. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}
 
dsp["Morph Bridge 3 - make a bridging-interpolation between two sound spectra - Output level follows moment to moment amplitude of infile1"] = {
  cmds = { exe = "morph", mode = "bridge 3", tip = "3 Output level follows moment to moment amplitude of infile1. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}
 
dsp["Morph Bridge 4 - make a bridging-interpolation between two sound spectra - Output level follows moment to moment amplitude of infile2"] = {
  cmds = { exe = "morph", mode = "bridge 4", tip = "4 Output level follows moment to moment amplitude of infile2. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}
 
dsp["Morph Bridge 5 - make a bridging-interpolation between two sound spectra - Output level moves, through interpolation, from that of infile1 to that of infile2"] = {
  cmds = { exe = "morph", mode = "bridge 5", tip = "5 Output level moves, through interpolation, from that of infile1 to that of infile2. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}
 
dsp["Morph Bridge 6 - make a bridging-interpolation between two sound spectra - Output level moves, through interpolation, from that of infile2 to that of infile1"] = {
  cmds = { exe = "morph", mode = "bridge 6", tip = "6 Output level moves, through interpolation, from that of infile2 to that of infile1. Formerly SPECINTE, this process interpolates between two existing soundfiles. Unlike MORPH MORPH (formerly VOCINTE), this process interpolates between the fixed state of one sound at a specific time in that soundfile, and the fixed state of the 2nd sound at a specific time in that 2nd soundfile. The interpolation only works smoothly with sounds of quite stable spectrum. You can get an idea of what this means by understanding how the vibrato on a sound can be lost during a BRIDGE process. When a window (fixed state) is taken from the first soundfile, the data is as it were frozen at that point. Then an interpolation takes place simply between the data in the two windows, the two 'frozen' states in each file, not on the data in the first file after the window taken, or on the data in the second file before the window taken. Thus, if there were some vibrato on the first sound after the point at which the window is taken, it is lost. For more unstable, or noisier sounds, use MORPH MORPH, because this actually works on the data within the time period selected. Note that there is one time period which applies to both files. During this time period, a transition is made from the 1st to the 2nd sound. The 6 Modes provide a way to weight the process in favour of the salient features of the soundfiles, and which one you choose to predominate. The process also allows a further degree of weighting of the amount of the 2nd sound's frequency or amplitude interpolated at the start and end of the specified time. Musical applications; The application here is to move easily between two different sounds, each of which has a steady spectrum. The stability within each sound supports this kind of movement from one sound to the other. Perhaps it can be likened to the kind of smooth aural displacement that takes place when the harmony moves between triads with two notes in common (such as between G-Major and E-minor, E-minor and A-minor, etc.). The MORPH BRIDGE process is designed to handle movement between sounds which are more (but not too) dissimilar. It takes a great deal of delicate handling to achieve these types of smooth movement between sounds: there is a need to trick the ear at the point of transition ­ analagous to what needs to happen with any harmonic modulation. But in this case, the psycho-acoustics of the process are complex and still very little understood. The various amplitude following options, for example used with sounds with large variations of amplitude, can be used to combine sounds in remarkable ways. " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "start", switch = "-a", min = 0, max = length/1000, def = 0, tip = " time of startwindow for interpolation, in secs: Default: 0.0" },
  arg5 = { name = "end", switch = "-b", min = 0, max = 1, def = 1, tip = " time of endwindow for interpolation, in secs" },
  arg6 = { name = "sf2", switch = "-c", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's frq interpolated at start; (Range: 0 to 1, Default 0)" },
  arg7 = { name = "ef2", switch = "-d", min = 0, max = 1, def = 1, tip = "  fraction of 2nd sound's frq interpolated at end (Default 1)" },
  arg8 = { name = "sa2", switch = "-e", min = 0, max = 1, def = 0, tip = " fraction of 2nd sound's amp interpolated at start (Default 0)" },
  arg9 = { name = "ea2", switch = "-f", min = 0, max = 1, def = 1, tip = "ea2 fraction of 2nd sound's amp interpolated at end (Default 1)" },
}

dsp["Morph Glide - interpolate, linearly, between 2 single analysis windows which have been extracted with SPEC GRAB"] = { 
  cmds = { exe = "morph", mode = "glide", tip = "We can think of the two grabbed windows as tables of spectral data, a little snapshot cropped from the main source file. The contents will be the frequency and amplitude information for each of the e.g. 1024 channels (whichever value was entered for the number of channels). There will be different spectral data in each of these window-tables.  The data in infile1 becomes the start value, channel by channel, and the data in infile2 becomes the end value, channel by channel. GLIDE creates a linear interpolation between the start and end values, channel by channel, over the duration specified. Note what a linear interpolation is. Think of a straight line drawn between the start and end values. The number of values and the difference between those values will depend on the duration specified. So we can invoke our visual imagination to create a picture for these two example situations: 1. Widely differing values over a short duration will result in dramatically rising or falling lines.2. Scarcely differing values over a long duration will result in very subtle, gradual changes. Because the interpolation is linear ­ that is, proceding in equal steps ­ the resulting sound is likely to comprise more or less gradual transitions, which is why GLIDE has been placed in the MORPH category. Musical applications; Whether rapid and possibly somewhat dramatic, or slow and ever so gradual, MORPH GLIDE provides a way to move between two sounds in a smooth way. But note that the transition is never likely to be as inaudibly smooth as a full spectral interpolation over time as with MORPH MORPH itself. With MORPH GLIDE we should hear and feel the sound being redirected from one state to another. The extremes of the effects possible can therefore be described, perhaps, as a forcible turning on the one hand, and a hardly noticeable translation on the other. When two timbrally very different windows are grabbed, GLIDE can produce transitions with wonderful timbral modulations." },
  arg1 = { name = "Input 1", input = "data", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "data", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "duration", min = 0, max = 30, def = 2, tip = "duration of output sound required" },
}

dsp["Morph Morph 1 - morph one spectrum into another"] = {
  cmds = { exe = "morph", mode = "morph 1", tip = "MORPH MORPH is a program which can be used to create smooth aural transitions between different (but fairly similar) sounds. It does this by interpolating between the values in two analysis files generated by PV to analyze the source soundfiles and re-synthesize the outfile of MORPH MORPH. The resulting sound will be a more or less successful interpolation between the two original source sounds. It is important to understand that the interpolation takes place between the values in the two files while they continue to evolve in time. MORPH interpolates each successive window in the first file with its corresponding (simultaneous) window in the second file, so that the two sounds are continuing to unfold their original morphology during the process of interpolation.  Musical applications; We are now familiar with the practice of gradual shape transformations between visual images. The process has a wonderful surprise value as the first image seems to alter before our very eyes, becoming another quite different image, such as a face becoming that of another person. You may also possibly have observed that just at the moment of transition, the first image begins to melt or distort in some way. I say 'possibly' because it tends to happen very quickly, and because our eyes/brain tend to construct a familiar image, it's rather difficult to notice what happens inbetween. However, it is very instructive to do so. If it didn't change in some way before the second image made its presence felt, there would in fact not be a transition, just an abrupt change. This is the principle of 'modulation' in action. ­ which is precisely where we need to turn our attention when seeking to carry out an audio-morph. Modulation in music has been used up till now to move from one tonal region to another, and you will recall how it is common practice to begin by dissolving the identity of the existing tonal region, such as by using tonally ambigous diminished 7th harmonies. The idea of creating surprise and astonishment by causing one sound to become another is in itself easy to grasp. What's not so easy is making this smooth to the point of being almost imperceptible. Herein lies the art of the morph, and once again the concept of modulation is relevant. ('Morph', by the way, is a Greek word meaning 'form'.) This program gives a taste of the morphing process. The CDP spectral domain programs provide many other processes that can be used to 'pre-process' sounds to achieve a more psycho-acoustically effective transition. Here, the best results will be achieved by using sounds which are already quite similar.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "Amp Start", min = 0, max = length/1000, tip = "time in seconds when the amplitude interpolation begins" },
  arg5 = { name = "Amp End", min = 0, max = length/1000, tip = "time in seconds when the amplitude interpolation ends" },
  arg6 = { name = "Freq Start", min = 0, max = length/1000, tip = "time in seconds when the frequency interpolation begins" },
  arg7 = { name = "Freq End", min = 0, max = length/1000, tip = "time in seconds when the frequency interpolation ends" },
  arg8 = { name = "Amp Exp", min = 0.02, max = 50, tip = "exponent of the amplitude interpolation" },
  arg9 = { name = "Freq Exp", min = 0.02, max = 50, tip = "exponent of the frequency interpolation" },
  arg10 = { name = "Stagger", switch = "-s", min = 0, max = length/1000, def = 0, tip = "Time-delay of entry of 2nd file (default is 0.0)" },
}
 
dsp["Morph Morph 2 - morph one spectrum into another - interpolate over a cosinusoidal spline "] = {
  cmds = { exe = "morph", mode = "morph 2", tip = " interpolate over a cosinusoidal spline - MORPH MORPH is a program which can be used to create smooth aural transitions between different (but fairly similar) sounds. It does this by interpolating between the values in two analysis files generated by PV to analyze the source soundfiles and re-synthesize the outfile of MORPH MORPH. The resulting sound will be a more or less successful interpolation between the two original source sounds. It is important to understand that the interpolation takes place between the values in the two files while they continue to evolve in time. MORPH interpolates each successive window in the first file with its corresponding (simultaneous) window in the second file, so that the two sounds are continuing to unfold their original morphology during the process of interpolation.  Musical applications; We are now familiar with the practice of gradual shape transformations between visual images. The process has a wonderful surprise value as the first image seems to alter before our very eyes, becoming another quite different image, such as a face becoming that of another person. You may also possibly have observed that just at the moment of transition, the first image begins to melt or distort in some way. I say 'possibly' because it tends to happen very quickly, and because our eyes/brain tend to construct a familiar image, it's rather difficult to notice what happens inbetween. However, it is very instructive to do so. If it didn't change in some way before the second image made its presence felt, there would in fact not be a transition, just an abrupt change. This is the principle of 'modulation' in action. ­ which is precisely where we need to turn our attention when seeking to carry out an audio-morph. Modulation in music has been used up till now to move from one tonal region to another, and you will recall how it is common practice to begin by dissolving the identity of the existing tonal region, such as by using tonally ambigous diminished 7th harmonies. The idea of creating surprise and astonishment by causing one sound to become another is in itself easy to grasp. What's not so easy is making this smooth to the point of being almost imperceptible. Herein lies the art of the morph, and once again the concept of modulation is relevant. ('Morph', by the way, is a Greek word meaning 'form'.) This program gives a taste of the morphing process. The CDP spectral domain programs provide many other processes that can be used to 'pre-process' sounds to achieve a more psycho-acoustically effective transition. Here, the best results will be achieved by using sounds which are already quite similar.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "Amp Start", min = 0, max = length/1000, tip = "time in seconds when the amplitude interpolation begins" },
  arg5 = { name = "Amp End", min = 0, max = length/1000, tip = "time in seconds when the amplitude interpolation ends" },
  arg6 = { name = "Freq Start", min = 0, max = length/1000, tip = "time in seconds when the frequency interpolation begins" },
  arg7 = { name = "Freq End", min = 0, max = length/1000, tip = "time in seconds when the frequency interpolation ends" },
  arg8 = { name = "Amp Exp", min = 0.02, max = 50, tip = "exponent of the amplitude interpolation" },
  arg9 = { name = "Freq Exp", min = 0.02, max = 50, tip = "exponent of the frequency interpolation" },
  arg10 = { name = "Stagger", switch = "-s", min = 0, max = length/1000, def = 0, tip = "Time-delay of entry of 2nd file (default is 0.0)" },
}
 
------------------------------------------------
-- mton
------------------------------------------------
 
dsp["Mton Mton - Convert a mono sound to a stereo sound with identical signal in all channels "] = {
  cmds = { exe = "mton", mode = "mton", tip = "MTON is the multi-channel equivalent of HOUSEKEEP CHANS Mode 5 (convert mono to stereo). The output is a multi-channel file where all channels are identical. Musical Applications; A mono source can be generated in multi-channel format, then possibly modifed (e.g., with FLUTTER) and added to a multi-channel mix, providing an all-encompassing ambience." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outchans", min = 2, max = 2, def = 2, tip = "the number of channels in the output soundfile" },
}
 
------------------------------------------------
-- newdelay
------------------------------------------------
 
dsp["Newdelay - Delay with pitch-defined output sound"] = {
  cmds = { exe = "newdelay", mode = "newdelay", channels = "any", tip = "CDP already has a time-variable delay in MODIFY REVECHO, Mode 2. This program specifies the delay time as a MIDI (or pseudo-MIDI) pitch value. In a delay line, with sufficient feedback, very short delays of typically less than 40ms (25Hz) create a pitched resonance. Midipitch allows you to set this delay time according to the required pitch. However, discrete echoes can also be expressed as a pseudo-MIDI value (e.g., -12 produces delays of about 1/4 sec.), thereby making an important connection between pitch and rhythm." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "midipitch", min = -76, max = 136, input = "brk", def = 12, tip = "pitch of the output, expressed as a MIDI Pitch Value. MIDI Range: -76 to +136." },
  arg4 = { name = "mix", min = 0, max = 1, def = 0.5, tip = "the amount of delayed signal in the final mix. 0 gives a 'dry' result, 1 a 'wet' result" },
  arg5 = { name = "feedback", min = -1, max = 1, def = 0.5, tip = "produces resonance related to delay time (with short times)" },
}
 
------------------------------------------------
-- newmorph
------------------------------------------------
 
dsp["Newmorph 1 - Morph between dissimilar spectra - Interpolate linearly"] = {
  cmds = { exe = "newmorph", mode = "newmorph 1", tip = "1 – Interpolate linearly (exponent = 1) between the average peak channels or over a curve of increasing (exponent > 1) or decreasing (exp <1) slope, simultaneously moving spectral peaks, and interpolating all remaining channels. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}
 
dsp["Newmorph 2 - Morph between dissimilar spectra - Interpolate cosinusoidally"] = {
  cmds = { exe = "newmorph", mode = "newmorph 2", tip = "2 – Interpolate cosinusoidally (exponent = 1) between the average peak channels or over a warped cosinusoidal spline (exponent not equal to 1), simultaneously moving spectral peaks, and interpolating all remaining channels. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}
 
dsp["Newmorph 3 - Morph between dissimilar spectra - As mode 1, using channel-by-channel calculation of peaks."] = {
  cmds = { exe = "newmorph", mode = "newmorph 3", tip = "3 – As mode 1, using channel-by-channel calculation of peaks. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}
 
dsp["Newmorph 4 - Morph between dissimilar spectra - As mode 2, using channel-by-channel calculation of peaks."] = {
  cmds = { exe = "newmorph", mode = "newmorph 4", tip = "4 – As mode 2, using channel-by-channel calculation of peaks. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}
 
dsp["Newmorph 5 - Morph between dissimilar spectra - Sound 1 is (gradually) tuned to the (averaged) harmonic field sound 2 linearly."] = {
  cmds = { exe = "newmorph", mode = "newmorph 5", tip = "5 – Sound 1 is (gradually) tuned to the (averaged) harmonic field sound 2 linearly. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}
 
dsp["Newmorph 6 - Morph between dissimilar spectra - Sound 1 is (gradually) tuned to the (averaged) harmonic field sound 2 cosinusoidally"] = {
  cmds = { exe = "newmorph", mode = "newmorph 6", tip = "6 – Sound 1 is (gradually) tuned to the (averaged) harmonic field sound 2 cosinusoidally. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", min = 0, max = length/1000, def = 0, tip = "time before the entry of analysisfile2" },
  arg5 = { name = "startmorph", min = 0, max = length/1000, def = 0, tip = "time in the 1st file at which the morph starts" },
  arg6 = { name = "endmorph", min = 0, max = length/1000, tip = "time in the 1st file at which the morph ends" },
  arg7 = { name = "exponent", min = 0.02, max = 50, tip = "exponent of interpolation (Range: 0.02 to 50)" },
  arg8 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg9 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg10 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" },
  arg11 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },
}

--[[

dsp["[Doesn't work - Multiple Output files] Newmorph 7 - Morph between dissimilar spectra - Sound 1 is morphed towards sound2 in outcnt steps, each step a new output file. - multiple sound results"] = { 
  cmds = { exe = "newmorph", mode = "newmorph 7", input = "2pvoc", tip = "7 – Sound 1 is morphed towards sound2 in outcnt steps, each step a new output file. NEWMORPH is similar to MORPH MORPH in that there are two input analysis files and there is a gradual movement from the first to the second, with start and end times for the morph and exponent and stagger parameters. Where it differs is that NEWMORPH's morph is focused on the peaks of the two spectra. This helps the program to handle dissimilar spectra, whereas the 'normal' morph works best with similar sounds. There are also a larger number of modes, adding channel-by-channel operation, tuning and morphing in steps.  " },
  arg1 = { name = "peaks", min = 1, max = 16, def = 1, tip = "number of peaks to interpolate (Range: 1 to 16)" },
  arg2 = { name = "outcnt", min = 0, max = 20, def = 1, tip = "makes outcnt distinct ouput analysis files" }, 
  arg3 = { name = "-e", switch = "-e", tip = "retain the loudness envelope of the first sound. In this case the outanalfile is terminated when the end of the first sound is reached." },
  arg4 = { name = "-n", switch = "-n", tip = "there is no interpolation of anything except the peaks" }, 
  arg5 = { name = "-f", switch = "-f", tip = "only frequency is determined by the peaks in analysisfile2 " },  
}

--]]

------------------------------------------------
-- newsynth
------------------------------------------------

dsp["Newsynth Synthesis - 1 Generates tones with any number of (possibly varying) partials specified in a textfile"] = { 
  cmds = { exe = "newsynth", mode = "synthesis 1", tip = "1 - Modes 1 and 2 generate a waveform based on a pre-defined and possibly time-varying spectrum. The partial number can be harmonic (e.g. 2) or inharmonic (e.g. 2.1), except for the fundamental, which must be 1. Mode 2 further allows a narrowing of each wave-cycle envelope (too high a value produces silence), which ultimately reduces to clicks, and a centring of this envelope's peak on the start, middle or end of each cycle. Mode 3 produces a multi-channel output, and its partial content changes by random transposition every rate seconds, within maxrange, with further parameters to control this change. Care should be taken, especially with middle to high fundamental frequencies, that no partial is specified which would be above the Nyquist limit (22050 Hz)." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "spectrum", input = "txt", tip = "textfile listing partial ratios and their relative levels, against time" },
  arg3 = { name = "srate", min = 0, max = 48000, def = 44100, tip = "the sample rate of the resultant synthesised sound" },
  arg4 = { name = "dur", min = 0, max = 20, def = 6, tip = "the duration of the resultant synthesised sound" },
  arg5 = { name = "frq",  min = 0, max = 44100, def = 440, input = "brk", tip = "the (possibly time-varying) fundamental frequency of the ouput" },
} 

dsp["Newsynth Synthesis - 2 Generates wave-packet streams with any number of (possibly varying) partials specified in a textfile"] = { 
  cmds = { exe = "newsynth", mode = "synthesis 2", tip = "2 - Modes 1 and 2 generate a waveform based on a pre-defined and possibly time-varying spectrum. The partial number can be harmonic (e.g. 2) or inharmonic (e.g. 2.1), except for the fundamental, which must be 1. Mode 2 further allows a narrowing of each wave-cycle envelope (too high a value produces silence), which ultimately reduces to clicks, and a centring of this envelope's peak on the start, middle or end of each cycle. Mode 3 produces a multi-channel output, and its partial content changes by random transposition every rate seconds, within maxrange, with further parameters to control this change. Care should be taken, especially with middle to high fundamental frequencies, that no partial is specified which would be above the Nyquist limit (22050 Hz)." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "spectrum", input = "txt", tip = "textfile listing partial ratios and their relative levels, against time" },
  arg3 = { name = "srate", min = 0, max = 48000, def = 44100, tip = "the sample rate of the resultant synthesised sound" },
  arg4 = { name = "dur", min = 0, max = 20, def = 6, tip = "the duration of the resultant synthesised sound" },
  arg5 = { name = "frq",  min = 0, max = 44100, def = 440, input = "brk", tip = "the (possibly time-varying) fundamental frequency of the ouput" },
  arg6 = { name = "narrowing", switch = "-n", min = 0, max = 1000, def = 1, tip = "narrowing of the packet envelope (Range: 0 to 1000)." }, 
  arg7 = { name = "centring", switch = "-c", min = -1, max = 1, def = 0, tip = "position of the centre of the packet envelope's peak" },  
} 

dsp["Newsynth Synthesis - 3 – Multi-channel mode in which partials spread over N octaves fade in and out randomly"] = { 
  cmds = { exe = "newsynth", mode = "synthesis 3", tip = "3 - Multi-channel mode in which partials spread over N octaves fade in and out randomly. The partial number can be harmonic (e.g. 2) or inharmonic (e.g. 2.1), except for the fundamental, which must be 1. Mode 2 further allows a narrowing of each wave-cycle envelope (too high a value produces silence), which ultimately reduces to clicks, and a centring of this envelope's peak on the start, middle or end of each cycle. Mode 3 produces a multi-channel output, and its partial content changes by random transposition every rate seconds, within maxrange, with further parameters to control this change. Care should be taken, especially with middle to high fundamental frequencies, that no partial is specified which would be above the Nyquist limit (22050 Hz)." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "spectrum", input = "txt", tip = "textfile listing partial ratios and their relative levels, against time" },
  arg3 = { name = "srate", min = 0, max = 48000, def = 44100, tip = "the sample rate of the resultant synthesised sound" },
  arg4 = { name = "dur", min = 0, max = 20, def = 6, tip = "the duration of the resultant synthesised sound" },
  arg5 = { name = "frq",  min = 0.00100, max = 10000, def = 440, input = "brk", tip = "the (possibly time-varying) fundamental frequency of the ouput" },
  arg6 = { name = "chns", min = 2, max = 2, def = 2, tip = "the number of output channels" }, 
  arg7 = { name = "maxrange", min = 1, max = 8, def = 2, tip = " the maximum range of the transposition of spectral components: specify as number of whole octaves" },  
  arg8 = { name = "rate", min = 0.004000, max = 100, def = 2, tip = "the average time between changes to the partial content of the output" }, 
  arg9 = { name = "rise", switch = "-u", min = 0, max = 60, def = 1, tip = "time over which to expand to the maximum range" }, 
  arg10 = { name = "fall", switch = "-d", min = 0, max = 60, def = 1, tip = "time over which to return to the initial range, before the end is reached" }, 
  arg11 = { name = "steady", switch = "-f", min = 0, max = 60, def = 1, tip = "the duration of steady-stage sound at the end" }, 
  arg12 = { name = "splice", switch = "-s", min = 0, max = 1000, def = 5, tip = "splice-width in milliseconds for partial entry and exit" }, 
  arg13 = { name = "N", switch = "-n", min = 0, max = 1000, def = 128, tip = "N is the same fixed number of partials that will be chosen for each event" }, 
  arg14 = { name = "spacetype", switch = "-t", min = 0, max = 100, def = 1, tip = "the type of output spatialisation" }, 
  arg15 = { name = "rotspeed", switch = "-r", min = -20.0, max = 20, def = 10, tip = "the rotation speed (for certain spatialisation types)" }, 
  arg16 = { name = "-a", switch = "-a", tip = "activate an initial increase in the number of partials starting from only the fundamental" }, 
  arg17 = { name = "-z", switch = "-z", tip = "activate a decrease in the number of partials as the sound moves from steady-state back to the fundamental" }, 
  arg18 = { name = "-x", switch = "-x", tip = "(Xclusive): change all partials (as far as possible) from event to event" }, 
  arg19 = { name = "-m", switch = "-m", tip = "(Move): distribute partials in space" }, 
  arg20 = { name = "-j", switch = "-j", tip = "(Jump): all partials are assigned to the same location for any one event" }, 
  arg21 = { name = "from", switch = "-e", min = 1, max = 2, def = 2, tip = "from (Emerge): the sound emerges from channel from, used with.." }, 
  arg22 = { name = "time", switch = "-E", min = 0, max = 60, def = 2, tip = "(Emerge): the time from the beginning of the sound over which the sound emerges" }, 
  arg23 = { name = "to", switch = "-c", min = 1, max = 2, def = 1, tip = "(Converge): the sound converges to channel to, used with" }, 
  arg24 = { name = "time", switch = "-C", min = 0, max = 60, def = 3, tip = "(Converge): the time from the end over which the sound converges" }, 
} 

------------------------------------------------
-- newtex
------------------------------------------------
 
dsp["Newtex 1 - Create a texture of grains made from a source, transposition details from data text file"] = {
  cmds = { exe = "newtex", mode = "newtex 1", channels = "1in2out", tip = "1 - The transpositions of insndfile are spread over N octaves and spatially, and fade in and out randomly. NEWTEX is a powerhouse of a program that combines aspects of MODIFY BRASSAGE, EXTEND DRUNK and TEXTURE. It generates a (time-varying) texture from segments cut from a source sound or sounds." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transposes", input = "txt", tip = "textfile containing a list of transposition ratios and relative levels, against time. See docs for correct syntax." },
  arg4 = { name = "dur", min = 0, max = 60, def = 6, tip = "the duration of the output sound" },
  arg5 = { name = "chans", min = 2, max = 2, def = 2, tip = "the number of output channels" },
  arg6 = { name = "maxrange", min = 1, max = 8, input = "brk", def = 8, tip = "maxrange is the range in octaves of the transpositions of the input" },
  arg7 = { name = "step", min = 0, max = length/1000, input = "brk", tip = "the average time between changes to the stream-content of the output" },
  arg8 = { name = "spacetype", min = 0, max = 10, def = 1, tip = "the type of output spatialisation" },
  arg9 = { name = "splice", switch = "-s", min = 0, max = length, tip = "the splice-lengths in milliseconds for component entry and exit" },
  arg10 = { name = "number", switch = "-n", min = 0, max = 1000, input = "brk", def = 2, tip = "the number of components chosen for each event" },
  arg11 = { name = "-x", switch = "-x", tip = "'Xclusive': change all components, as far as possible, from event to event" },
  arg12 = { name = "rotspeed", switch = "-r", max = 20000, tip = "rotation speed for certain spatialisation types" },
  arg13 = { name = "-j", switch = "-j", tip = "'Jump': all components are assigned to the same location for any one event, and then it jumps to the next location" },
  arg14 = { name = "from", switch = "-e", min = 0, max = 1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg15 = { name = "time", switch = "-E", min = 0, max = length/1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg16 = { name = "to", switch = "-c", min = 0, max = 1000, tip = "'Converge': the sound converges from channel to over time time at end " },
  arg17 = { name = "time", switch = "-C", min = 0, max = length/1000, tip = "'Converge': the sound converges from channel to over time time at end " },
}
 
dsp["Newtex 2 - Create a texture of grains made from a source or sounds  "] = {
  cmds = { exe = "newtex", mode = "newtex 2", channels = "1in2out", tip = "2 - Insndfile is read at its original rate (i.e., no transpositions), spread spatially, and fades in and out randomly. NEWTEX is a powerhouse of a program that combines aspects of MODIFY BRASSAGE, EXTEND DRUNK and TEXTURE. It generates a (time-varying) texture from segments cut from a source sound or sounds." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "dur", min = 0, max = 30, def = 6, tip = "the duration of the output sound" },
  arg5 = { name = "chans", min = 2, max = 2, def = 2, tip = "the number of output channels" },
  arg6 = { name = "maxrange", min = 1, max = 8, input = "brk", def = 8, tip = "maxrange is the number of simultaneous soundings of any source soundfile." },
  arg7 = { name = "step", min = 0, max = length/1000, input = "brk", tip = "the average time between changes to the stream-content of the output" },
  arg8 = { name = "spacetype", min = 0, max = 0, def = 0, tip = "the type of output spatialisation - only zero applies to stereo files" },
  arg9 = { name = "delay", min = 0, max = length/1000, tip = "the time delay between identical components" },
  arg10 = { name = "splice", switch = "-s", min = 0, max = length, tip = "the splice-lengths in milliseconds for component entry and exit" },
  arg11 = { name = "number", switch = "-n", min = 0, max = 1000, input = "brk", def = 2, tip = "the number of components chosen for each event" },
  arg12 = { name = "-x", switch = "-x", tip = "'Xclusive': change all components, as far as possible, from event to event" },
  arg13 = { name = "rotspeed", switch = "-r", max = 20000, tip = "rotation speed for certain spatialisation types" },
  arg14 = { name = "-j", switch = "-j", tip = "'Jump': all components are assigned to the same location for any one event, and then it jumps to the next location" },
  arg15 = { name = "from", switch = "-e", min = 0, max = 1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg16 = { name = "time", switch = "-E", min = 0, max = length/1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg17 = { name = "to", switch = "-c", min = 0, max = 1000, tip = "'Converge': the sound converges from channel to over time time at end " },
  arg18 = { name = "time", switch = "-C", min = 0, max = length/1000, tip = "'Converge': the sound converges from channel to over time time at end " },
}
 
dsp["Newtex 3 - Create a texture of grains made from a source or sounds, transposition details from data text file"] = {
  cmds = { exe = "newtex", mode = "newtex 3", channels = "1in2out", tip = "3 Insndfile is read as 'drunken walks', spread spatially, and fades in and out randomly. NEWTEX is a powerhouse of a program that combines aspects of MODIFY BRASSAGE, EXTEND DRUNK and TEXTURE. It generates a (time-varying) texture from segments cut from a source sound or sounds." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "transposes", input = "txt", tip = "textfile containing a list of transposition ratios and relative levels, against time. See docs for correct syntax." },
  arg5 = { name = "dur", min = 0, max = 30, def = 6, tip = "the duration of the output sound" },
  arg6 = { name = "chans", min = 2, max = 2, def = 2, tip = "the number of output channels" },
  arg7 = { name = "maxrange", min = 1, max = 10000, input = "brk", def = 10, tip = "maxrange is the number of simultaneous soundings of any source soundfile." },
  arg8 = { name = "step", min = 0, max = length/1000, input = "brk", tip = "the average time between changes to the stream-content of the output" },
  arg9 = { name = "spacetype", min = 0, max = 0, def = 0, tip = "the type of output spatialisation, only zero works with stereo files" },
  arg10 = { name = "splice", switch = "-s", min = 0, max = length, tip = "the splice-lengths in milliseconds for component entry and exit" },
  arg11 = { name = "number", switch = "-n", min = 0, max = 1000, input = "brk", def = 2, tip = "the number of components chosen for each event" },
  arg12 = { name = "-x", switch = "-x", tip = "'Xclusive': change all components, as far as possible, from event to event" },
  arg13 = { name = "rotspeed", switch = "-r", max = 20000, tip = "rotation speed for certain spatialisation types" },
  arg14 = { name = "-j", switch = "-j", tip = "'Jump': all components are assigned to the same location for any one event, and then it jumps to the next location" },
  arg15 = { name = "from", switch = "-e", min = 0, max = 1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg16 = { name = "time", switch = "-E", min = 0, max = length/1000, tip = "'Emerge': the sound emerges from channel fromfrom' over time time at start" },
  arg17 = { name = "to", switch = "-c", min = 0, max = 1000, tip = "'Converge': the sound converges from channel to over time time at end " },
  arg18 = { name = "time", switch = "-C", min = 0, max = length/1000, tip = "'Converge': the sound converges from channel to over time time at end " },
}

------------------------------------------------
-- nmx
------------------------------------------------

--[[
dsp["[Doesn't work - multichannel format] Nmix - Mix two multi-channel files together, with optional offset for the second soundfile "] = { 
  cmds = { exe = "nmx", mode = "", input = "2audio", tip = "There is nothing much to say about this program. It mixes the two named soundfiles together. The channel counts (and speaker layouts, if any) must match. Note that it does not use a mixfile as input." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" }, 
  arg4 = { name = "-d", switch = "-d", tip = "apply TPDF dither (16-bit format output only)" },
  arg5 = { name = "-f", switch = "-f", tip = "set the output sample type to floats. The default sets the outfile sample type to that of infile1." },
  arg6 = { name = "-o", switch = "-o", min = 0, max = length/1000, def = 0, tip = "start infile2 at offset seconds" },
}
--]] 
 
------------------------------------------------
-- oneform
------------------------------------------------
 
dsp["Oneform Get - Extract formant-envelope at a specific time in an existing CDP formant file "] = {
  cmds = { exe = "oneform", mode = "get", tip = "Note that the starting point for these procedures is an existing CDP formant file, as created with the CDP program, FORMANTS GET. There are various important options to understand when extracting formants. These options are explained in a passage inside the FORMANTS reference manual. Here is a summary of the steps involved: Analyse the soundfile (PVOC ANAL Mode 1, using the default parameters 1024 (FFTsize) : 3 (FFToverlap) – see the Technical Glossary) Extract the formants, using an appropriate value for -p or -f (see above), Determine the time at which there is a formant that you want to extract (see below). Run ONEFORM GET, using this time. Run ONEFORM PUT, using the one-formant file and the analysis file, either in Mode 1 or Mode 2. Audition the resulting analysis file to check the result. Convert to a soundfile with PVOC SYNTH. The spectrum of a sound has an overall contour. In speech, the contour of the spectrum determines the vowel that you hear, but all sounds will have a spectral contour of some kind. Often (as in speech) this spectral contour will change from moment to moment. This is what allows us to communicate with speech on a monotone where the pitch remains static but the spectral contour changes as we articulate the different vowels in our spoken communication. The spectral colouration (i.e., 'timbre') changes (with the different vowels) although the pitch of the sound remains the same. The ONEFORM GET function allows the spectral colouration occuring at a specified time within a sound, to be extracted. This spectral colouration is referred to as the 'formant envelope'. What is extracted is the spectral complex making up the formant at this particular time-point – thus the expression 'single-moment-formant.' Note that there needs to be a clear formant structure in the sound at the time at which the formant is extracted. One way to find the right time at which a formant occurs is to use FORMANTS SEE to convert a (CDP) formants file into a pseudo soundfile for viewing. Then you can zoom in on the peaks to get an accurate time. In VIEWSF you can zoom in – about 32 seems useful to see where the formants are. A time in the soundfile where there is NOT a clear formant is likely to result in a poor result in ONEFORM, such as a buzzy sound. Here are some more suggestions about how to identify exactly where the formants are located. Listen to the the sound and chose a time clearly within a vocal vowel sound, or within a clearly pitched area of an instrumental sound – but note that there may be no clearly defined formants in instrumental sounds. In Sound Loom, go to CHOSEN FILES mode and get the analysis file. Then PROCESS > FORMANTS > get & view. Run this and save the resulting (pseudo-wav) file. On the Parameters Page for ONEFORM use the Sound View button to view and listen to the sound, or any part of it. When you think you have located a part of the sound with clear formants (e.g., a vocal vowel sound), you can mark the time on the graphic display with the mouse and output this (OUTPUT DATA) directly to the Parameters Page, where it will appear in the parameter box. (If the Sound View button is not there, it is because you have an earlier version of Sound Loom. In this case use the Sound View button on a different Parameter Page (e.g., PITCH:SPEED) to view the pseudo-wav file.) In general, on the Sound Loom, many processes have a Sound View option, which calls up a graphic display of the sound from which it can be played, and on which times – or blocks of time, depending on which program you're running and what kind of data it needs – can be marked. When the data is output from the graphics, the corresponding parameter values are set automatically. Where a process needs a file (e.g. a breakpoint file, a set of times as in ZIGZAG, or a list of time-pairs as in [SF]EDIT CUTMANY) then the Sound View button is found on the file-creation page you reach from Make File. Each Sound View is designed to output the appropriate data for the process you are running. It is probably important to note that many vowels in standard English are diphthongs, gliding between one vowel formation and another. For example in about, the sound for a is reasonably constant, but the ou is really a glide from a to oo. So, depending on where you sample inside the vowel, you will get a different result: an a, an oo or something in between. In Soundshaper (Pro), having analysed the soundfile, you can use the following processing chain without having to save any intermediate files: go to Spectral > Morph/Formants > Get Formants, then Spectral > Morph/Formants > See Formants, then Tools > VIEWSF and zoom to an appropriate level. Move the cursor to a formant area, or an approximate time in the soundfile that you have identified aurally as one you want to use, and note the exact time where there is a formant peak. A more direct route is to analyse the original soundfile and then go directly to Spectral > Morph/Formants > GetSee Formants, and then VIEWSF, but at the moment this display looks different and perhaps not as useful as the longer method just described. Vocal sounds have wide formant bands which are usually (not guaranteed to be always) detected by the formant extraction process. Synthetic tones have no formants, so the output of the formant extraction process will be junk. Instrumental tones may have extractable formants, but one may have to fiddle with the parameters to get a workable result. Musical applications; This function allows a formant-shape (the spectral content of a spoken vowel, or of any other sound) occuring at a specified time within a sound, to be extracted and used to shape the spectrum of another sound (using ONEFORM PUT). Thus, for example, the vowel sound of 'ey' in the recorded text here they are, could be extracted, and then used to shape another sound, imposing the vowel-colour of 'ey' on that other sound. The 'imposing' is done with ONEFORM PUT. For another approach to colouration, see HILITE VOWELS." },
  arg1 = { name = "Input.for", input = "for", },
  arg2 = { name = "Output.for", output = "for", },
  arg3 = { name = "-time", min = 0, max = 60, def = 1, tip = "the time in seconds in the informantfile where the single-moment-formants are to be extracted." },
}
 
dsp["Oneform Put - 1 Impose the formant-envelope in a single-moment-formants datafile onto the sound in an analysis file "] = {
  cmds = { exe = "oneform", mode = "put 1", tip = "1 The single-moment-formant is imposed on the analysis file – i.e., added to the existing formants. A fixed contour can be imposed on the spectrum of a sound, emphasising some frequency areas, and de-emphasizing (or even removing) others. This fixed spectral contour can be derived from a particular moment in another sound, using ONEFORM GET. ONEFORM PUT applies this contour to the whole sound, acting like a static graphic-equaliser on the whole sound. As with all filtering processes, it will only be effective if the target sound has something to be filtered. e.g., if you use the imposed formant-shape on a very narrow bandwidth signal, you may hear no effect (except a change in level). Broadband signals (noisy sounds, for example) will tend to give the best results. Musical applications; This function allows a formant-shape (the spectral shape of a spoken vowel, or of any other sound) extracted from some other sound (using ONEFORM GET) to be used to change the spectrum of another sound. Thus, for example, the vowel sound of 'ey' in the recorded text 'here they are', could be extracted, and then used to shape a target sound, imposing the vowel-colour of 'ey' on that other sound. The difference between the two modes is important. Because Mode 1 adds the single-moment to existing formants, the formant content becomes more complex, perhaps resulting in a buzzy sound. On the other hand, Mode 2 replaces existing formants with the single-moment formant, thus re-colouring the whole sound with that of the single-moment formant." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lolim", switch = "-l", min = 0, max = 22100, def = 50, tip = "low frequency limit: the spectrum is set to zero below this limit" },
  arg5 = { name = "hilim", switch = "-h", min = 0, max = 22100, def = 10000, tip = "high frequency limit: the spectrum is set to zero above this limit" },
  arg6 = { name = "gain", switch = "-g", min = 0, max = 100, def = 1, tip = "overall gain on the output" },
}
 
dsp["Oneform Put - 2 Impose the formant-envelope in a single-moment-formants datafile onto the sound in an analysis file "] = {
  cmds = { exe = "oneform", mode = "put 2", tip = "2 Attempts to replace the existing formants in the analysis file with the single-moment-formants. A fixed contour can be imposed on the spectrum of a sound, emphasising some frequency areas, and de-emphasizing (or even removing) others. This fixed spectral contour can be derived from a particular moment in another sound, using ONEFORM GET. ONEFORM PUT applies this contour to the whole sound, acting like a static graphic-equaliser on the whole sound. As with all filtering processes, it will only be effective if the target sound has something to be filtered. e.g., if you use the imposed formant-shape on a very narrow bandwidth signal, you may hear no effect (except a change in level). Broadband signals (noisy sounds, for example) will tend to give the best results. Musical applications; This function allows a formant-shape (the spectral shape of a spoken vowel, or of any other sound) extracted from some other sound (using ONEFORM GET) to be used to change the spectrum of another sound. Thus, for example, the vowel sound of 'ey' in the recorded text 'here they are', could be extracted, and then used to shape a target sound, imposing the vowel-colour of 'ey' on that other sound. The difference between the two modes is important. Because Mode 1 adds the single-moment to existing formants, the formant content becomes more complex, perhaps resulting in a buzzy sound. On the other hand, Mode 2 replaces existing formants with the single-moment formant, thus re-colouring the whole sound with that of the single-moment formant." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "lolim", switch = "-l", min = 0, max = 22100, def = 50, tip = "low frequency limit: the spectrum is set to zero below this limit" },
  arg5 = { name = "hilim", switch = "-h", min = 0, max = 22100, def = 10000, tip = "high frequency limit: the spectrum is set to zero above this limit" },
  arg6 = { name = "gain", switch = "-g", min = 0, max = 100, def = 1, tip = "overall gain on the output" },
}

dsp["Oneform Combine - Generate a new sound from pitch information and single-moment-formants data"] = { 
  cmds = { exe = "oneform", mode = "combine", tip = "This function takes information about the spectral contour (or formant-shape) of a sound at a specific time, extracted using ONEFORM GET, and combines it with pitch-trajectory information (a plot of changing pitch extracted using REPITCH GETPITCH) possibly derived from a different sound. The result will be a sound having the same pitch-trajectory, and using the 'colour' of the formant-shape. Musical applications; This process may be used to create pitched, or pitch-changing events, using formant-shapes (e.g. vowel sounds from speech) extracted from a particular source. It is complementary to the processes which grab and extend FOFs, but works with (and outputs) analysis files, and operates in a different way." },  
  arg1 = { name = "Input.frq", input = "frq", tip = "Select the data input to the process" },
  arg2 = { name = "Input.for", input = "for", tip = "Select the data input to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}

------------------------------------------------
-- packet
------------------------------------------------

dsp["[Doesn't work?] Packet 1 -  Isolate or generate a sound packet - Found packet: looks for signal minima to determine the edges of the wave-packet"] = {
cmds = { exe = "packet", mode = "packet 1", tip = "1 - Found packet: looks for signal minima to determine the edges of the wave-packet. In effect, PACKET cuts out a portion of soundfile, envelopes it, and places the amplitude peak at the beginning, centre or end of that envelope as specified by the user. Musical Applications; This process streamlines the task of creating a usable snippet of soundfile. Trevor suggests that the main use of such snippets will be in the TEXTURE set of programs. Recall that these programs always begin each sound unit from the beginning of the input soundfile, with the option to then use the whole of the input soundfile for each iteration (the -w flag). PACKET makes it easier to prepare a specific short snippet that can then be proliferated into a texture. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "times", min = 0, max = length/1000, def = 0, input = "brk", tip = "a single time or a textfile of times at which the packet or packets is/are extracted or created" },
  arg4 = { name = "mindur", min = 0, max = length, tip = "the minimum duration in milliseconds of the (found) packet; it must be less than half the source duration" },
  arg5 = { name = "narrowing", min = 0, max = 1000, tip = "narrows the packet envelope (Range: 0 to 1000)" },
  arg6 = { name = "centring", min = 0, max = 1, tip = "centres the peak of the packet envelope." },
  arg7 = { name = "-n", switch = "-n", tip = "normalise the packet level" },
  arg8 = { name = "-f", switch = "-f", tip = "the packet wave maxima and minima are forced up or down to the packet contour." },
  arg9 = { name = "-s", switch = "-s", tip = "shave off a leading or trailing silence" },
}

dsp["[Doesn't work?] Packet 2 -  Isolate or generate a sound packet - Found packet: looks for signal minima to determine the edges of the wave-packet"] = {
cmds = { exe = "packet", mode = "packet 2", tip = "2 - Forced packet: creates a packet at a specified time. In effect, PACKET cuts out a portion of soundfile, envelopes it, and places the amplitude peak at the beginning, centre or end of that envelope as specified by the user. Musical Applications; This process streamlines the task of creating a usable snippet of soundfile. Trevor suggests that the main use of such snippets will be in the TEXTURE set of programs. Recall that these programs always begin each sound unit from the beginning of the input soundfile, with the option to then use the whole of the input soundfile for each iteration (the -w flag). PACKET makes it easier to prepare a specific short snippet that can then be proliferated into a texture. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "times", min = 0, max = length/1000, def = 0, input = "brk", tip = "a single time or a textfile of times at which the packet or packets is/are extracted or created" },
  arg4 = { name = "dur", min = 0.0, max = length, tip = "the duration in milliseconds of the (forced) packet; it must be less than half the source duration" },
  arg5 = { name = "narrowing", min = 0.0, max = 1000, tip = "narrows the packet envelope (Range: 0 to 1000)" },
  arg6 = { name = "centring", min = 0, max = 1, tip = "centres the peak of the packet envelope." },
  arg7 = { name = "-n", switch = "-n", tip = "normalise the packet level" },
  arg8 = { name = "-f", switch = "-f", tip = "the packet wave maxima and minima are forced up or down to the packet contour." },
  arg9 = { name = "-s", switch = "-s", tip = "shave off a leading or trailing silence" },
}

------------------------------------------------
-- partition
------------------------------------------------

--[[ obsolete program, exe not longer supplied with cdp install, maybe works with older versions - has multiple outputs
dsp["Partition - 1 Partition a mono soundfile into disjunct files in blocks defined by groups of wavesets"] = { 
  cmds = { exe = "partition", mode = "partition 1", tip = "1 - block durations are determined by number of wavesets. The purpose of PARTITION is to facilitate your sound design activities. The program cuts ALL of the sound in a source soundfile into disjunct pieces, assigning each of them in turn to N output soundfiles and producing N streams of disjunct segments. For example, if 8 pieces are cut and 2 output files are specified, each output soundfile will get 4 pieces. The cut pieces, furthermore, are time-positioned in the output soundfiles at the same times that they occurred in the source. This is achieved by (automatically) inserting appropriate silences between the cut pieces in the output soundfiles. The result of this process is that the different segment-streams can then be treated differently. Having done so, the resulting sounds can be remixed, synchronised at time zero, and all the original segments (now treated) will be returned to their original locations in the source. Implied here is that treatments which alter time-location within the soundfile should be avoided. There are two modes. the blocks are defined by number of wavesets. This means that the duration of the resulting blocks will be set by the length of those wavesets. the blocks are defined by specifying a duration. This means that you have some control over the size of the blocks. Suppose there is an input soundfile which contains a series of sounds-blocks (determined by a waveset analysis – zero crossings?). This series of successive blocks can be labelled abcdefghijklmno.... We decide to create 3 output soundfiles (streams) with these sounds. What happens is that every third block is assigned to a different stream (the - represents a silence replacing a missing block): Outfile 1 contains: a--d--g--j--m--... Outfile 2 contains: -b--e--h--k--n-.. Outfile 3 contains: --c--f--i--l--o ..." },
  arg1 = { name = "outcnt", min = 1, max = 32, def = 2, tip = "the number of output soundfiles" },
  arg2 = { name = "groupcnt", min = 0, max = 1000, def = 20, tip = "the number of wavesets per block" },
}
--]]

--[[ obsolete program, exe not longer supplied with cdp install, maybe works with older versions - has multiple outputs
dsp["Partition - 2 Partition a mono soundfile into disjunct files in blocks defined by groups of wavesets"] = { 
  cmds = { exe = "partition", mode = "partition 2", tip = "2 – block durations are specified by the user. The purpose of PARTITION is to facilitate your sound design activities. The program cuts ALL of the sound in a source soundfile into disjunct pieces, assigning each of them in turn to N output soundfiles and producing N streams of disjunct segments. For example, if 8 pieces are cut and 2 output files are specified, each output soundfile will get 4 pieces. The cut pieces, furthermore, are time-positioned in the output soundfiles at the same times that they occurred in the source. This is achieved by (automatically) inserting appropriate silences between the cut pieces in the output soundfiles. The result of this process is that the different segment-streams can then be treated differently. Having done so, the resulting sounds can be remixed, synchronised at time zero, and all the original segments (now treated) will be returned to their original locations in the source. Implied here is that treatments which alter time-location within the soundfile should be avoided. There are two modes. the blocks are defined by number of wavesets. This means that the duration of the resulting blocks will be set by the length of those wavesets. the blocks are defined by specifying a duration. This means that you have some control over the size of the blocks. Suppose there is an input soundfile which contains a series of sounds-blocks (determined by a waveset analysis – zero crossings?). This series of successive blocks can be labelled abcdefghijklmno.... We decide to create 3 output soundfiles (streams) with these sounds. What happens is that every third block is assigned to a different stream (the - represents a silence replacing a missing block): Outfile 1 contains: a--d--g--j--m--... Outfile 2 contains: -b--e--h--k--n-.. Outfile 3 contains: --c--f--i--l--o ..." },
  arg1 = { name = "outcnt", min = 1, max = 32, def = 2, tip = "the number of output soundfiles" },
  arg2 = { name = "groupcnt", min = 0, max = 1000, def = 20, tip = "the number of wavesets per block" },
  arg3 = { name = "rand", switch = "-r", min = 0, max = 1, def = 0.5, tip = "the randomisation of durations (only) (Range: 0 to 1)" },
  arg4 = { name = "splice", switch = "-s", min = 0, max = 1000, def = 3, tip = "the splice length in milliseconds (Default: 3mS) " },
}
--]]

------------------------------------------------
-- peak
------------------------------------------------
 
dsp["Peak Extract - 1 List the spectral peaks as they vary in time - Extract spectral peaks from analysis file and write to a text file"] = {
  cmds = { exe = "peak", mode = "extract 1", tip = "1 List the spectral peaks as they vary in time. PEAK EXTRACT searches for the peaks in the spectrum in an analysis file. Unlike REPITCH GETPITCH, it is not searching for a specific pitch in each window, but merely whatever peaks it can find. It is therefore more appropriate for finding the pitch content of multipitched sources. The process is based on a method using median amplitudes, suggested by Bill Sethares (verbal communication at CCMIX in Paris) together with a best fit algorithm and semitone-bin assignment devised by T Wishart. Musical Applications; PEAK EXTRACT searches for the peaks in the spectrum in an analysis file, regardless of whether the spectrum is pitched or not. It is appropriate for finding the pitch foci of multi-pitched sources. The output data can be used in processes where pitch or frequency data is required. In particular, the FILTER VARIBANK output can be used to create filters which tune material to the spectrum found in the analysis file. Also see the entry on the FILTER VARIBANK file format in CDP File Formats & Codes." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "winsiz", min = 1, max = 96, def = 12, tip = "the process divides each analysis window into a number of (overlapping) subwindows, and winsize determines the width of this subwindow, in semitones. Smaller values reduce the number of peaks found. Range: 1 - 96. (Recommended 12)" },
  arg4 = { name = "peak", min = 1, max = 1000, def = 1.5, tip = "in each subwindow, the process finds the median amplitude of the channels. It then looks for those channels whose amplitude is peak times louder than the median and initially assumes these are spectral peaks. Range: 1 - 1000. (Recommended 1.5+)" },
  arg5 = { name = "floor", min = 0.0001, max = 1, def = 0.001, tip = "the peak amplitudes must also exceed a minimum floor amplitude before they are accepted as true peaks – floor sets this level. Range: 0.0001 - 1. (Recommended ca .001)" },
  arg6 = { name = "lo", min = 0, max = 44100, def = 50, tip = "determines the lowest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg7 = { name = "hi", min = 0, max = 44100, def = 6000, tip = "determines the highest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg8 = { name = "tune", switch = "-h", min = 0, max = 6, def = 1, tip = "If tune is non-zero, the process searches the peaks to discover if any of them are harmonics of other peaks. If they are, the harmonic-peaks are excluded. Peaks are judged to be harmonics if they are in tune with the expected frequency of a harmonic. Range: 0 - 6. (Recommended 1)." },
  arg9 = { name = "-a", switch = "-a", tip = "suppress amplitude information in the output" },
  arg10 = { name = "-m", switch = "-m", tip = "express frequency data as (posibly fractional) MIDI values" },
  arg11 = { name = "-q", switch = "-q", tip = "quantise frequency or midi output to the nearest quarter-tones of the tempered scale" },
  arg12 = { name = "-z", switch = "-z", tip = "show peakfree segments in the data, by printing zeros in the output" },
}
 
dsp["Peak Extract - 2 – Stream the spectral peaks, using the maximum number of peaks found - Extract spectral peaks from analysis file and write to a text file"] = {
  cmds = { exe = "peak", mode = "extract 2", tip = "2 – Stream the spectral peaks, using the maximum number of peaks found. PEAK EXTRACT searches for the peaks in the spectrum in an analysis file. Unlike REPITCH GETPITCH, it is not searching for a specific pitch in each window, but merely whatever peaks it can find. It is therefore more appropriate for finding the pitch content of multipitched sources. The process is based on a method using median amplitudes, suggested by Bill Sethares (verbal communication at CCMIX in Paris) together with a best fit algorithm and semitone-bin assignment devised by T Wishart. Musical Applications; PEAK EXTRACT searches for the peaks in the spectrum in an analysis file, regardless of whether the spectrum is pitched or not. It is appropriate for finding the pitch foci of multi-pitched sources. The output data can be used in processes where pitch or frequency data is required. In particular, the FILTER VARIBANK output can be used to create filters which tune material to the spectrum found in the analysis file. Also see the entry on the FILTER VARIBANK file format in CDP File Formats & Codes." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "winsiz", min = 1, max = 96, def = 12, tip = "the process divides each analysis window into a number of (overlapping) subwindows, and winsize determines the width of this subwindow, in semitones. Smaller values reduce the number of peaks found. Range: 1 - 96. (Recommended 12)" },
  arg4 = { name = "peak", min = 1, max = 1000, def = 1.5, tip = "in each subwindow, the process finds the median amplitude of the channels. It then looks for those channels whose amplitude is peak times louder than the median and initially assumes these are spectral peaks. Range: 1 - 1000. (Recommended 1.5+)" },
  arg5 = { name = "floor", min = 0.0001, max = 1, def = 0.001, tip = "the peak amplitudes must also exceed a minimum floor amplitude before they are accepted as true peaks – floor sets this level. Range: 0.0001 - 1. (Recommended ca .001)" },
  arg6 = { name = "lo", min = 0, max = 44100, def = 50, tip = "determines the lowest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg7 = { name = "hi", min = 0, max = 44100, def = 6000, tip = "determines the highest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg8 = { name = "tune", switch = "-h", min = 0, max = 6, def = 1, tip = "If tune is non-zero, the process searches the peaks to discover if any of them are harmonics of other peaks. If they are, the harmonic-peaks are excluded. Peaks are judged to be harmonics if they are in tune with the expected frequency of a harmonic. Range: 0 - 6. (Recommended 1)." },
  arg9 = { name = "-a", switch = "-a", tip = "suppress amplitude information in the output" },
  arg10 = { name = "-m", switch = "-m", tip = "express frequency data as (posibly fractional) MIDI values" },
  arg11 = { name = "-q", switch = "-q", tip = "quantise frequency or midi output to the nearest quarter-tones of the tempered scale" },
  arg12 = { name = "-z", switch = "-z", tip = "show peakfree segments in the data, by printing zeros in the output" },
  arg13 = { name = "-f", switch = "-f", tip = "format the output as a data file for the FILTER VARIBANK process. Also see Files & Codes for more information on this file format." },
}
 
dsp["Peak Extract - 3 Stream the spectral peaks, using the most prominent pitches found. - Extract spectral peaks from analysis file and write to a text file"] = {
  cmds = { exe = "peak", mode = "extract 3", tip = "3 – Stream the spectral peaks, using the most prominent pitches found. PEAK EXTRACT searches for the peaks in the spectrum in an analysis file. Unlike REPITCH GETPITCH, it is not searching for a specific pitch in each window, but merely whatever peaks it can find. It is therefore more appropriate for finding the pitch content of multipitched sources. The process is based on a method using median amplitudes, suggested by Bill Sethares (verbal communication at CCMIX in Paris) together with a best fit algorithm and semitone-bin assignment devised by T Wishart. Musical Applications; PEAK EXTRACT searches for the peaks in the spectrum in an analysis file, regardless of whether the spectrum is pitched or not. It is appropriate for finding the pitch foci of multi-pitched sources. The output data can be used in processes where pitch or frequency data is required. In particular, the FILTER VARIBANK output can be used to create filters which tune material to the spectrum found in the analysis file. Also see the entry on the FILTER VARIBANK file format in CDP File Formats & Codes." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "winsiz", min = 1, max = 96, def = 12, tip = "the process divides each analysis window into a number of (overlapping) subwindows, and winsize determines the width of this subwindow, in semitones. Smaller values reduce the number of peaks found. Range: 1 - 96. (Recommended 12)" },
  arg4 = { name = "peak", min = 1, max = 1000, def = 1.5, tip = "in each subwindow, the process finds the median amplitude of the channels. It then looks for those channels whose amplitude is peak times louder than the median and initially assumes these are spectral peaks. Range: 1 - 1000. (Recommended 1.5+)" },
  arg5 = { name = "floor", min = 0.0001, max = 1, def = 0.001, tip = "the peak amplitudes must also exceed a minimum floor amplitude before they are accepted as true peaks – floor sets this level. Range: 0.0001 - 1. (Recommended ca .001)" },
  arg6 = { name = "lo", min = 0, max = 44100, def = 50, tip = "determines the lowest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg7 = { name = "hi", min = 0, max = 44100, def = 6000, tip = "determines the highest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg8 = { name = "tune", switch = "-h", min = 0, max = 6, def = 1, tip = "If tune is non-zero, the process searches the peaks to discover if any of them are harmonics of other peaks. If they are, the harmonic-peaks are excluded. Peaks are judged to be harmonics if they are in tune with the expected frequency of a harmonic. Range: 0 - 6. (Recommended 1)." },
  arg9 = { name = "-a", switch = "-a", tip = "suppress amplitude information in the output" },
  arg10 = { name = "-m", switch = "-m", tip = "express frequency data as (posibly fractional) MIDI values" },
  arg11 = { name = "-q", switch = "-q", tip = "quantise frequency or midi output to the nearest quarter-tones of the tempered scale" },
  arg12 = { name = "-z", switch = "-z", tip = "show peakfree segments in the data, by printing zeros in the output" },
  arg13 = { name = "-f", switch = "-f", tip = "format the output as a data file for the FILTER VARIBANK process. Also see Files & Codes for more information on this file format." },
}
 
dsp["Peak Extract - 4 List the most prominent peaks found as fixed average values - Extract spectral peaks from analysis file and write to a text file"] = {
  cmds = { exe = "peak", mode = "extract 4", tip = "4 – List the most prominent peaks found as fixed average values. PEAK EXTRACT searches for the peaks in the spectrum in an analysis file. Unlike REPITCH GETPITCH, it is not searching for a specific pitch in each window, but merely whatever peaks it can find. It is therefore more appropriate for finding the pitch content of multipitched sources. The process is based on a method using median amplitudes, suggested by Bill Sethares (verbal communication at CCMIX in Paris) together with a best fit algorithm and semitone-bin assignment devised by T Wishart. Musical Applications; PEAK EXTRACT searches for the peaks in the spectrum in an analysis file, regardless of whether the spectrum is pitched or not. It is appropriate for finding the pitch foci of multi-pitched sources. The output data can be used in processes where pitch or frequency data is required. In particular, the FILTER VARIBANK output can be used to create filters which tune material to the spectrum found in the analysis file. Also see the entry on the FILTER VARIBANK file format in CDP File Formats & Codes." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "winsiz", min = 1, max = 96, def = 12, tip = "the process divides each analysis window into a number of (overlapping) subwindows, and winsize determines the width of this subwindow, in semitones. Smaller values reduce the number of peaks found. Range: 1 - 96. (Recommended 12)" },
  arg4 = { name = "peak", min = 1, max = 1000, def = 1.5, tip = "in each subwindow, the process finds the median amplitude of the channels. It then looks for those channels whose amplitude is peak times louder than the median and initially assumes these are spectral peaks. Range: 1 - 1000. (Recommended 1.5+)" },
  arg5 = { name = "floor", min = 0.0001, max = 1, def = 0.001, tip = "the peak amplitudes must also exceed a minimum floor amplitude before they are accepted as true peaks – floor sets this level. Range: 0.0001 - 1. (Recommended ca .001)" },
  arg6 = { name = "lo", min = 0, max = 44100, def = 50, tip = "determines the lowest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg7 = { name = "hi", min = 0, max = 44100, def = 6000, tip = "determines the highest frequency acceptable for a retained peak. Range: analysis channel width to nyquist." },
  arg8 = { name = "tune", switch = "-h", min = 0, max = 6, def = 1, tip = "If tune is non-zero, the process searches the peaks to discover if any of them are harmonics of other peaks. If they are, the harmonic-peaks are excluded. Peaks are judged to be harmonics if they are in tune with the expected frequency of a harmonic. Range: 0 - 6. (Recommended 1)." },
  arg9 = { name = "-a", switch = "-a", tip = "suppress amplitude information in the output" },
  arg10 = { name = "-m", switch = "-m", tip = "express frequency data as (posibly fractional) MIDI values" },
  arg11 = { name = "-q", switch = "-q", tip = "quantise frequency or midi output to the nearest quarter-tones of the tempered scale" },
  arg12 = { name = "-z", switch = "-z", tip = "show peakfree segments in the data, by printing zeros in the output" },
  arg13 = { name = "-f", switch = "-f", tip = "format the output as a data file for the FILTER VARIBANK process. Also see Files & Codes for more information on this file format." },
}
 
------------------------------------------------
-- peakfind
------------------------------------------------
 
dsp["Peakfind - Find times of loudness peaks in a sound and output as a datafile "] = {
  cmds = { exe = "peakfind", mode = "peakfind", tip = "This process was originally developed to track the syllables in speech. It outputs a textfile, with a list of the times of the peaks found. Only the times of the peaks are shown. A very large windowsize will miss detail in the sound. A very small windowsize will confuse the waveform's amplitude variation with the loudness envelope and produce an excessive number of 'peak' times. In the example command line above, the input soundfile, count.wav is 8.065986 seconds long and consists of a spoken count from 1 to 10. A fairly large windowsize in order to focus on the the main peaks in each counted number. This was the output: 0.165465, 1.066667, 2.001655 et cetera. The default of the threshold parameter is to ignore peaks whose level is less than one-fifth of the local maximum level. The local maximum is measured over 10 windows. Trevor Wishart explains: 'In a complex signal there can be lots of 'peaks' i.e., localised moments where the signal is slightly louder than the preceding and following windows, but we're really interested in events which stand out in loudness, and are therefore heard as rhythmic 'accents' or 'downbeats' in the event. The algorithm therefore picks out the local maximum peak, then ignores local fluctuations in level (producing localised maxima) which are close by and much lower in level than this peak. If the threshold is > 0.0, the algorithm works in the same way, but if any of the the local peaks it finds lie below some absolute level (i.e., the threshold which you have set), they are ignored.'  I may have focused even more on the peaks in the numbers with windowsize = 100 and threshold = 0.4. This is the output: 0.165465, 1.066667, 2.001655 et cetera. Musical Applications; PEAKFIND is useful for obtaining rhythmic information about musical materials. The data produced can be analysed, for example to find tempo, in the TABLE EDITOR in Sound Loom." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "windowsize", min = 1, max = 500, def = 3, tip = "size, in milliseconds, of the window used for locating succesive peaks (Range: 1 to 500)" },
  arg4 = { name = "threshold", switch = "-t", min = 0, max = 1, def = 0.5, tip = "the level below which peaks are ignored in any window (Range 0-1)." },
}
 
------------------------------------------------
-- phase
------------------------------------------------
 
dsp["Phase - Invert phase"] = {
  cmds = { exe = "phase", mode = "phase 1", channels = "any", tip = "Playback of the output inverted soundfile shows that there is no audible difference from the input soundfile. However, if you mix the original sound with the phase inverted sound, you will generate a silent output: positive and negative amplitude values cancel out. However, if you mix the input and output with an offset, you get signal and hear an overlap of the two soundfiles (the cancelling out does not fully happen). You can try this with SUBMIX MERGE." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Phase - Stereo Enhance stereo file"] = {
  cmds = { exe = "phase", mode = "phase 2", channels = "2in2out", tip = "This mode is supposed to be a means of creating an enhanced stereo image by suppressing any aspects of the Left image which have bled to the Right, by adding (part of) the phase inverted Left signal on the Right, and vice versa. I (T Wishart) learned of this idea from a mixing engineer I met in France, but I'm still not convinced that this achieves anything except in very special circumstances (with particular types of material). The output may sound the same as the input, except perhaps on the largest PA systems." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "transfer", switch = "-t", min = 0, max = 1, tip = "amount of signal to be used in phase-cancellation." },
}
 
------------------------------------------------
-- pitch
------------------------------------------------
 
dsp["Pitch Altharms 1 - Delete odd harmonics"] = {
  cmds = { exe = "pitch", mode = "altharms 1", tip = "1 delete odd harmonics. This usually produces a transposition an octave higher, with no formant change.  To run this function, you must first extract the pitch information from the infile by running REPITCH GETPITCH. When you run PITCH ALTHARMS, it first automatically removes all the non-harmonic partials from the infile (the SPEC BARE process is built into it). Using the combination of this harmonic data and the pitch information in pitchfile, it can then remove the odd or even partials, depending on which mode you choose. Using Mode 1 to remove the odd partials makes the transposition without changing the spectral contour: the formants are preserved. Removing the even partials will timbrally colour the original sound, also without changing the spectral contour. Musical applications; A reference point for this program could be the difference between the timbre of a clarinet and that of an oboe. The sharper tone of the oboe is a result of the predominance of odd harmonics. So PITCH ALTHARMS is all about altering the tone of a sound. It does this without altering the formants, i.e., the frequency regions with the highest amplitude, so the recognisability of the sound remains, but the tone changes. It is likely that this function will produce more interesting results the more the source sound is rich in harmonics" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-x", switch = "-x", tip = "alternative spectral reconstruction" },
}
 
dsp["Pitch Altharms 2 - delete even harmonics"] = {
  cmds = { exe = "pitch", mode = "altharms 2", tip = "2 delete even harmonics. This usually produces a transposition an octave higher, with no formant change.  To run this function, you must first extract the pitch information from the infile by running REPITCH GETPITCH. When you run PITCH ALTHARMS, it first automatically removes all the non-harmonic partials from the infile (the SPEC BARE process is built into it). Using the combination of this harmonic data and the pitch information in pitchfile, it can then remove the odd or even partials, depending on which mode you choose. Using Mode 1 to remove the odd partials makes the transposition without changing the spectral contour: the formants are preserved. Removing the even partials will timbrally colour the original sound, also without changing the spectral contour. Musical applications; A reference point for this program could be the difference between the timbre of a clarinet and that of an oboe. The sharper tone of the oboe is a result of the predominance of odd harmonics. So PITCH ALTHARMS is all about altering the tone of a sound. It does this without altering the formants, i.e., the frequency regions with the highest amplitude, so the recognisability of the sound remains, but the tone changes. It is likely that this function will produce more interesting results the more the source sound is rich in harmonics" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-x", switch = "-x", tip = "alternative spectral reconstruction" },
}
 
dsp["Pitch Chord - Transposed versions of the sound are transposed onto the original based upon transpositions text file  "] = {
  cmds = { exe = "pitch", mode = "chord", tip = "(frq ratio -96.341276 to 95.999997) The transpose_file, bot and top parameters are handled as in PITCH CHORDF below. The difference here is that the formants are not extracted. This results in an increase in the inharmonic relationships among partials. Musical applications; The process enriches the sound timbrally, though also with a certain degree of resonance resulting from the presence of the 'chord'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "transpose_file", input = "txt", tip = "transpose_file a file of (possibly fractional) semitone transposition values" },
  arg4 = { name = "bot", switch = "-b", min = 0, max = 22050, input = "brk", def = 250, tip = "bottom frequency, below which data is filtered out" },
  arg5 = { name = "top", switch = "-t", min = 0, max = 22050, input = "brk", def = 10000, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg6 = { name = "-x", switch = "-x", tip = "for a fuller spectrum" },
}
 
dsp["Pitch Chordf - Transposed versions of the spectrum are superimposed within the existing spectral envelope, uses transposition text file"] = {
  cmds = { exe = "pitch", mode = "chordf", tip = "(Values for transposefile; frq ratio -96.341276 to 95.999997) For this process to work, you first need to enable one of the N toggles! A brief discussion of the extraction options can be found in the documentation for FORMANT GET. The bot and top parameters allow the user to optimise the operation of PITCH CHORDF by focusing on the main part of the sound. The output of PITCHINFO INFO might be useful for this, as it provides data on the pitch range of the sound. (Remember that you need a binary pitch file as produced by REPITCH GETPITCH as an input for PITCHINFO INFO.) The transpose_file is written as a series of semitone transposition values. 0 is used for no transposition and the 'chord' is built up from the bottom. The values can be written on separate lines or in a row, separated by spaces. Here's an example: 0 3 4.5 6 7 10 12 OR: 0 3 4.5 6 7 10 12. If the chord began on C, for example, this chord would be C-Eb-E¼#- F#-G-Bb-C1. Musical applications; This function provides a way to tune a sound to a chordal harmony. It is a spectral equivalent of a harmoniser: i.e., a multi-pitch shift which preserves formant information. This makes it especially suitable for the processing of vocal sounds. The net result is that the original sound becomes a chord of arbitrary dimensions. Once a sound is tuned to a chord, it can be developed in various ways. To explore this, you might try tuning a sound with the transpose_file above, then stretching the resulting file x3 with STRETCH TIME, and then sustaining the data of the stretched sound with FOCUS ACCU, giving a value of 0.1 for decay. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 2, max = 256, def = 12, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 12, def = 12, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "transpose_file", input = "txt", tip = "a file of (possibly fractional) semitone transposition values" },
  arg7 = { name = "bot", switch = "-b", min = 0, max = 22050, input = "brk", def = 250, tip = "bottom frequency, below which data is filtered out" },
  arg8 = { name = "top", switch = "-t", min = 0, max = 22050, input = "brk", def = 10000, tip = "required signal level for grain to be seen (Range: 0 to 1; the Default is 0.3)" },
  arg9 = { name = "-x", switch = "-x", tip = "for a fuller spectrum" },
}
 
dsp["Pitch Octmove 1 - Transpose up - Octave transpose without formant shift"] = {
  cmds = { exe = "pitch", mode = "octmove 1", tip = "1 transpose up. OCTMOVE provides a straightforward way to effect an octave shift of the spectrum. Note that it does this while preserving the formant characteristics of the original. Step One is to run REPITCH GETPITCH. Step two is to run PITCH OCTMOVE, using the GETPITCH pitchfile. Caution: PITCH OCTMOVE can only work if pitch is satisfactorily extracted by REPITCH GETPITCH. Any source with more than one pitch in it as a time (e.g., a chord, an orchestral passage etc.) does not have 'a pitch' as far as GETPITCH is concerned. Transposition of harmonically complex sound material is best done with REPITCH TRANSPOSEF. However, there is nothing to stop you exploring what might happen by running PITCH OCTMOVE with this kind of input. Musical applications; The transformation here results from the preserved formants appearing in different (octave) frequency locations. (The process works by deleting alternate harmonics.) You might want to compare this result with that of PITCH TRANSP which does not preserve formants. If you wanted to emphasise harmonic material, you could try running SPEC BARE on the infile first, and using the harmonics-only result as the revised infile for PITCH OCTMOVE. From an idea of Miller Puckette. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-i", switch = "-i", tip = "alternative spectral reconstruction" },
  arg5 = { name = "transposition", min = 2, max = 4096, def = 2, tip = "transposition an integer transposition ratio (2 is an octave, 3 is a 12th, etc.)" },
}
 
dsp["Pitch Octmove 2 - Transpose down - Octave transpose without formant shift"] = {
  cmds = { exe = "pitch", mode = "octmove 2", tip = "2 transpose down. OCTMOVE provides a straightforward way to effect an octave shift of the spectrum. Note that it does this while preserving the formant characteristics of the original. Step One is to run REPITCH GETPITCH. Step two is to run PITCH OCTMOVE, using the GETPITCH pitchfile. Caution: PITCH OCTMOVE can only work if pitch is satisfactorily extracted by REPITCH GETPITCH. Any source with more than one pitch in it as a time (e.g., a chord, an orchestral passage etc.) does not have 'a pitch' as far as GETPITCH is concerned. Transposition of harmonically complex sound material is best done with REPITCH TRANSPOSEF. However, there is nothing to stop you exploring what might happen by running PITCH OCTMOVE with this kind of input. Musical applications; The transformation here results from the preserved formants appearing in different (octave) frequency locations. (The process works by deleting alternate harmonics.) You might want to compare this result with that of PITCH TRANSP which does not preserve formants. If you wanted to emphasise harmonic material, you could try running SPEC BARE on the infile first, and using the harmonics-only result as the revised infile for PITCH OCTMOVE. From an idea of Miller Puckette. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-i", switch = "-i", tip = "alternative spectral reconstruction" },
  arg5 = { name = "transposition", min = 2, max = 4096, def = 2, tip = "transposition an integer transposition ratio (2 is an octave, 3 is a 12th, etc.)" },
}
 
dsp["Pitch Octmove 3 - Transpose down with bass reinforcement - Octave transpose without formant shift"] = {
  cmds = { exe = "pitch", mode = "octmove 3", tip = "3 transpose down with bass reinforcement. OCTMOVE provides a straightforward way to effect an octave shift of the spectrum. Note that it does this while preserving the formant characteristics of the original. Step One is to run REPITCH GETPITCH. Step two is to run PITCH OCTMOVE, using the GETPITCH pitchfile. Caution: PITCH OCTMOVE can only work if pitch is satisfactorily extracted by REPITCH GETPITCH. Any source with more than one pitch in it as a time (e.g., a chord, an orchestral passage etc.) does not have 'a pitch' as far as GETPITCH is concerned. Transposition of harmonically complex sound material is best done with REPITCH TRANSPOSEF. However, there is nothing to stop you exploring what might happen by running PITCH OCTMOVE with this kind of input. Musical applications; The transformation here results from the preserved formants appearing in different (octave) frequency locations. (The process works by deleting alternate harmonics.) You might want to compare this result with that of PITCH TRANSP which does not preserve formants. If you wanted to emphasise harmonic material, you could try running SPEC BARE on the infile first, and using the harmonics-only result as the revised infile for PITCH OCTMOVE. From an idea of Miller Puckette. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-i", switch = "-i", tip = "alternative spectral reconstruction" },
  arg5 = { name = "transposition", min = 2, max = 4096, def = 2, tip = "transposition an integer transposition ratio (2 is an octave, 3 is a 12th, etc.)" },
  arg6 = { name = "bassboost", min = 0, max = 100, def = 0, tip = "reinforcement (Range: >= 0.0)" },
}
 
dsp["Pitch Pick 1 - Only retain channels which might hold the partials specified - Harmonic series"] = {
  cmds = { exe = "pitch", mode = "pick 1", tip = "1 Harmonic series - This function locates the analysis channels which would contain the specified sets of partials, and adjusts the frequency data in those channels onto the specified frequencies. The data in the other channels may or may not be (partially) suppressed with the clarity parameter. Musical applications; PITCH PICK will tune a spectrum onto the partial sets specified by the Mode. This tuning can be made to emerge or disappear gradually by increasing or decreasing the clarity factor through time. Various degrees of data reduction are likely to appear in the various modes. Modes 1 and 5 appear to maintain a fairly rich version of the original, while Modes 2 and 3 significantly thin the sound, the latter sounding a bit 'hollow'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fundamental", min = 10, max = 22050, def = 120, tip = "actual fundamental frequency in Hz of the harmonic series, or arbitrary fundamental frequency in Hz used for the calculation" },
  arg4 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "extent to which data in other channels is suppressed. Range: 0–1. Default 1." },
}
 
dsp["Pitch Pick 2 - Only retain channels which might hold the partials specified - Octaves"] = {
  cmds = { exe = "pitch", mode = "pick 2", tip = "2 Octaves - This function locates the analysis channels which would contain the specified sets of partials, and adjusts the frequency data in those channels onto the specified frequencies. The data in the other channels may or may not be (partially) suppressed with the clarity parameter. Musical applications; PITCH PICK will tune a spectrum onto the partial sets specified by the Mode. This tuning can be made to emerge or disappear gradually by increasing or decreasing the clarity factor through time. Various degrees of data reduction are likely to appear in the various modes. Modes 1 and 5 appear to maintain a fairly rich version of the original, while Modes 2 and 3 significantly thin the sound, the latter sounding a bit 'hollow'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fundamental", min = 10, max = 22050, def = 120, tip = "actual fundamental frequency in Hz of the harmonic series, or arbitrary fundamental frequency in Hz used for the calculation" },
  arg4 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "extent to which data in other channels is suppressed. Range: 0–1. Default 1." },
}
 
dsp["Pitch Pick 3 - Only retain channels which might hold the partials specified - Odd partials of harmonic series only"] = {
  cmds = { exe = "pitch", mode = "pick 3", tip = "3 Odd partials of harmonic series only - This function locates the analysis channels which would contain the specified sets of partials, and adjusts the frequency data in those channels onto the specified frequencies. The data in the other channels may or may not be (partially) suppressed with the clarity parameter. Musical applications; PITCH PICK will tune a spectrum onto the partial sets specified by the Mode. This tuning can be made to emerge or disappear gradually by increasing or decreasing the clarity factor through time. Various degrees of data reduction are likely to appear in the various modes. Modes 1 and 5 appear to maintain a fairly rich version of the original, while Modes 2 and 3 significantly thin the sound, the latter sounding a bit 'hollow'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fundamental", min = 10, max = 22050, def = 120, tip = "actual fundamental frequency in Hz of the harmonic series, or arbitrary fundamental frequency in Hz used for the calculation" },
  arg4 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "extent to which data in other channels is suppressed. Range: 0–1. Default 1." },
}
 
dsp["Pitch Pick 4 - Only retain channels which might hold the partials specified - Partials are successive linear steps (each of frqstep from the fundamental"] = {
  cmds = { exe = "pitch", mode = "pick 4", tip = "4 Partials are successive linear steps (each of frqstep from the fundamental - This function locates the analysis channels which would contain the specified sets of partials, and adjusts the frequency data in those channels onto the specified frequencies. The data in the other channels may or may not be (partially) suppressed with the clarity parameter. Musical applications; PITCH PICK will tune a spectrum onto the partial sets specified by the Mode. This tuning can be made to emerge or disappear gradually by increasing or decreasing the clarity factor through time. Various degrees of data reduction are likely to appear in the various modes. Modes 1 and 5 appear to maintain a fairly rich version of the original, while Modes 2 and 3 significantly thin the sound, the latter sounding a bit 'hollow'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fundamental", min = 10, max = 22050, def = 120, tip = "actual fundamental frequency in Hz of the harmonic series, or arbitrary fundamental frequency in Hz used for the calculation" },
  arg4 = { name = "frqstep", min = 10, max = 22050, def = 100, tip = "frequency step in Hz to be added to another frequency." },
  arg5 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "extent to which data in other channels is suppressed. Range: 0–1. Default 1." },
}
 
dsp["Pitch Pick 5 - Only retain channels which might hold the partials specified - Add linear displacement (frqstep) to harmonic partials over the fundamental"] = {
  cmds = { exe = "pitch", mode = "pick 5", tip = "5 Add linear displacement (frqstep) to harmonic partials over the fundamental - This function locates the analysis channels which would contain the specified sets of partials, and adjusts the frequency data in those channels onto the specified frequencies. The data in the other channels may or may not be (partially) suppressed with the clarity parameter. Musical applications; PITCH PICK will tune a spectrum onto the partial sets specified by the Mode. This tuning can be made to emerge or disappear gradually by increasing or decreasing the clarity factor through time. Various degrees of data reduction are likely to appear in the various modes. Modes 1 and 5 appear to maintain a fairly rich version of the original, while Modes 2 and 3 significantly thin the sound, the latter sounding a bit 'hollow'. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "fundamental", min = 10, max = 22050, def = 120, tip = "actual fundamental frequency in Hz of the harmonic series, or arbitrary fundamental frequency in Hz used for the calculation" },
  arg4 = { name = "frqstep", min = 10, max = 22050, def = 100, tip = "frequency step in Hz to be added to another frequency." },
  arg5 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0.5, tip = "extent to which data in other channels is suppressed. Range: 0–1. Default 1." },
}
 
dsp["Pitch Transp 1 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Octave shift up, above frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 1", tip = "PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Transp 2 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Octave shift down, below frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 2", tip = "2 Octave shift down, below frq_split - PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Transp 3 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Octave shift up and down from frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 3", tip = "3 Octave shift up and down from frq_split - PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Transp 4 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Pitch shift up, above frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 4", tip = "4 Pitch shift up, above frq_split - PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "transpos", min = -133.278752, max = 133.278752, def = 10, tip = "transposition above or below frq_split (semitones)" },
  arg5 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Transp 5 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Pitch shift down, below frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 5", tip = "5 Pitch shift down, below frq_split - PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "transpos", min = -133.278752, max = 133.278752, def = 0, tip = "transposition above or below frq_split (semitones)" },
  arg5 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Transp 6 - Shift pitch of (part of) the spectrum, keeping harmonic relationships - Pitch shift up and down from frq_split"] = {
  cmds = { exe = "pitch", mode = "transp 6", tip = "6 Pitch shift up and down from frq_split - PITCH TRANSP transposes (part of) the spectrum up or down a specified number of semitones (including fractional parts of semitones). All the partials are moved, preserving the relationships among the partials: because the semitone transposition value becomes a multiplier within the program. Musical applications; What this means musically, is that some of the timbral character of the original remains with the PITCH SHIFT process (relationships among partials maintained), and some of the timbral character is lost (formants not preserved). The output of PITCHINFO INFO can be useful because it returns the maximum, minimum and mean pitch of a soundfile. Remember that it works on a binary pitch data file as produced by REPITCH GETPITCH. It would be useful to compare, experimentally, the results of PITCH TRANSP with PITCH OCTMOVE and with STRETCH SPECTRUM. PITCH OCTMOVE preserves formants, whereas PITCH TRANSP does not. And with PITCH TRANSP, the relationships between the partials are maintained, whereas with STRETCH SPECTRUM, the stretch factor is applied to the uppermost (or lowest) channel only, and all the partials inbetween are rescaled by intermediate amounts, depending on their relative position. Try listening to these subtle differences: 1) formants preserved (OCTMOVE), 2) formants not preserved, but harmonic relationships maintained (TRANSP), and 3) rescaling the partial relationships (STRETCH SPECTRUM)." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_split", min = 5, max = 22050, def = 6000, tip = "frequency in Hz above or below which the shift takes place" },
  arg4 = { name = "transpos1", min = -133.278752, max = 133.278752, def = 0, tip = "transposition above frq_split (semitones)" },
  arg5 = { name = "transpos2", min = -133.278752, max = 133.278752, def = 0, tip = "transposition below frq_split(semitones)" },
  arg6 = { name = "depth", switch = "-d", min = 0, max = 1, def = 0.5, tip = "transposition effect on source: from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Pitch Tune 1 - Replace spectral frequencies by harmonics of the specified pitch(es) - enter pitch_template data as frequency (in Hz)"] = {
  cmds = { exe = "pitch", mode = "tune 1", tip = "1 enter pitch_template data as frequency (in Hz) - This program provides the most elegant way to tune a spectrum to a specified pitch set, or 'harmony'. Every component of the analysis is tuned to either one of the specified pitches, or to one of their harmonics. The final sound is a very 'natural' sounding resonance based on the specified chord. carrying the spectral articulation of the original sound. The process works best on unpitched, pitch-unspecific, or noisy materials, but can be applied to any sound. The specified pitch set is best situated within the main pitch range of the sound: in the region of the fundamental(s). The simplest way to do this is with a tuning fork. SPECINFO REPORT Mode 3 will also list the frequencies of the peaks in the evolving spectrum. And if there is sufficient pitch material to get a 'reading', PITCHINFO INFO can give a useful overview of the maximum, minimum and mean frequencies. NB: the PITCHINFO functions will not work with binary pitch data files (produced by REPITCH GETPITCH) if the -z flag was used.) The data is entered on the same line separated by spaces, or as a vertical list. A pitch name to MIDI note number chart (e.g., C-x : 60) arranged in octaves can be handy when entering pitch data in MIDI values. The 'x' indicates which octave, starting from the bottom of the MIDI range. Musical applications; This tuning process can be used simply to colour or sound, to build harmonic structure into the composition, or to make direct links with other pitch material, such as from live acoustic instruments. Note that the 'specified pitch set' can be arbitrarily constructed." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch_template", min = 0.1, max = 22050, input = "brk", def = 1250, tip = "frequency (in Hz)" },
  arg4 = { name = "focus", switch = "-f", min = 0, max = 1, input = "brk", def = 1, tip = "determines the degree of focusing of the partial pitches onto the template (Range: 0 to 1. Default is 1)" },
  arg5 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0, tip = " determines the degree to which non-template partials are suppressed (Range 0 to 1. Default is 0)" },
  arg6 = { name = "trace", switch = "-t", min = 2, max = 40000, input = "brk", def = 512, tip = "specifies the number of (window-by-window) most prominent channels to be replaced by the template frequencies" },
  arg7 = { name = "bcut", switch = "-b", min = 0, max = 22050, input = "brk", def = 250, tip = "ignore frequencies below bcut, the Bass cutoff frequency" },
}
 
dsp["Pitch Tune 2 - Replace spectral frequencies by harmonics of the specified pitch(es) - enter pitch_template data as (possibly fractional) MIDI values"] = {
  cmds = { exe = "pitch", mode = "tune 2", tip = "2 enter pitch_template data as (possibly fractional) MIDI values - This program provides the most elegant way to tune a spectrum to a specified pitch set, or 'harmony'. Every component of the analysis is tuned to either one of the specified pitches, or to one of their harmonics. The final sound is a very 'natural' sounding resonance based on the specified chord. carrying the spectral articulation of the original sound. The process works best on unpitched, pitch-unspecific, or noisy materials, but can be applied to any sound. The specified pitch set is best situated within the main pitch range of the sound: in the region of the fundamental(s). The simplest way to do this is with a tuning fork. SPECINFO REPORT Mode 3 will also list the frequencies of the peaks in the evolving spectrum. And if there is sufficient pitch material to get a 'reading', PITCHINFO INFO can give a useful overview of the maximum, minimum and mean frequencies. NB: the PITCHINFO functions will not work with binary pitch data files (produced by REPITCH GETPITCH) if the -z flag was used.) The data is entered on the same line separated by spaces, or as a vertical list. A pitch name to MIDI note number chart (e.g., C-x : 60) arranged in octaves can be handy when entering pitch data in MIDI values. The 'x' indicates which octave, starting from the bottom of the MIDI range. Musical applications; This tuning process can be used simply to colour or sound, to build harmonic structure into the composition, or to make direct links with other pitch material, such as from live acoustic instruments. Note that the 'specified pitch set' can be arbitrarily constructed." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch_template", min = 0, max = 136, input = "brk", def = 0, tip = "(possibly fractional) MIDI values" },
  arg4 = { name = "focus", switch = "-f", min = 0, max = 1, input = "brk", def = 1, tip = "determines the degree of focusing of the partial pitches onto the template (Range: 0 to 1. Default is 1)" },
  arg5 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0, tip = " determines the degree to which non-template partials are suppressed (Range 0 to 1. Default is 0)" },
  arg6 = { name = "trace", switch = "-t", min = 2, max = 40000, input = "brk", def = 512, tip = "specifies the number of (window-by-window) most prominent channels to be replaced by the template frequencies" },
  arg7 = { name = "bcut", switch = "-b", min = 0, max = 22050, input = "brk", def = 250, tip = "ignore frequencies below bcut, the Bass cutoff frequency" },
}
 
------------------------------------------------
-- pitchinfo
------------------------------------------------
 
dsp["Pitchinfo Convert - Convert a binary pitch data file to a time frequency breakpoint text file"] = {
  cmds = { exe = "pitchinfo", mode = "convert", tip = "Formerly SPEC PCNV, PITCHINFO CONVERT converts the binary format of a REPITCH GETPITCH binary pitch contour file to a breakpoint text file format, which can then be viewed and edited with a text editor. The Data reduction parameter controls the pitch resolution of the breakpoint data, reducing the amount of data displayed, and therefore making it easier to read the data. Set the parameter so that you do not lose too much detail of pitch resolution. Musical applications; The list of times and frequencies can be useful when planning filtering operations, because it shows what the main pitch range of the sound is and when key frequency changes occur. " },
  arg1 = { name = "Input.trn", input = "trn", tip = "Select an input .trn file" },
  arg2 = { name = "Ouput.txt", output = "txt", tip = "Select an output .txt file" },
  arg3 = { name = "I", switch = "-d", min = 0, max = 24, def = 0.25, tip = " is the acceptable pitch error in the breakpoint file data-reduction process (I is given in semitones, where 1 = one semitone, 0.5 = ¼ tone, etc.). Range: > 0.0. Default: eighth-tone = 0.250000." },
}

dsp["Pitchinfo Hear - Convert pitchfile to analysis test tone file (resynthesise to hear pitch)"] = { 
  cmds = { exe = "pitchinfo", mode = "hear", tip = " Once the pitch contour of a sound has been extracted using REPITCH GETPITCH, the pitch contour can be heard by converting the binary pitchfile to a synthetic tone which follows the pitch contour. This provides a rapid aural check on the accuracy of the pitch-extraction process. Remember that you need to resynthesise the outfile. Note that, if unpitched windows are retained when using REPITCH GETPITCH, PITCHINFO HEAR will refuse to operate, returning the message: 'File ... contains unpitched windows: cannot proceed.' Don't be too surprised by the output! It follows the contour in a continuous manner, i.e., glissing from value to value. A test file of this nature can also be produced directly by REPITCH GETPITCH. Musical applications; This function produces a synthetic tone which follows the pitch contour, enabling it to be heard. This provides a rapid aural check on the accuracy of the pitch-extraction process. It can be important actually to hear what REPITCH GETPITCH has made of a particular source file, particularly if the pitch contour is going to be a key shaping device for another process. Formerly SPEC PAUD. The outfile produced by REPITCH GETPITCH can also be resynthesised for audition. " }, 
  arg1 = { name = "Input.frq", input = "frq", tip = "Select the input data to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", switch = "-g", min = 0, max = 1, def = 1, tip = "scales the amplitude of the outfile. Must be > 0.0 (Default: 1.0)" },
}

dsp["Pitchinfo Info - Display information about pitch data in a (binary) pitchfile"] = { 
  cmds = { exe = "pitchinfo", mode = "info", tip = "This function analyses the data in a binary pitchfile and reports on the mean pitch, the highest and lowest pitch (including when they occurred) in both frequency-in-Hz and MIDI note number formats. It also displays the total range, in octaves and semitones. The display is on-screen, not to a file. Here is an example of the output of this function: PITCHINFO INFO Output Example MAX PITCH: 343.59HZ   MIDI: 64.72   TIME 1.26, MIN PITCH : 79.97HZ   MIDI : 39.48   TIME 2.94, MEAN PITCH : 198.27HZ   MIDI : 55.20, TOTAL RANGE: 2 OCTAVES and 1.24 SEMITONES. Musical applications; This output gives us a snapshot of the overall pitch range of a soundfile, with the times at which the highest and lowest pitches occur. This can be particularly useful when, for example, defining a filterbank to operate on this file. See the Groucho program, USERBANK." },
  arg1 = { name = "pitchfile", input = "frq", tip = "input binary pitch data file, as output by REPITCH GETPITCH" },
}

dsp["Pitchinfo See - 1 Convert binary pitchfile or transposition file to a pseudo-soundfile, for viewing"] = { 
  cmds = { exe = "pitchinfo", mode = "see 1", input = "data", tip = "1 Use binary pitch data file as input and scale viewing range with scalefact. In Mode 1, pitchfile is a binary pitch data file. Scalefact (> 0.0) multiplies pitch values, for ease of viewing. Pitch data scaled by (e.g.) 100 can be read directly from outsndfile, remembering to divide numeric values by 100 to return to their actual value.  Musical applications; Formerly SPEC PSEE. Open the pseudo-soundfile outsndfile for viewing with a sound viewer editor, such as by CDP's VIEWSF or by Cool Edit Pro. Remember that this is a pseudo-soundfile! Its display is not sample-by-sample, but window-by-window. Also, as the amplitude range of soundfiles is -32768 to 32767 and the range of pitches is likely to be between 16Hz and ca 3500Hz, it may be a good idea to scale up the pitch data by e.g., a factor of 100. It will then use the whole range of the pitch viewing screen, giving a clear indication of pitch contour, and the pitch data can be read off, to the 2nd decimal place – remembering to divide the viewed values by 100 (whatever scalefact was), in order to get the real values. If scalefact is too high, the values will be truncated, and you'll probably see some clipping (horizontal lines at the top). Reduce the value for scalefact if this occurs." },
  arg1 = { name = "Input.frq", input = "frq" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "scalefact", min = 0.01, max = 1000, def = 1, tip = "viewing scale factor (must be > 0.0)" },
}

dsp["Pitchinfo See - 2 Convert binary pitchfile or transposition file to a pseudo-soundfile, for viewing"] = { 
  cmds = { exe = "pitchinfo", mode = "see 2", input = "data", tip = "2 Use binary transposition data file as input – scaling is automatic. NB – this input transposition file is made by REPITCH COMBINE (or REPITCH COMBINEB). In Mode 2, transposfile is a binary transposition data file. Transposition data is automatically scaled to ½ the maximum range and displayed in logarithmic format (0 = no transposition, + = up, - = down), giving a schematic idea ONLY, of transposition data. Musical applications; Formerly SPEC PSEE. Open the pseudo-soundfile outsndfile for viewing with a sound viewer editor, such as by CDP's VIEWSF or by Cool Edit Pro. Remember that this is a pseudo-soundfile! Its display is not sample-by-sample, but window-by-window. Also, as the amplitude range of soundfiles is -32768 to 32767 and the range of pitches is likely to be between 16Hz and ca 3500Hz, it may be a good idea to scale up the pitch data by e.g., a factor of 100. It will then use the whole range of the pitch viewing screen, giving a clear indication of pitch contour, and the pitch data can be read off, to the 2nd decimal place – remembering to divide the viewed values by 100 (whatever scalefact was), in order to get the real values. If scalefact is too high, the values will be truncated, and you'll probably see some clipping (horizontal lines at the top). Reduce the value for scalefact if this occurs." },
  arg1 = { name = "Input.trn", input = "trn" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}

dsp["Pitchinfo Zeros - Show whether a pitch data file contains uninterpolated zeros (unpitched windows) "] = { 
  cmds = { exe = "pitchinfo", mode = "zeros", input = "pvoc_data_to_pvoc", tip = " When REPITCH GETPITCH analyses a sound for its pitch contour, it is looking for pitch data in each window. If the data does not meet the criteria set for finding a pitch, and if the -z flag is set, it will retain these windows, setting them to -1. Musical applications; With this function you can see how well pitch is being extracted from your sound. Is it that it does not have any significant pitch content, or have you perhaps set the parameters inappropriately? And where exactly are these non-pitch events? " },
  arg1 = { name = "Input.frq", input = "frq" },
}
 
------------------------------------------------
-- prefix
------------------------------------------------
 
dsp["Prefix Silence - Add silence to the beginning of a soundfile"] = {
  cmds = { exe = "prefix", mode = "silence", channels = "any", tip = "PREFIX SILENCE simply places the specified duration of silence at the beginning of the infile. Musical Applications; This can be a straightforward way to create a gap between soundfiles, or time an entry. The utility might come in handy when running batch files" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "dur", min = 0, max = 10, tip = "duration of silence to add" },
}
 
------------------------------------------------
-- psow
------------------------------------------------
 
dsp["Psow Dupl - Timestretch/transpose a sound by duplicating the pitch-synchronised grains, uses text file"] = {
  cmds = { exe = "psow", mode = "dupl", tip = "This process time-stretches the sound, but preserves the pitch (like MODIFY SPEED) and preserves the vowel formants. FOFS-per-grain (segcnt) is best set at 1 but other small values do not alter the output a great deal. Also see PSOW STRETCH. I have found that inputs made with PSOW GRAB can have very few grains in them, and you can get an error message 'too few grains'. For example, if the GRAB file has 4 grains in it and you give a segcnt of 5, it will fail for this reason. Note that it will also fail if segcnt is 4, because the count starts at 0: 0-1-2-3 is 4 grains. Thus in this case, segcnt = 3 will succeed. Also, please remember that you cannot use a GRAB file until you have created the corresponding pitch-brkpnt-data file for it (the 3-step procedure). Musical applications; This is another way of lengthening a sound. The number of repetitions affects how much longer it will be, while the number of grains in a chunk affects granulation. If segcnt = 1, the result will be as smooth as it is possible to be with this kind of input and process. There may be pitch steps related to the pitch trace of the file. If segcnt is considerably longer, e.g., 10 or 15, the result will be more granulated, i.e., have a coarser grain. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros (indicating moments of no-signal) but NOT pitch-zeros (indicating moments of no-pitch)." },
  arg4 = { name = "repeat-cnt", min = 0, max = 220, def = 10, tip = "number of repetitions in each chunk." },
  arg5 = { name = "segcnt ", min = 0, max = 220, def = 3, tip = "number of grains in a chunk." },
}
 
dsp["Psow Cutatgrain - 1 Retain file BEFORE (exact) specified grain time. Cut at exact grain time"] = {
  cmds = { exe = "psow", mode = "cutatgrain 1", tip = "1 Retain file BEFORE (exact) specified grain time. This process allows you to cut a sound at a FOF boundary (the one nearest to the time you specify). This may be useful, as the files are cut at zero-crossings in the signal, and can hence be joined together, without splices, in various ways. In Sound Loom, the time you set in CUTATGRAIN is automatically transferred to the edit-time box in GRAB (and vice-versa. Musical applications; Here we are specifying a time at which presumably we intend to perform a PSOW FOF-manipulation. With PSOW CUTATGRAIN we can cut and retain the portion of sound before this time (Mode 1) or at and after this time (Mode 2). The cut is automatically made at a zero-crossing, creating ideal conditions for a trouble-free splice later. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros (indicating moments of no-signal) but NOT pitch-zeros" },
  arg4 = { name = "time", min = 0, max = length/1000, tip = "time in seconds at which to cut the file" },
}
 
dsp["Psow Cutatgrain - 2 Retain file AT and AFTER (exact) specified grain. Cut at exact grain time  "] = {
  cmds = { exe = "psow", mode = "cutatgrain 2", tip = "2 Retain file AT and AFTER (exact) specified grain. This process allows you to cut a sound at a FOF boundary (the one nearest to the time you specify). This may be useful, as the files are cut at zero-crossings in the signal, and can hence be joined together, without splices, in various ways. In Sound Loom, the time you set in CUTATGRAIN is automatically transferred to the edit-time box in GRAB (and vice-versa. Musical applications; Here we are specifying a time at which presumably we intend to perform a PSOW FOF-manipulation. With PSOW CUTATGRAIN we can cut and retain the portion of sound before this time (Mode 1) or at and after this time (Mode 2). The cut is automatically made at a zero-crossing, creating ideal conditions for a trouble-free splice later. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros (indicating moments of no-signal) but NOT pitch-zeros" },
  arg4 = { name = "time", min = 0, max = length/1000, tip = "time in seconds at which to cut the file" },
}
 
dsp["Psow Delete - Time-shrink sound by deleting a proportion of the pitch-synchronised grains"] = {
  cmds = { exe = "psow", mode = "delete", tip = "This process will timeshrink a sound, without changing pitch or vowels. FOFS-per-grain (segcnt) greater than 1 give more realistic results. This is a straightforward thinning process, like straining the sound through different sizes of mesh. Note that the 'mesh' is specified in two ways: the proportion to keep and the length of the grain chunk. Musical applications; Let's consider several types of 'mesh': keeping a high proportion of big chunks, keeping a high proportion of small chunks, keeping a low proportion of big chunks, keeping a low proportion of small chunks, keeping a midrange proportion of medium-sized chunks" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg4 = { name = "propkeep", min = 0, max = 1000, def = 2, tip = "proportion of chunks to keep. '2' keeps 1 in 2; '7' keeps 1 in 7, etc." },
  arg5 = { name = "segcnt", min = 0, max = 1000, def = 10, tip = "number of grains in a chunk" },
}
 
dsp["Psow Features - 1 Impose new features on vocal-type sound, preserving or modifying FOF-grains"] = {
  cmds = { exe = "psow", mode = "features 1", tip = "1 Transposition accompanied by 'timewarp': pitch is higher and the sound is shorter, i.e., standard Time Domain transposition. Mode 1 transposing involving timewarp does its pitch transposition like MODIFY SPEED, shortening the sound if the pitch goes up. PSOW FEATURES combines various elements of some of the other PSOW processes, and the parameters have the same functions, except that the transposition parameters are here entered in semitones rather than ratios. Some additional information about the parameters may be useful: attenuation – This parameter attenuates the output (applies amplitude reduction), and may be necessary when using a 'FOF-stretching' value of more than 1. subharmno ('Subharmonic number') – This determines the pitch of the subharmonic generated. You can use 0 or 1 if you do not want a subharmonic. For example, 2 divides the fundamental pitch by 2, producing a pitch an octave lower, 3 divides it by 3, producing a pitch an octave and a 5th lower and so on. subharmamp ('subharmonic level') – This determines the loudness of any subharmonic introduced. FOF-stretching sustains individual FOFs (it does NOT time-stretch the sound) producing an effect akin to reverberation. Musical applications; This program is one of the most versatile in the set." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg4 = { name = "segcnt", min = 0, max = 1000, def = 2, tip = "number of grains in a chunk to be retained as-is." },
  arg5 = { name = "trans", min = -24, max = 24, def = 2, tip = "pitch transposition in semitones." },
  arg6 = { name = "vibfrq", min = 0, max = 1000, def = 10, tip = "frequency of any added vibrato." },
  arg7 = { name = "vibdepth", min = 0, max = 1000, def = 10, tip = "depth in semitones of any added vibrato." },
  arg8 = { name = "spectrans", min = -24, max = 24, def = 10, tip = "amount of transposition of the spectrum in semitones (will not change the fundamental pitch)." },
  arg9 = { name = "hoarseness", min = 0, max = 1, def = 0.5, tip = "degree of vocal hoarseness (roughness) to introduce. Range: 0 to 1." },
  arg10 = { name = "attenuation", min = 0, max = 1, def = 0.6, tip = "attentuation. Range 0 to 1. May be necessary when FOF-stretching, due to overlaps." },
  arg11 = { name = "subharmno", min = -48, max = 48, def = 0, tip = "amount by which the fundamental pitch is divided. Both 0 and 1 give NO subharmonics." },
  arg12 = { name = "subharmamp", min = 0, max = 1, def = 0.9, tip = "amplitude level of any subharmonic introduced. Range: 0 to 1." },
  arg13 = { name = "FOF-stretching", min = 1, max = 512, def = 3, tip = "time extension of the FOF components. This does NOT stretch the overall soundfile length. Range: 1 to 512." },
  arg14 = { name = "-a", switch = "-a", tip = "option to use an alternative algorithm for the FOF stretch. " },
}
 
dsp["Psow Features - 2 Impose new features on vocal-type sound, preserving or modifying FOF-grains"] = {
  cmds = { exe = "psow", mode = "features 2", tip = "2 The transposed pitch is accompanied by an additional lower pitch. Mode 2 transposing involving double pitches does not change the timeframe of the sound but may introduce octavation or other double-pitch features when it transposes by large amounts. PSOW FEATURES combines various elements of some of the other PSOW processes, and the parameters have the same functions, except that the transposition parameters are here entered in semitones rather than ratios. Some additional information about the parameters may be useful: attenuation – This parameter attenuates the output (applies amplitude reduction), and may be necessary when using a 'FOF-stretching' value of more than 1. subharmno ('Subharmonic number') – This determines the pitch of the subharmonic generated. You can use 0 or 1 if you do not want a subharmonic. For example, 2 divides the fundamental pitch by 2, producing a pitch an octave lower, 3 divides it by 3, producing a pitch an octave and a 5th lower and so on. subharmamp ('subharmonic level') – This determines the loudness of any subharmonic introduced. FOF-stretching sustains individual FOFs (it does NOT time-stretch the sound) producing an effect akin to reverberation. Musical applications; This program is one of the most versatile in the set." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg4 = { name = "segcnt", min = 0, max = 1000, def = 2, tip = "number of grains in a chunk to be retained as-is." },
  arg5 = { name = "trans", min = -24, max = 24, def = 2, tip = "pitch transposition in semitones." },
  arg6 = { name = "vibfrq", min = 0, max = 1000, def = 10, tip = "frequency of any added vibrato." },
  arg7 = { name = "vibdepth", min = 0, max = 1000, def = 10, tip = "depth in semitones of any added vibrato." },
  arg8 = { name = "spectrans", min = -24, max = 24, def = 10, tip = "amount of transposition of the spectrum in semitones (will not change the fundamental pitch)." },
  arg9 = { name = "hoarseness", min = 0, max = 1, def = 0.5, tip = "degree of vocal hoarseness (roughness) to introduce. Range: 0 to 1." },
  arg10 = { name = "attenuation", min = 0, max = 1, def = 0.6, tip = "attentuation. Range 0 to 1. May be necessary when FOF-stretching, due to overlaps." },
  arg11 = { name = "subharmno", min = -48, max = 48, def = 0, tip = "amount by which the fundamental pitch is divided. Both 0 and 1 give NO subharmonics." },
  arg12 = { name = "subharmamp", min = 0, max = 1, def = 0.9, tip = "amplitude level of any subharmonic introduced. Range: 0 to 1." },
  arg13 = { name = "FOF-stretching", min = 1, max = 512, def = 3, tip = "time extension of the FOF components. This does NOT stretch the overall soundfile length. Range: 1 to 512." },
  arg14 = { name = "-a", switch = "-a", tip = "option to use an alternative algorithm for the FOF stretch. " },
}
 
dsp["Psow Grab - Grab a pitch-synchronised grain from a file, and use it to create a new sound "] = {
  cmds = { exe = "psow", mode = "grab", tip = "This process grabs an individual (group of) FOF(s) and uses it to produce a new sound. You must specify the time in the source sound where the FOF-to-be-grabbed is located, and the duration of the output sound to produce. Duration ZERO grabs a single (group of) FOF(s). These outputs are specifically for use with PSOW INTERP. While they may actually run in other functions, they will be too short to do anything useful. Therefore, when using GRAB to create FOF-grains to use with other functions, give a duration suitable for the purpose. Note that if there is no pitch at the grab-time you choose, you will not get any result. The other parameters provided with PSOW GRAB enable you do carry out a significant amount of processing at the same time. Density (pitch transposition) specifies transposition of the fundamental as a frequency ratio. So 2.0 is an octave up, and 0.25 is 2 octaves down). Spectrans (spectral transposition) specifies the transposition of the spectrum in the output sound. This is a bit like changing the vowel without changing the pitch. Rand (randomisation) randomizes the position of the FOFs in the output, introducing a hoarse noisiness. Gain (amplitude adjustment downwards) may be needed if density is greater than one, because the overlaying of grains will sum amplitudes. In Sound Loom, when you grab a FOF, the time you set is automatically transferred to the edit-time box in CUTATGRAIN (and vice-versa. Musical applications; The most direct use for outputs of PSOW GRAB is as inputs to PSOW INTERP – interpolating between two pitch-synchronised grains. GRAB files can be used for other functions, but bear in mind the advice given above: first to create the matching pitch-brkpnt-data file, and to grab a sufficient number of grains appropriate for the function that you are then going to use. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros (indicating moments of no-signal) but NOT pitch-zeros" },
  arg4 = { name = "time", min = 0, max = length/1000, tip = "time in seconds at which to grab the grain(s)." },
  arg5 = { name = "duration", min = 0, max = 60, def = 6, tip = "Duration of the output soundfile. If duration is set to 0, a single grain (chunk) is grabbed" },
  arg6 = { name = "segcnt", min = 0, max = 1000, def = 10, tip = "The number of grains in a chunk." },
  arg7 = { name = "spectrans", min = 0.001, max = 8, def = 1, tip = "amount of transposition of the spectrum in semitones" },
  arg8 = { name = "density", min = 0.125, max = 8, def = 1, tip = "The rate at which the grain-chunks in the output soundfile succeed one another." },
  arg9 = { name = "randomisation", min = 0, max = 1, def = 0.5, tip = "Randomisation of the position of the grain-chunks in the output soundfile." },
  arg10 = { name = "gain", min = 0, max = 1, def = 0.9, tip = "Amplitude adjustment applied to the output. 1 = full amplitude. " },
}
 
dsp["Psow Impose - Attempts to impose vocal FOFs in 1st sound onto a 2nd sound "] = {
  cmds = { exe = "psow", mode = "impose", tip = "This is an experimental program that attempts to impose the FOF characteristics of one sound onto another. Some of the parameters could use further explanation: Depth of application relates to the degree to which the 2nd soundfile is affected. If depth = 0, the 2nd sound is not altered. If depth = 1, the 2nd sound is completely altered by the 1st. Values inbetween give intermediate results: closer to 0 results in a limited degree of alteration, whereas values closer to one create more alteration. Wsize sets the length of the segments in the second soundfile to use to determine amplitude. Longer lengths mean less resolution, i.e., less responsiveness to changes of amplitude. The results affect how the second sound is normalised. In a fairly steady-state resultant sound, longer windows may be fine, but if there are rapid changes of amplitude, it will be better to use a finer resolution: a smaller wsize. Gate enables you to set the maximum level in the output soundfile. Note that the parameter is in decibels. A gain reduction of 0.7 represents -3dB. For our Chart of Gain-Decibel relationships. Musical applications; The pitch synchronous grains of the first soundfile, which emphasise vowel material, colour those of the second. The weight parameter in Mode 2 seems particularly useful. Note that the focus is on the psow-grains, and not formants (vowel resonances), so the aural results will be different from the 'cross-synthesis' achieved with FORMANTS VOCODE." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg5 = { name = "depth", min = 0, max = 1, tip = "Depth of application of FOFs to the second sound. Range: 0 to 1." },
  arg6 = { name = "wsize", min = 0, max = 1000, def = 20, tip = "Windowsize in milliseconds to track the envelope of the second sound (for normalisation purposes)" },
  arg7 = { name = "gate", min = -96, max = 0, def = 0, tip = "Level in decibels in the second sound at which it is assumed to be zero (full amplitude)." },
}
 
dsp["Psow Interleave - Interleave FOFs from two different files"] = {
  cmds = { exe = "psow", mode = "interleave", tip = "This process alternates (groups of) FOFs from two different sounds and hence requires 2 pitch-data files, one corresponding to each of the input sounds. You may notice in the example above that om.wav is 0.9 sec long, sixi.wav is 3.0 sec long, and the output ominterlsixi.wav is 0.929 seconds long, very close to the shortest soundfile. Sometimes the output is actually shorter than the shortest input. This is thought to be caused by FOFs not being found in some regions of the file (T. Wishart). Using the parameters: Bias determines to what extent the output pitch is biased towards the pitch of one file or the other (values: 0, no bias, 1 biased to 1st, -1 biased to 2nd) with intermediate values giving intermediate degrees of bias. Balance determines the relative loudness of the two components. 1 gives an equal balance, while values greater than 1 make the 1st louder and values less than 1 make the 2nd louder. Weight (Mode 2) determines the relative number of FOF components from the 2 sources in the input, and the parameter works like the balance parameter. Musical applications; This function performs the same sort of operation as COMBINE INTERLEAVE, but with the focus on FOF-grain type sonic material. One hears a pulsed interleaving of the two sounds. with grplen determining the resolution of the interleave (fine with low values, coarse – longer units – with higher values). The three parameters to balance the relative strengths of the two inputs are very important and useful in determining the tonal quality of the output. Fractional values are allowed, and they can be adjusted with great precision." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "pitch-brkpnt-data1", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "pitch-brkpnt-data2", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile2." },
  arg6 = { name = "grouplength", min = 0, max = 1000, def = 20, tip = "the number of FOFs in each manipulated segment" },
  arg7 = { name = "bias", min = -1, max = 1, def = 0, tip = "Is the output pitch biased to one or the other of the infiles? 0: no bias; 1: biased to the first soundfile; -1: biased to the second soundfile." },
  arg8 = { name = "balance", min = -10, max = 10, def = 0, tip = "the level balance of the components of the two input soundfiles to be placed in the output soundfile: 1: equally loud; > 1: the first soundfile is louder; < 1: the second soundfile is louder." },
  arg9 = { name = "weight", min = -10, max = 10, def = 0, tip = "relative number of components from the two input soundfiles to place in the output soundfile: 1.0: equal amounts; > 1: more of the first soundfile; < 1: more of the second soundfile. " },
}
 
dsp["Psow Replace - Combine FOFs of 1st sound with the pitch of the 2nd sound "] = {
  cmds = { exe = "psow", mode = "replace", tip = "PSOW REPLACE combines the FOF structure of the first sound with the pitch of 2nd. It requires two pitch data files, one for each source sound. Musical applications; This function comes into its own when the pitch structure of the 2nd file has a distinctive character. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "pitch-brkpnt-data1", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "pitch-brkpnt-data2", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile2." },
  arg6 = { name = "grpcnt", min = 0, max = 1000, def = 20, tip = "number of FOFs in a grain-chunk" },
}
 
dsp["Psow Space - Distribute the alternate FOFs in the sound over a stereo space  "] = {
  cmds = { exe = "psow", mode = "space", tip = "Distributes the alternate FOFs in the sound over the stereo space. Note that placing alternate FOFs to the left and then to the right, causes the sound to drop an octave in pitch, hence the subharmonic number parameter begins with value 2 (producing the octave downwards shift). Higher subharmonic values shift the heard pitch down further (via the subharmonic series). The fact that the output soundfile sounds lower may be unexpected and deserves further explanation. Trevor Wishart writes: 'Because FOFs are being alternated in space, it's like having 2 separate sound-streams, each at ½ the original pitch (the pitch drops by an octave). Subharmonic_number allows you to drop the pitch further (divide the frequency by 3, 4, 5 – i.e., down an 8va + a 5th, two 8vas, and two 8vas + a major 3rd respectively.) It is like the harmonic series upside-down.' The separation parameter determines where the FOFs are placed in the space. With a value of 0, they all appear at the centre (and there is no pitch-shift, whatever the subharmonic number parameter), while 1 places the alternate FOFs at the extreme right and left (in that order), and -1 at the extreme left and right (in that order). Intermediate values give intermediate values of spatial separation, which means that fractional values are allowed. The Balance, with a range from 0 to 8, determines the relative loudness of the signals on left and right speakers. While 1.0 sets the balance equal in both speakers, note that when the value is greater than 1, the level on the left is lowered by dividing the signal by the number entered for balance, and when the value is less than 1, the level on the right is increased by multiplying by the number entered for balance. This use of multiplication and division means that the values entered can create major changes in level. A muffling of the sound will result by making use of the hisuppress parameter. Together with the ability to lower the pitch of the sound, low, throaty sounds can be produced. Musical applications; This is the only PSOW function that produces a stereo output. Given the alternation between speakers, there is some similarity to effects that can be achieved with TEXTURE. Low, gravelly intonations akin to Tibetan chanting are easily produced with PSOW SPACE. Also note that, unlike other time domain downward transpositions, the output sound is lower but not longer." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg4 = { name = "subharmno", min = 2, max = 5, def = 3, tip = "subharmonic number, which divides the frequency of the source. Range: 2 to 5." },
  arg5 = { name = "separation", min = -1, max = 1, def = 0, tip = "spatial separation of alternate FOFs. Range: -1 to 1." },
  arg6 = { name = "balance", min = 0, max = 8, def = 4, tip = "of left and right components. Range: 0 to 8." },
  arg7 = { name = "hisuppress", min = 0, max = 1, def = 0, tip = "suppress high-frequency components. Range: 0 to 1." },
}
 
dsp["Psow Split - Split vocal FOFs into subharmonic and upwardly transposed pitch "] = {
  cmds = { exe = "psow", mode = "split", tip = "Allows you to transpose the pitch up (without changing the timeframe or the vowels) and to add subharmonic frequencies, independently and simultaneously. Musical applications; This process makes the sound thicker, a voice sound hoarse and 'gravelly'. Note the wide range of acceptable transposition values and the ability to bring out the higher frequencies by using a higher value for balance." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg4 = { name = "subharmno", min = 3, max = 8, def = 3, tip = "subharmonic number, which divides the frequency of the source. Range: 2 to 5." },
  arg5 = { name = "uptrans", min = 0, max = 48, def = 0, tip = "upward transposition in semitones. Range: 0 to 48." },
  arg6 = { name = "balance", min = 0, max = 8, def = 4, tip = "amplitude level of the up-transposed components relative to the subharmonics. Range: 0 to 8." },
}
 
dsp["Psow Stretch - Timestretch/transpose a sound by repositioning the pitch-synchronised grains."] = {
  cmds = { exe = "psow", mode = "stretch", tip = "This process time-stretches the sound, and changes the pitch (like MODIFY SPEED) but will preserve the vowel formants. FOFS-per-grain (segcnt) is best set at 1 for realistic results. The result when segcnt was one was continuous, but granular in character: a sequence of adjacent grains/pulses. Musical applications; Again, the nature of the FOF process as essentially granular in character affects the results. As seen, even 'realistic results' are pulsed. Note how gaps can be produced with higher values for timestretch as in our first example. This distinguishes the process from PSOW DUPL, while its granular character is essentially different from normal time-stretch. Higher values for segcnt make the FOF-chunks longer, giving more of the source and reducing the gaps (a little). On the other hand, timestretch values less than 1, as in our second example, make the stretched sound more continuous. However, there does seem to be a tendency for amplitude overload with values less than 0.9, probably because the grains start to overlap. This tendency increases when segcnt is increased. There is therefore an important interplay between the amount of timestretch and the values for segcnt." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg4 = { name = "timestretch", min = 0.1, max = 10, def = 2, tip = "proportion by which the sound is to be stretched (> 1) or compressed (< 1). (Larger values create longer gaps.) Range: 0.1 to 10.0." },
  arg5 = { name = "segcnt", min = 0, max = 1000, def = 2, tip = "number of grains in a chunk retained without change, while gaps between segments are stretched." },
}
 
dsp["Psow Strtrans - Timestretch/transpose a sound by repositioning the pitch-synchronised grains, and overlapping them "] = {
  cmds = { exe = "psow", mode = "strtrans", tip = "This process will timestretch a sound, without changing vowels. Each FOF is individually repeated and hence, with many repeats, the process produces artefacts similar to waveset duplication. The pitch can also be (independently) shifted (without changing the vowels). These 2 parameters tend to interact to produce subharmonic artefacts. Musical applications; With PSOW STRTRANS the output can be more continuous, whereas PSOW STRETCH produces discrete pulsations, whether adjacent or with a gap. Also, it appears that higher values for trans, along with a larger timestretch factor tends to create a stepped effect, similar to Sample-Hold or FOCUS FREEZE and FOCUS HOLD." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg4 = { name = "timestretch", min = 0.1, max = 10, def = 2, tip = "proportion by which the sound is to be stretched (> 1) or compressed (< 1). (Larger values create longer gaps.) Range: 0.1 to 10.0." },
  arg5 = { name = "segcnt", min = 0, max = 1000, def = 2, tip = "number of grains in a chunk retained without change, while gaps between segments are stretched." },
  arg6 = { name = "trans", min = -48, max = 48, def = 3, tip = "transposition in semitones, corresponds to overlap between successive segments." },
}
 
dsp["Psow Sustain - Freeze and sustain a sound on a specified pitch-synchronised grain"] = {
  cmds = { exe = "psow", mode = "sustain", tip = "This process plays the sound up until a specified time. It then sustains the (group of) FOF(s) found at that time for dur length of time. Having done so, it plays the remainder of the sound. You may add vibrato to the sustained FOF. Musical applications; This is a way of lengthening a particular FOF sound and is particularly useful to stretch out the central portion of a sound, leaving onset and conclusion the same. The sound is reasonably clean and smooth if the vib parameters are set to 0, but becomes more pulsated as their values increase, sometimes with unpredictable artefacts because of the granular nature of the process. The smoothest results are achieved by using the -s flag." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg4 = { name = "time", min = 0, max = length/1000, tip = "time – time at which to freeze the grain(s)." },
  arg5 = { name = "dur", min = 0, max = 60, def = 10, tip = "duration – duration of output soundfile. This must be greater than the duration of the input soundfile." },
  arg6 = { name = "segcnt", min = 0, max = 1000, def = 10, tip = "segcnt – number of grains in a chunk." },
  arg7 = { name = "vibfrq", min = 0, max = 20, def = 10, tip = "frequency of any added vibrato." },
  arg8 = { name = "vibdepth", min = 0, max = 3, tip = "depth in semitones of any added vibrato. Range: 0.0 to 3.0." },
  arg9 = { name = "transpose", min = -48, max = 24, input = "brk", tip = "transpose – transposition of grain in semitones (Range: -48 to +24 semitones). This parameter may be time-varying, but time = 0 will be the start of an expanded" },
  arg10 = { name = "gain", min = 0, max = 10, def = 5, tip = "loudness contour of the entire output (Range: 0 to 10)" },
  arg11 = { name = "-s", switch = "-s", tip = "option to smooth the grabbed fofs" },
}
 
dsp["Psow Synth - 1 fixed frequency bands - Impose vocal FOFs on a stream of synthesised sound"] = {
  cmds = { exe = "psow", mode = "synth 1", tip = "1 – fixed frequency bands: each line of oscdatafile has a pair of values for a frequency and an amplitude. Amplitude range is from 0 to 1. The program runs, but the results, at least with this short sound, seem not to show the frequency spread expected. So far, attempts to produce interesting results have not achieved anything. Musical applications; I think the best thing to do at this stage is to try it out with various types of source files: shorter, longer, with distinct pitch traces, with significant vowel changes etc. One expects to find results similar to, but distinguishable from time-varying filtering – see FILTER VARIBANK. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "oscdatafile", input = "txt", tip = "optional file for amplitude values (Amplitude range is from 0 to 1.)" },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "depth", min = 0, max = 100, def = 0.9, tip = "depth of application of FOFs to the synthesised sound." },
}
 
dsp["Psow Synth - 2 fixed MIDI bands - Impose vocal FOFs on a stream of synthesised sound"] = {
  cmds = { exe = "psow", mode = "synth 2", tip = "2 – fixed MIDI bands: each line of oscdatafile has a pair of values for a midipitch and an amplitude. Amplitude range is from 0 to 1. The program runs, but the results, at least with this short sound, seem not to show the frequency spread expected. So far, attempts to produce interesting results have not achieved anything. Musical applications; I think the best thing to do at this stage is to try it out with various types of source files: shorter, longer, with distinct pitch traces, with significant vowel changes etc. One expects to find results similar to, but distinguishable from time-varying filtering – see FILTER VARIBANK. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "oscdatafile", input = "txt", tip = "optional file for amplitude values (Amplitude range is from 0 to 1.)" },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "depth", min = 0, max = 100, def = 0.9, tip = "depth of application of FOFs to the synthesised sound." },
}
 
dsp["Psow Synth - 3 variable frequency bands - Impose vocal FOFs on a stream of synthesised sound"] = {
  cmds = { exe = "psow", mode = "synth 3", tip = "3 – variable frequency bands: each line of oscdatafile frequency and amplitude values in the format used for FILTER VARIBANK. Time Frq Amp Frq Amp ... Amplitude range is from 0 to 1. The program runs, but the results, at least with this short sound, seem not to show the frequency spread expected. So far, attempts to produce interesting results have not achieved anything. Musical applications; I think the best thing to do at this stage is to try it out with various types of source files: shorter, longer, with distinct pitch traces, with significant vowel changes etc. One expects to find results similar to, but distinguishable from time-varying filtering – see FILTER VARIBANK. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "oscdatafile", input = "txt", tip = "optional file for amplitude values (Amplitude range is from 0 to 1.)" },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "depth", min = 0, max = 100, def = 0.9, tip = "depth of application of FOFs to the synthesised sound." },
}
 
dsp["Psow Synth - 4 variable MIDI bands - Impose vocal FOFs on a stream of synthesised sound"] = {
  cmds = { exe = "psow", mode = "synth 4", tip = "4 – variable MIDI bands: each line of oscdatafile has midipitch and amplitude values in the format used for FILTER VARIBANK: Time MPV Amp MPV Amp ... Amplitude range is from 0 to 1. The program runs, but the results, at least with this short sound, seem not to show the frequency spread expected. So far, attempts to produce interesting results have not achieved anything. Musical applications; I think the best thing to do at this stage is to try it out with various types of source files: shorter, longer, with distinct pitch traces, with significant vowel changes etc. One expects to find results similar to, but distinguishable from time-varying filtering – see FILTER VARIBANK. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "oscdatafile", input = "txt", tip = "optional file for amplitude values (Amplitude range is from 0 to 1.)" },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg5 = { name = "depth", min = 0, max = 100, def = 0.9, tip = "depth of application of FOFs to the synthesised sound." },
}
 
dsp["Psow Synth - 5 noise - Impose vocal FOFs on a stream of synthesised sound"] = {
  cmds = { exe = "psow", mode = "synth 5", tip = "5 – noise: no oscdatafile; the synthetic source is noise. The program runs, but the results, at least with this short sound, seem not to show the frequency spread expected. So far, attempts to produce interesting results have not achieved anything. Musical applications; I think the best thing to do at this stage is to try it out with various types of source files: shorter, longer, with distinct pitch traces, with significant vowel changes etc. One expects to find results similar to, but distinguishable from time-varying filtering – see FILTER VARIBANK. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form for insndfile1." },
  arg4 = { name = "depth", min = 0, max = 100, def = 0.9, tip = "depth of application of FOFs to the synthesised sound." },
}
 
dsp["Psow Reinforce - 1 Reinforce the harmonic content of the sound. Reinforce harmonics in a vocal-type FOF-grain file  "] = {
  cmds = { exe = "psow", mode = "reinforce 1", tip = "1 Reinforce the harmonic content of the sound. This process attempts to reinforce harmonics in the sound by overlaying FOFs in particular ways. The special data file required contains paired values for harmonic_number and amplitude. In Mode 2, the harmonic numbers can be fractional and produces inharmonic spectra from pitched sounds. In addition, in Mode 2, these constituents can be sustained for longer, using the weight paramemter. Musical applications; Mode 1 enables us to re-weight the harmonic content and therefore tonal character of the pitched sound. In mode 2, the inharmonic character of the output is apparent in the example above. A review of the harmonic series can be useful for this program, and those that involve subharmonics: namely that the 2nd harmonic is the octave, the 3rd a perfect fifth above that, the 4th two octaves, and the 5th harmonic is two octaves plus a major third, etc. See PSOW SPACE and PSOW SPLIT, which employ subharmonics." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "reinforcement-data", input = "txt", tip = "text file with pairs of values: for harmonic_number level." },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros " },
  arg5 = { name = "delay", switch = "-d", min = 0, max = 1000, tip = "optional time in milliseconds by which to delay the onset of the added harmonics" },
  arg6 = { name = "-s", switch = "-s", tip = "option to omit FOFs generated for higher harmonics which coincide with FOFs of lower harmonics." },
}
 
dsp["Psow Reinforce - 2 Reinforce the sound with inharmonic partials. Reinforce harmonics in a vocal-type FOF-grain file  "] = {
  cmds = { exe = "psow", mode = "reinforce 2", tip = "2 Reinforce the sound with inharmonic partials. This process attempts to reinforce harmonics in the sound by overlaying FOFs in particular ways. The special data file required contains paired values for harmonic_number and amplitude. In Mode 2, the harmonic numbers can be fractional and produces inharmonic spectra from pitched sounds. In addition, in Mode 2, these constituents can be sustained for longer, using the weight paramemter. Musical applications; Mode 1 enables us to re-weight the harmonic content and therefore tonal character of the pitched sound. In mode 2, the inharmonic character of the output is apparent in the example above. A review of the harmonic series can be useful for this program, and those that involve subharmonics: namely that the 2nd harmonic is the octave, the 3rd a perfect fifth above that, the 4th two octaves, and the 5th harmonic is two octaves plus a major third, etc. See PSOW SPACE and PSOW SPLIT, which employ subharmonics." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "reinforcement-data", input = "txt", tip = "text file with pairs of values: for harmonic_number level." },
  arg4 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros " },
  arg5 = { name = "weight", switch = "-w", min = 1, max = 256, def = 4, tip = "sustain inharmonic components. A higher weight gives a longer sustain. Please note that a very high weight may cause buffers to overflow." },
}
 
dsp["Psow Sustain2 - Freeze and sustain a sound on an explicitly specified grain"] = {
  cmds = { exe = "psow", mode = "sustain2", tip = "This process behaves very similarly to PSOW SUSTAIN, but asks you to specify the precise start and end times at which the FOF is located. Larger durations between the start and end times produce a coarser result. The nudge parameter is particularly important because it allows you to shift the position of the sustained FOF-grain just a little bit. You can produce phasing effects by creating several outputs, each with a slightly different nudge amount, and then mixing them together, all starting in the mix at the same time. Musical applications; We can produce cleanly sustained sounds, sounds that wobble slowly, sounds that are thicker and more unpredictable, and sounds with phasing effects (after mixing several different outputs)." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000, def = 0, tip = "time in seconds at which to cut the grain." },
  arg4 = { name = "end", min = 0, max = length/1000, tip = "time in seconds of the end of the grain." },
  arg5 = { name = "dur", min = 0, max = 30, tip = "duration of output soundfile. This must be greater than the input soundfile's duration." },
  arg6 = { name = "vibfrq", min = 0, max = 20, def = 10, tip = "frequency of any added vibrato." },
  arg7 = { name = "vibdepth", min = 0, max = 3, tip = "depth in semitones of any added vibrato. Range: 0.0 to 3.0." },
  arg8 = { name = "nudge", min = -24, max = 24, tip = "move selected grain position by nudge zero crossings." },
}
 
dsp["Psow Interp - Interpolate between 2 pitch-synchronised grains, to produce a new sound."] = {
cmds = { exe = "psow", mode = "interp", tip = "Interpolate between 2 pitch-synchronised grains, to produce a new sound. Grains acquired by PSOW GRAB, with duration 0.0. This process creates a new sound by interpolating between two SINGLE (groups of) FOFs extracted with 0 duration from the same source sound or different sources) by PSOW GRAB. (It therefore does not need the usual pitch-brkpnt-data textfile). If you use input files not created with PSOW GRAB, you will get the error message: 'File 1 is not a valid pitch-sync grain file.' Note that the process was intended to work with single FOFs as input, but will also work with groups of FOFs, and between groups of FOFs of different sizes. To get larger 'groups of FOFs', make segcnt longer when using PSOW GRAB. As noted above, a longer segcnt in the sources increases the granulation in the output of PSOW INTERP. The output sound can be shaped using the various parameters. Startdur and enddur shape the beginning and the end by allowing you to sustain the first and last grains. The length of time interpdur over which to realise the interpolation between the two grains, can also be set. Note that 'vibrato' refers to the rate and amount of frequency change, and 'tremolo' does the same with amplitude. Musical applications; A 'plain vanilla' output can be made by setting the two vib and two trem parameters to 0. But when applied, they can thicken and otherwise shape the output sound in very precise ways. The aural difference between the two input sounds (vowels, usually) and the length of time over which the interpolation takes place, shape the nature of the transition. You might want to compare the nature of the output (somewhat granulated) with transitions produced by MORPH BRIDE, MORPH GLIDE or COMBINE CROSS." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
  arg3 = { name = "startdur", min = 0, max = length/1000, tip = "length of time in seconds to sustain the initial grain" },
  arg4 = { name = "interpdur", min = 0, max = 30, def = 1, tip = "duration of the interpolation, affecting the length of the outfile." },
  arg5 = { name = "enddur", min = 0, max = length/1000, tip = "length of time in seconds to sustain the final grain" },
  arg6 = { name = "vibfrq", min = 0, max = 1000, def = 0, tip = "frequency of any added vibrato (can be 0)" },
  arg7 = { name = "vibdepth", min = 0, max = 3, def = 0, tip = "depth in semitones of any added vibrato. Range: 0 to 3." },
  arg8 = { name = "tremfrq", min = 0, max = 1000, def = 0, tip = "frequency of any added tremolo (can be 0)" },
  arg9 = { name = "tremdepth", min = 0, max = 100, def = 0, tip = "depth of any added tremolo" },
}

dsp["Psow Locate - Locate the exact start time of the nearest FOF-grain "] = {
cmds = { exe = "psow", mode = "locate", tip = "This process returns the exact start time of the FOF nearest to the time in the source that you specify. Musical applications; This utility can help to determine times for CUTATGRAIN or GRAB with more precision. It may be useful to know where exactly the file is being cut, e.g., when going on to some other process in which material has to synchronise exactly to the sample." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form." },
  arg3 = { name = "time", min = 0, max = length/1000, tip = "time at which to find the start of a grain" },
}

--[[ audio input, textfile input, multiple audio results, not sure about the first argument, maybe it automatically appends numbers to the audio file's input name? Isn't necessary to input a string? 
dsp["Psow Chop - Chop sound into sections "] = {
cmds = { exe = "psow", mode = "chop", tip = "Chop sound into sections between specified FOF-grain (chunks) OR: Chop away sections of soundfile that you DON'T want to manipulate with PSOW functions. This process extracts lengths of pitch-focused sound from the input soundfile, saving each length as a separate soundfile. It does not remove anything from the original soundfile, which remains intact. These extracted lengths will be the sections that you DO NOT want to manipulate with one of the PSOW functions. The point is to cut around the material you want to FOF-process, process the material, and then splice back the extracted lengths (using 0 length splices). This process cuts up and stores a sound as a sequence of segments, extracting the material around the points where you want to apply FOF processing. The output is therefore at least 2 sound segments. You can then use PSOW GRAB (using the same edit points) to capture specific FOFs in a sound, extend them, and then reinsert them into the original sound at the edit points, by splicing the segments cut using PSOW CHOP and the new sounds made with PSOW GRAB in the correct order. Alternatively, you can use PSOW GRAB first, possibly making use of its processing features to develop the sound extracted right away. You would then use PSOW CHOP with the same segcnt (= graincnt) to save the material around these grabbed-and-transformed segments so that you could splice it all back together again. When the sound transformations are carried out while running PSOW GRAB, a special pitch-brkpnt-data text file for the grabbed portion is not needed, as the one for the sound from which the portion is grabbed is being used. It would be needed if you were going to use the grabbed portion with a different function, except for PSOW INTERP, which does not require a pitch-brkpnt-data file. See PSOW GRAB for an example of a sound transformed while grabbed. The time grain pairs (File of cuttime : grains-in-chunk pairs) is a textfile containing pairs of values, where the first value is a time for a cut, and the second value determines how many FOFs are to be 'stepped over' after this cut point. Note that a 'FOF' is very short – only a few milliseconds. The cut times therefore indicate where the FOF material to use is located: PSOW CHOP cuts around them, and PSOW GRAB gets the FOF material at these points for processing, and SFEDIT JOIN splices the resulting files back into a single soundfile. Additional Observations; The real point of PSOW CHOP is to cut away what you do not want to FOF-manipulate (e.g., consonants) and save this unwanted material as separate soundfiles. This means studying your sound in a sound editor (or by ear using FROM-TO playback) to determine where the good material you want to use as FOF source is. Note that the actual FOF material will be tiny: we are thinking 'grains', so the unused portions will contain the rest of the vowel material as well as consonants. It is not just a matter of finding start and end points for consonants and editing them out, a persistent misconception I have had about PSOW, and another way entirely of working with vocal material. In the 'time-grain' value pairs text file, you specify the start times for your FOF grains and their length as a number of 'grain-chunks'. PSOW CHOP then cuts out all of the material before and after these FOF sections (i.e., the material 'around' them) saving it as a series of separate soundfiles. After manipulating the FOF material with the other PSOW functions, you are then able to put back the original material by splicing these separate soundfiles (SFEDIT JOIN) with your FOF-manipulations, thus recreating aspects of the original vocal material, but now with FOF-manipulated enhancements inbetween. It is very important to remember that the CHOP process cuts 'around' the desired FOF source material. You do not have it yet! You need to use PSOW GRAB to extract the FOF grains that you will manipulate. But note that you can use the same times as used in PSOW CHOP: instead of cutting around them, you now cut out and keep the FOF grains. (PSOW CUTATGRAIN can also be used to cut out portions of infile.) However, you cannot use these newly acquired FOF grains until you create a pitch-brkpnt-data file for them, except for PSOW INTERP (for which the duration parameter must be 0.0). The lengths of the FOF-source file and the pitch-brkpnt-data file must match. This means repeating for each FOF grain soundfile the 3-step procedure you had to do before using PSOW CHOP: ANALYSE: pvoc anal 1 ingrabfile.wav outgrabfile.ana, GET PITCH TRACE: repitch getpitch 1 outgrabfile.ana outdummyfile outgrabfile.frq, CONVERT TO BREAKPOINT: ptobrk withzeros outgrabfile.frq outtgrabfile.txt 20 . In the Appendix 2, I provide generic batch files to do this with any input. NB: Soundshaper Pro runs these processes behind the scenes for the first – but not the second – input file. Musical applications; PSOW CHOP therefore enables you to carry out FOF processing with pinpoint precision, keeping the rest of the soundfile (now saved as a series of soundfiles) untouched. You then put it altogether again with SFEDIT JOIN. Note that you can repeat the same soundfile in the list to join, as well as reassemble the component soundfiles in any order. This program is a prime candidate for putting together a batch processing sequence, either with a command line interpreter, Sound Loom's Instruments or Soundshaper Pro's Grid system." },
arg1 = { name = "outsndfile-rootname", input = "string", def = "rootname", tip = "base name for a series of soundfile outputs; the first has no number attached, and subsequent outfiles have '_001', '_002' etc. appended." },
arg2 = { name = "pitch-brkpnt-data", input = "txt", tip = "text file with a pitch trace in time frequency breakpoint form. It may contain zeros (indicating moments of no-signal) but NOT pitch-zeros" },
arg3 = { name = "time-graincount-pairs", input = "txt", tip = "This is a text file of cut times expressed as time grain-count value pairs." },
}
--]]
 
------------------------------------------------
-- ptobrk
------------------------------------------------
 
dsp["Ptobrk Withzeros - convert pitch trace from binary .frq to text breakpoint file (.txt or .brk) for PSOW "] = {
  cmds = { exe = "ptobrk", mode = "withzeros", tip = "This process should be used instead of REPITCH GETPITCH Mode 2, which creates a breakpoint textfile of pitch data. REPITCH GETPITCH Mode 1 is used without the -z flag to create a .frq file with no-pitch and no-sound markers. PTOBRK WITHZEROS then converts this kind of .frq file to a time herz breakpoint file.  Musical applications; This is a simple utility for creating the correct format for the pitch trace used in PSOW to make the FOF-grains pitch-synchronous. The creation of this 'pitch-brkpnt-data' file for PSOW is in fact a 3-step process involving PVOC analysis, REPITCH GETPITCH and PTOBRK. This may sometimes be taken care of behind the scenes in a GUI, and may sometimes need to be done by hand. The PSOW Reference Manual Appendix 2 provides batch files to speed up this process." },
  arg1 = { name = "Input.frq", input = "frq", tip = "binary pitch trace file created by REPITCH GETPITCH Mode 1 without retaining pitch zeros." },
  arg2 = { name = "Output.txt", output = "txt", tip = "outtextfile - text breakpoint file format for the pitch trace data. This format is required by PSOW." },
  arg3 = { name = "min-pitch-dur", min = 1, max = 1000, def = 20, tip = "gives minimum time (in milliseconds) that any stretch data must persist to be regarded as valid data. Range: 1 to 1000 ms. Recommended value: 20ms. " },
}
 
------------------------------------------------
-- pvoc
------------------------------------------------
 
dsp["Extract - Analyse, then resynthesise sound with various options"] = {
  cmds = { exe = "pvoc", mode = "extract", tip = "EXTRACT – Analyse, then resynthesise sound with various options. NB: If no flags are set, the output sound will be the same as the input." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "points", switch = "-c", min = 2, max = 32768, def = 1024, tip = "the number of analysis points (2-32768 (power of 2)); Default: 1024 (was the -N flag)" },
  arg4 = { name = "overlap", switch = "-o", min = 1, max = 4, def = 3, tip = "overlap factor (1 – 4) Default: 3" },
  arg5 = { name = "dochans", switch = "-d", min = 1, max = 2, def = 1, tip = "resynthesise ODD (1) or EVEN (2) channels only" },
  arg6 = { name = "lochan", switch = "-l", min = 0, max = 512, def = 0, tip = "ignore analysis channels below this when resynthesising (Default: 0)" },
  arg7 = { name = "hichan", switch = "-h", min = 0, max = 512, def = 512, tip = "ignore analysis channels above this when resynthesising (Default: highest channel)" },
}
 
------------------------------------------------
-- repitch
------------------------------------------------
 
dsp["*Repitch Getpitch 1 - Extracts a pitch trace - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "getpitch 1", tip = "This is the key program in the REPITCH Group. It extracts a pitch trace from analysis data, producing a binary pitch data file (.frq), which can then be massaged by the other REPITCH functions. A number of these functions handle noise or silent data in the the .frq file. The other key program is COMBINE/B, which can combine binary pitch data files (i.e., pitch traces) to produce transposition data etc." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "Output.frq", output = "frq", },
  arg4 = { name = "TRange", switch = "-t", min = 0, max = 6, def = 1, tip = "Tuning range(semitones) within which harmonics are accepted as in tune" },
  arg5 = { name = "MinWin", switch = "-g", min = 0, max = 977, def = 2, tip = "Minimum number of adjacent windows that must be pitched, for a pitch-value to be registered" },
  arg6 = { name = "SNR", switch = "-s", min = 0, max = 1000, def = 80, tip = "Signal to noise ratio, in decibels - Windows which are more than SdB below maximum level in sound, are assumed to be noise, and any detected pitch value is assumed spurious" },
  arg7 = { name = "Harmonics", switch = "-n", min = 1, max = 8, def = 5, tip = "How many of the 8 loudest peaks in spectrum must be harmonics to confirm sound is pitched" },
  arg8 = { name = "Low Freq", switch = "-l", min = 10, max = 2756, def = 10, tip = "Frequency of LOWEST acceptable pitch" },
  arg9 = { name = "Top Freq", switch = "-h", min = 10, max = 2756, def = 2756, tip = "Frequency of TOPMOST acceptable pitch" },
  arg10 = { name = "Alt Pitch", switch = "-a", tip = "Alternative pitch-finding algorithm (avoid N < 2)" },
  arg11 = { name = "Retain", switch = "-z", tip = "Retain unpitched windows (set them to -1)" },
}
 
dsp["*Repitch Transpose - 4 Transposition as a binary data file (.trn)"] = {
  cmds = { exe = "repitch", mode = "transpose 4", tip = "4 Transposition as a binary data file (.trn) - REPITCH TRANSPOSE does fixed or time varying transposition in the spectral domain. It accepts a variety of breakpoint file inputs as well as a binary data file. The spectral envelope moves with the transposition (as opposed to TRANSPOSEF, where it does not), meaning that the timbres associated with formants are going to change. Musical applications; The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract a pitch contour with REPITCH GETPITCH, saving as a breakpoint (.brk) file and use this output to combine with another pitch contour or transposition file in COMBINE/B to form another (time varying) transposition file to apply to an infile. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSE in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSE. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. Modify pitchfile data in some way, save as a transposition file and use as an input to TRANSPOSE/F. The following REPITCH pitchfile modification functions have a transposition file output mode: APPROX, EXAG, INVERT, QUANTISE, RANDOMISE, SMOOTH and VIBRATO. The results of these transpositions can be used to alter timbre, especially with TRANSPOSE. Also, some use of transposition could form an important step when realising a morph: to draw one sound closer to that of another in frequency region or timbre, or to create a aurally transitional stage in the morphing process" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.trn", input = "trn", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "minfrq", switch = "-l", min = 0, max = 22050, def = 50, tip = "minimum frequency, below which data is filtered out" },
  arg5 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, def = 10000, tip = "maximum frequency, above which data is filtered out" },
  arg6 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["*Repitch TransposeF - 4 Transposition with a binary data file (.trn) as input - Transpose spectrum: but retain original spectral envelope"] = {
  cmds = { exe = "repitch", mode = "transposef 4", tip = "4 Transposition with a binary data file (.trn) as input - For this process to work, you need to enable one of the N toggles first! Here, formants are preserved (see TRANSPOSE, where they are not). Thus, for example, the vowel characteristics of the original source should be preserved. Musical applications; This process may be used when one wants to retain the original sound as much as possible, though in a different register. On the other hand, it creates an interesting play between the unfamiliar (appearing in a different, and perhaps unnatural, register), and the familiar (original formants are preserved). The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSEF in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSEF. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.trn", input = "trn", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "N", switch = "-f", min = 1, max = 256, def = 2, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally spaced frequency channels" },
  arg5 = { name = "N", switch = "-p", min = 0, max = 12, def = 2, tip = "extract formant envelope linear pitchwise, using N equally spaced pitch bands per octave" },
  arg6 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg7 = { name = "minfrq", switch = "-l", min = 0, max = 22050, input = "brk", def = 50, tip = "fuller spectrum" },
  arg8 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, input = "brk", def = 10000, tip = "fuller spectrum" },
  arg9 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch Analenv - Extract the window-loudness envelope of an analysis file"] = {
  cmds = { exe = "repitch", mode = "analenv", tip = "This function extracts the loudness envelope of an analysis file. It is part of a suite of processes to extract the pitch, formants or time-loudness envelope of spectral and then cross-combine them. The output is saved as a binary envelope file, for which .env is the standard CDP extension." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.env", output = "env", tip = "Select an output .env file" },
}
 
dsp["Repitch Approx 1 - Make an approximate copy of a pitchfile - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "approx 1", tip = "Gives a pitchfile as output. This produces a deliberately approximate copy of the original pitch file. Musical applications; When two people sing 'in unison', they do not sing exactly the same pitches, with exactly the same vibrato, exactly synchronously. This function might be used to make such a 'natural' copy of a pitch line." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "prange", switch = "-p", min = 0, max = 1000, input = "brk", def = 0, tip = " the interval in semitones over which the pitch varies (upwards or downwards) from the original (Range: > 0.0)" },
  arg4 = { name = "trange", switch = "-t", min = 0, max = length, input = "brk", def = 0, tip = "time interval in milliseconds by which the pitch value can stray from the original time." },
  arg5 = { name = "srange", switch = "-s", min = 0, max = length, input = "brk", def = 0, tip = "the time interval in milliseconds over which the pitch contour is scanned." },
}
 
dsp["Repitch Approx 2 - Make an approximate copy of a pitchfile - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "approx 1", tip = "Gives a transposition file as output. This produces a deliberately approximate copy of the original pitch file. Musical applications; When two people sing 'in unison', they do not sing exactly the same pitches, with exactly the same vibrato, exactly synchronously. This function might be used to make such a 'natural' copy of a pitch line." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .frq file" },
  arg3 = { name = "prange", switch = "-p", min = 0, max = 1000, input = "brk", def = 0, tip = " the interval in semitones over which the pitch varies (upwards or downwards) from the original (Range: > 0.0)" },
  arg4 = { name = "trange", switch = "-t", min = 0, max = length, input = "brk", def = 0, tip = "time interval in milliseconds by which the pitch value can stray from the original time." },
  arg5 = { name = "srange", switch = "-s", min = 0, max = length, input = "brk", def = 0, tip = "the time interval in milliseconds over which the pitch contour is scanned." },
}
 
dsp["Repitch Combine 1 - Generate transposition data from 2 sets of pitch data - Outputs a .trn file"] = {
  cmds = { exe = "repitch", mode = "combine 1", tip = "REPITCH COMBINE is the only way to produce a binary pitch transposition file (.trn) A .trn file is one of the required inputs for REPITCH TRANSPOSE/F. In Mode 1 two files of time-varying pitch data are used as inputs. They can both be produced from input analysis files by (separate runs of) REPITCH GETPITCH. Both can be binary pitch data files (.frq), or one can be a time pitch (frq-in-Hz) breakpoint file – not both, because this process will not generate (the needed) binary output from breakpoint data only. These input pitch data files are used by REPITCH COMBINE in Mode 1 to produce a new binary transposition file (.trn), or, using REPITCH COMBINEB (Text) Mode 1, a time transposition-ratio breakpoint file (.brk). This new transposition file contains the difference between the two input pitch data files translated into the transposition ratios needed to move from the pitch shape of the first file to the pitch shape of the second file. (The order of the two input pitch data files is therefore important!). You then go to REPITCH TRANSPOSE or REPITCH TRANSPOSEF, using Mode 4 (binary .trn file) to apply this transposition data to the original sound (as an analysis file) and create a new version reshaped with the pitch shape of the second file. REPITCH TRANSPOSE is used to apply this transposition data without preserving formants and REPITCH TRANSPOSEF is used to apply it while preserving formants." },
  arg1 = { name = "Input1.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Input2.frq", input = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Output.trn", output = "trn", tip = "Select an output .frq file" },
}
 
dsp["Repitch Combine 2 - Transpose pitch data with transposition data - Outputs a .frq file"] = {
  cmds = { exe = "repitch", mode = "combine 2", tip = "Mode 2 puts together a binary pitch data file (.frq) and a transposition file: a binary .trn file made from two sound sources, or a (hand-written) breakpoint file containing time transposition-ratio pairs. The transposition file reshapes the pitch trace of the pitch file. The output produced is a new pitch shape in binary pitch data file format (.frq). A binary transposition file (.trn) made in Mode 1 involves two source .frq files and can be used in Mode 2 to reshape the first of these sources with the characteristics of the second. However, it may also be applied to the pitch data (.frq) from any other sound. Thus the difference between the two Mode 1 source files is applied to a new, third file (with results that will be difficult to predict). The COMBINE Mode 2 output is another time varying binary pitch data file (.frq). NB: This file cannot be used as an input to TRANSPOSE or TRANSPOSEF, which only use transposition files (.trn or .brk) as inputs. The primary purpose of Mode 2 is to create binary pitch data (.frq) which can be given to various of the other REPITCH pitch shaping functions in order to further modify the data: see APPROX, CUT, EXAG, FIX, INVERT, QUANTISE, RANDOMISE, SMOOTH and VIBRATO in the listing at the top of this file." },
  arg1 = { name = "Input1.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Input2.trn", input = "trn", tip = "Select an input .trn file" },
  arg3 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
}
 
dsp["Repitch Combine 3 - Combine 2 sets of transposition data to form new transposition data - Outputs a .trn file"] = {
  cmds = { exe = "repitch", mode = "combine 3", tip = "REPITCH COMBINE is the only way to produce a binary pitch transposition file (.trn) A .trn file is one of the required inputs for REPITCH TRANSPOSE/F. In Mode 3, two sets of tranposition data (two binary .trn files – NB: this means 4 sources), or one binary .trn file and one time transposition-ratio breakpoint file – 3 sources) are the inputs combined to form a new binary .trn transposition file as the output. This process will sum the transposition data in the two files. At least one of these input files must be binary, because two input breakpoint files will NOT produce a binary output. The output transposition data can be cycled round again with Mode 2, or can be applied to a sound with TRANSPOSE or TRANSPOSEF, using Mode 4 (with a .trn binary input)." },
  arg1 = { name = "Input1.trn", input = "trn", tip = "Select an input .trn file" },
  arg2 = { name = "Input2.trn", input = "trn", tip = "Select an input .trn file" },
  arg3 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
}
 
dsp["Repitch Combineb 1 - Generate transposition data from 2 sets of pitch data, output .brk"] = {
  cmds = { exe = "repitch", mode = "combineb 1", tip = "1 Generate transposition data from 2 sets of pitch data (.frq OR time frq-in-Hz .brk + .frq OR time frq-in-Hz .brk = time transposition-ratio .brk). The inputs to COMBINEB can be the same mix of binary and breakpoint as for COMBINE. However, all the ouputs are breakpoint files, whether of pitch or of transposition data. (You can scroll up a bit to see the 'Table summarising the origin & uses of the Pitch & Transposition files').Musical applications; See REPITCH COMBINE above." },
  arg1 = { name = "Input1.frq", input = "frq", },
  arg2 = { name = "Input2.txt", input = "txt", },
  arg3 = { name = "Output.txt", output = "txt", },
  arg4 = { name = "I", switch = "-d", min = 0, max = 100, def = 0.25, tip = "acceptable pitch error in breakpoint file data reduction Range: I > 1.0. Default: eighth_tone = 0.250000" },
}
 
dsp["Repitch Combineb 2 - 2 Transpose pitch data with transposition data, output .brk"] = {
  cmds = { exe = "repitch", mode = "combineb 2", tip = "2 Transpose pitch data with transposition data (.frq OR time frq-in-Hz .brk + .trn OR time transposition-ratio = time frq-in-Hz .brk). The inputs to COMBINEB can be the same mix of binary and breakpoint as for COMBINE. However, all the ouputs are breakpoint files, whether of pitch or of transposition data. (You can scroll up a bit to see the 'Table summarising the origin & uses of the Pitch & Transposition files').Musical applications; See REPITCH COMBINE above." },
  arg1 = { name = "Input1.frq", input = "frq", },
  arg2 = { name = "Input2.trn", input = "trn", },
  arg3 = { name = "Output.txt", output = "txt", tip = "time pitch (frq-in-Hz) breakpoint file" },
  arg4 = { name = "I", switch = "-d", min = 0, max = 100, def = 0.25, tip = "acceptable pitch error in breakpoint file data reduction Range: I > 1.0. Default: eighth_tone = 0.250000" },
}
 
dsp["Repitch Combineb 3 - 3 Combine 2 sets of transposition data to form new transposition data, output .brk"] = {
  cmds = { exe = "repitch", mode = "combineb 3", tip = "3 Combine 2 sets of transposition data to form new transposition data (.trn OR time transposition-ratio .brk + .trn OR time transposition-ratio .brk = time transposition-ratio .brk). The inputs to COMBINEB can be the same mix of binary and breakpoint as for COMBINE. However, all the ouputs are breakpoint files, whether of pitch or of transposition data. (You can scroll up a bit to see the 'Table summarising the origin & uses of the Pitch & Transposition files').Musical applications; See REPITCH COMBINE above." },
  arg1 = { name = "Input1.trn", input = "trn", },
  arg2 = { name = "Input2.trn", input = "trn", },
  arg3 = { name = "Output.txt", output = "txt", },
  arg4 = { name = "I", switch = "-d", min = 0, max = 100, def = 0.25, tip = "acceptable pitch error in breakpoint file data reduction Range: I > 1.0. Default: eighth_tone = 0.250000" },
}
 
dsp["Repitch Cut 1 - Cut and retain from starttime to end of file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "cut 1", tip = "1 Cut and retain from starttime to end of file.  This function makes it possible to use part of a pitch data file without having to return to the time domain. PITCHINFO SEE and PITCHINFO HEAR might be helpful in deciding upon the time cut points in the binary pitch data file. Or you could view/listen to the original sound in the time domain (the timings ought to match!). Musical applications; The cut segment might be used, for example, as a template for the pitch of another sound, using REPITCH COMBINE to generate the appropriate transposition data (Mode 1 or 2), and REPITCH TRANSPOSE or REPITCH TRANSPOSEF to change the pitch of the other file. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "starttime", min = 0, max = length/1000, def = 0, tip = "time in seconds at which to begin the cut ­ if 0, begins at start of the file endtime time in seconds at which to end the cut" },
}
 
dsp["Repitch Cut 2 - Cut and retain from start of file to endtime - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "cut 2", tip = "2 Cut and retain from start of file to endtime. This function makes it possible to use part of a pitch data file without having to return to the time domain. PITCHINFO SEE and PITCHINFO HEAR might be helpful in deciding upon the time cut points in the binary pitch data file. Or you could view/listen to the original sound in the time domain (the timings ought to match!). Musical applications; The cut segment might be used, for example, as a template for the pitch of another sound, using REPITCH COMBINE to generate the appropriate transposition data (Mode 1 or 2), and REPITCH TRANSPOSE or REPITCH TRANSPOSEF to change the pitch of the other file. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "endtime", min = 0, max = length/1000, def = 0, tip = "time in seconds at which to begin the cut ­ if 0, begins at start of the file endtime time in seconds at which to end the cut" },
}
 
dsp["Repitch Cut 3 - Cut and retain portion between startime and endtime - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "cut 3", tip = "3 Cut and retain portion between startime and endtime. This function makes it possible to use part of a pitch data file without having to return to the time domain. PITCHINFO SEE and PITCHINFO HEAR might be helpful in deciding upon the time cut points in the binary pitch data file. Or you could view/listen to the original sound in the time domain (the timings ought to match!). Musical applications; The cut segment might be used, for example, as a template for the pitch of another sound, using REPITCH COMBINE to generate the appropriate transposition data (Mode 1 or 2), and REPITCH TRANSPOSE or REPITCH TRANSPOSEF to change the pitch of the other file. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "starttime", min = 0, max = length/1000, def = 0, tip = "time in seconds at which to begin the cut ­ if 0, begins at start of the file endtime time in seconds at which to end the cut" },
  arg4 = { name = "endtime", min = 0, max = length/1000, def = length/1000, tip = "time in seconds at which to begin the cut ­ if 0, begins at start of the file endtime time in seconds at which to end the cut" },
}
 
dsp["Repitch Exag 1 - Exaggerate pitch contour - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "exag 1", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "range", min = 0, max = 1000, input = "brk", def = 0, tip = "exaggeration of the pitch range: an interval expressed in semitones and used as a multiplier (Range: > 0)" },
}
 
dsp["Repitch Exag 2 - Exaggerate pitch contour - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "exag 2", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .frq file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "range", min = 0, max = 1000, input = "brk", def = 0, tip = "exaggeration of the pitch range: an interval expressed in semitones and used as a multiplier (Range: > 0)" },
}
 
dsp["Repitch Exag 3 - Exaggerate pitch contour - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "exag 3", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "contour", min = 0, max = 1, input = "brk", def = 0, tip = "exaggeration of the pitch contour (Range: values between 0 and 1)" },
}
 
dsp["Repitch Exag 4 - Exaggerate pitch contour - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "exag 4", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "contour", min = 0, max = 1, input = "brk", def = 0, tip = "exaggeration of the pitch contour (Range: values between 0 and 1)" },
}
 
dsp["Repitch Exag 5 - Exaggerate pitch contour - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "exag 5", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "range", min = 0, max = 1000, input = "brk", def = 0, tip = "exaggeration of the pitch range: an interval expressed in semitones and used as a multiplier (Range: > 0)" },
  arg5 = { name = "contour", min = 0, max = 1, input = "brk", def = 0, tip = "exaggeration of the pitch contour (Range: values between 0 and 1)" },
}
 
dsp["Repitch Exag 6 - Exaggerate pitch contour - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "exag 6", tip = "Mean pitch - which may vary over time – is the pitch around which the stretchings or shrinkings of intervals takes place. With the contour exaggeration, the program measures how far (above or below) each pitch is from the specified mean, then pushes it up or down, depending on the value set for contour. This either stretches the intervals close to the mean by a lot, and those far from the mean by a little, or vice versa. With the interval exaggeration, the program finds the interval between the mean and a note and multiplies it by the range variable. For example, a value of 0.5 will compress the intervals. Musical applications; Some form of alteration of the spectrum will result, but it is virtually impossible to tell what, as it is so highly dependent on the spectral content of the infile" },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "meanpch", min = -250, max = 250, input = "brk", def = 1, tip = "the pitch as a MIDI note number around which intervals are stretched" },
  arg4 = { name = "range", min = 0, max = 1000, input = "brk", def = 0, tip = "exaggeration of the pitch range: an interval expressed in semitones and used as a multiplier (Range: > 0)" },
  arg5 = { name = "contour", min = 0, max = 1, input = "brk", def = 0, tip = "exaggeration of the pitch contour (Range: values between 0 and 1)" },
}
 
dsp["Repitch Fix - Massage pitch data in a binary pitchfile - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "fix", tip = "Formerly SPECPMOD, this function attempts to improve on (or smooth) the pitch contour produced by REPITCH GETPITCH. It is easiest to use if the pitch contour is first converted to a pseudo-soundfile (with PITCHINFO SEE) and viewed at the sample level with VIEWSF or another sound editor capable of zooming to sample accuracy. This should reveal the presence of any unwanted glitches. Musical applications; If the pitch output from REPITCH GETPITCH sounds awry, you can look at it with PITCHINFO SEE to see what kind of glitches there might be. Then use the appropriate REPITCH FIX option to eliminate or reduce the glitches." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "T1", switch = "-r", min = 0, max = length/1000, def = 0, tip = "Starttime: remove pitch from time t1 (Default: 0.0)" },
  arg4 = { name = "T2", switch = "-x", min = 0, max = length/1000, def = length/1000, tip = "Endtime: end pitch removal at time t2 (Default: end of file)" },
  arg5 = { name = "bf", switch = "-l", min = 0, max = 22050, def = 200, tip = "Bottom frequency: remove pitch below frequency" },
  arg6 = { name = "tf", switch = "-h", min = 0, max = 22050, def = 10000, tip = "Top frequency: remove pitch above frequency" },
  arg7 = { name = "N", switch = "-s", min = 0, max = 6000, tip = "Smooth count: smooth onset errors & glitches in pitchdata, N times" },
  arg8 = { name = "f1", switch = "-b", min = 0, max = 22050, def = 5000, tip = "Start frequency: force start frequency to be f1" },
  arg9 = { name = "f2", switch = "-e", min = 0, max = 22050, def = 7500, tip = "End frequency: force end frequency to be f2" },
  arg10 = { name = "-w", switch = "-w", tip = "removes 2-window glitches (Default: 1-window) (Use ONLY with the -s paramter)" },
  arg11 = { name = "-i", switch = "-i", tip = "interpolate through ALL non-pitch windows in the pitch data, producing pitch everywhere" },
}
 
dsp["Repitch Generate - Create binary pitchdata from a textfile of time midi value pairs - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "generate", tip = "REPITCH GENERATE allows you to generate a binary pitchdata file .frq by directly typing in MIDI or musical note name information. Note that this is the same type of file as that produced by REPITCH GETPITCH. The difference is that with REPITCH GENERATE you are designing your own pitch trace by specifying the (changing) pitch levels using musician-friendly data entry, and with REPITCH GETPITCH the pitch levels are extracted automatically from the input analysis file." },
  arg1 = { name = "Output.frq", output = "frq", tip = "binary pitchdata file produced by the program" },
  arg2 = { name = "Midi Notes", input = "txt", tip = "a text file containing a list of paired time and midi-note-values" },
  arg3 = { name = "Samplerate", input = "string", def = "44100", tip = "the sample rate of the soundfile that might later be generated from the binary pitch data" },
}
 
dsp["Repitch Getpitch 2 - Extracts a pitch trace - Output a .brk file"] = {
  cmds = { exe = "repitch", mode = "getpitch 2", tip = "This is the key program in the REPITCH Group. It extracts a pitch trace from analysis data, producing a binary pitch data file (.frq), which can then be massaged by the other REPITCH functions. A number of these functions handle noise or silent data in the the .frq file. The other key program is COMBINE/B, which can combine binary pitch data files (i.e., pitch traces) to produce transposition data etc." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "Output.brk", output = "brk", },
  arg4 = { name = "TRange", switch = "-t", min = 0, max = 6, def = 1, tip = "Tuning range(semitones) within which harmonics are accepted as in tune" },
  arg5 = { name = "MinWin", switch = "-g", min = 0, max = 977, def = 2, tip = "Minimum number of adjacent windows that must be pitched, for a pitch-value to be registered" },
  arg6 = { name = "SNR", switch = "-s", min = 0, max = 1000, def = 80, tip = "Signal to noise ratio, in decibels - Windows which are more than SdB below maximum level in sound, are assumed to be noise, and any detected pitch value is assumed spurious" },
  arg7 = { name = "Harmonics", switch = "-n", min = 1, max = 8, def = 5, tip = "How many of the 8 loudest peaks in spectrum must be harmonics to confirm sound is pitched" },
  arg8 = { name = "Low Freq", switch = "-l", min = 10, max = 2756, def = 10, tip = "Frequency of LOWEST acceptable pitch" },
  arg9 = { name = "Top Freq", switch = "-h", min = 10, max = 2756, def = 2756, tip = "Frequency of TOPMOST acceptable pitch" },
  arg10 = { name = "Error", switch = "-i", min = 0, max = 12, def = 0.25, tip = "Acceptable pitch-ratio error in data reduction (semitones)" },
  arg11 = { name = "Alt Pitch", switch = "-a", tip = "Alternative pitch-finding algorithm (avoid N < 2)" },
}
 
dsp["Repitch InsertSil 1 – Mark areas as silent in a pitchdata file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "insertsil 1", tip = "This function enables you to replace questionable pitch information with silence. When the binary pitch data file (the 'pitch trace') is created with REPITCH GETPITCH, the software is examining the analysis bins for definable pitch content. This is not infrequently rather ambiguous, leading to spurious artefacts in the output, such as noise components. By default, REPITCH GETPITCH smooths over these places with an interpolation function, but you can set an option retain the unpitched windows (-z flag), which makes it possible for REPITCH INSERTSIL to distinguish between pitch and noise. It then goes through the file looking for clearly recognisable pitch data, retains this and replaces the 'questionable' material with silence. Suppose you had extracted the pitch trace (.frq) from some complex soundstream (A) containing pitch, noise and undecidable material, such as a sequence of avant-garde sounds from a string or brass instrument with noisy ormultiphonic effects. Then you also make a formant analysis (.for) – this could in fact be from another source (B). If you use COMBINE MAKE to combine the pitch (.frq) and formant data (.for) to make a new sound via the new analysis file produced, you will probably discover some rather unpleasant artefacts where the original signal (A) did not produce reliable pitch data. This applies even if the input is a perfectly acceptable harmonic sound but uses more than one pitch. You can use this program to mask out the bits that are a bit dubious, i.e., you don't get the pitch or pitchiness you expect, in order to achieve a file of (say) pure pitch and silence. You can then recombine the pitch and formant data and the silent sections in the pitch file will be silent in the output. Thus only sound for which you have reliable data will be resynthesized in the output." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Silence Data", input = "txt", tip = "text file specifying where to mask the unsatisfactory analysis data" },
}
 
dsp["Repitch InsertSil 2 – Mark areas as silent in a pitchdata file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "insertsil 2", tip = "This function enables you to replace questionable pitch information with silence. When the binary pitch data file (the 'pitch trace') is created with REPITCH GETPITCH, the software is examining the analysis bins for definable pitch content. This is not infrequently rather ambiguous, leading to spurious artefacts in the output, such as noise components. By default, REPITCH GETPITCH smooths over these places with an interpolation function, but you can set an option retain the unpitched windows (-z flag), which makes it possible for REPITCH INSERTSIL to distinguish between pitch and noise. It then goes through the file looking for clearly recognisable pitch data, retains this and replaces the 'questionable' material with silence. Suppose you had extracted the pitch trace (.frq) from some complex soundstream (A) containing pitch, noise and undecidable material, such as a sequence of avant-garde sounds from a string or brass instrument with noisy ormultiphonic effects. Then you also make a formant analysis (.for) – this could in fact be from another source (B). If you use COMBINE MAKE to combine the pitch (.frq) and formant data (.for) to make a new sound via the new analysis file produced, you will probably discover some rather unpleasant artefacts where the original signal (A) did not produce reliable pitch data. This applies even if the input is a perfectly acceptable harmonic sound but uses more than one pitch. You can use this program to mask out the bits that are a bit dubious, i.e., you don't get the pitch or pitchiness you expect, in order to achieve a file of (say) pure pitch and silence. You can then recombine the pitch and formant data and the silent sections in the pitch file will be silent in the output. Thus only sound for which you have reliable data will be resynthesized in the output." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Silence Data", input = "txt", tip = "text file specifying where to mask the unsatisfactory analysis data" },
}
 
dsp["Repitch InsertZeros 1 – Mark areas as unpitched in a pitchdata file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "insertzeros 1", tip = "REPITCH INSERTZEROS indicates where no pitch is detected in the binary pitch data file. No pitch means that there is actual noise or signal too complex to extract the pitch successfully. Note that this process is different from INSERTSIL which indicates where there is no signal in the file." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Zeros Data", input = "txt", tip = "a list of pairs of times between which unpitched data is to be inserted" },
}
 
dsp["Repitch InsertZeros 2 – Mark areas as unpitched in a pitchdata file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "insertzeros 2", tip = "REPITCH INSERTZEROS indicates where no pitch is detected in the binary pitch data file. No pitch means that there is actual noise or signal too complex to extract the pitch successfully. Note that this process is different from INSERTSIL which indicates where there is no signal in the file." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Zeros Data", input = "txt", tip = "a list of pairs of times between which unpitched data is to be inserted" },
}
 
dsp["Repitch Interp 1 - Glides from a previous valid pitch to the next valid pitch - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "interp 1", tip = "1 Glides from a previous valid pitch to the next valid pitch. When you use REPITCH GETPITCH to extract the pitch data from a source with interpolation turned off (option to 'Retain unpitched windows' / 'Keep pitch zeros'), the resulting binary pitch data file (.frq) will keep a record of any unpitched (noise) or silent sections. unpitched means noise or waveforms too complex to deduce pitch successfully, silent means no significant signal is detected. If you decide not to keep this information (i.e. you prefer the pitch data to be continuously pitched, for some musical purpose, rather than to contain gaps) REPITCH INTERP will cause the pitch data to interpolate across the unpitched gaps. Using one or the other Mode, you can do this in two ways: either you create pitch glissandi across the unpitched areas, moving ('gliding') from the pitch before the noise or silence to the pitch after it, or you sustain the pitch before the noise or silence until you reach the pitch after it. There are also facilities within the Sound Loom Pitch Editor to smooth across areas of noise or silence manually. Musical applications; This is particularly important when working with two different files in which continuity of pitch is important. For example, REPITCH COMBINE in Mode 1 enables you to combine two different pitch traces, creating a transposition file (.trn) that 'contains the difference between the two input pitch data files translated into the transposition ratios needed to move from the pitch shape of the first file to the pitch shape of the second file'. In other words, you are imposing the pitch contour of file 2 onto file 1. If, for example, you have recorded two instruments one or both of which is playing staccato, there will be silent gaps in your pitch files. Without interpolation, these gaps will make it impossible for the pitch traces to interact properly, unless they were perfectly in sync." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
}
 
dsp["Repitch Interp 2 - Sustains the previous valid pitch until the next valid pitch appears - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "interp 2", tip = "2 Sustains the previous valid pitch until the next valid pitch appears. When you use REPITCH GETPITCH to extract the pitch data from a source with interpolation turned off (option to 'Retain unpitched windows' / 'Keep pitch zeros'), the resulting binary pitch data file (.frq) will keep a record of any unpitched (noise) or silent sections. unpitched means noise or waveforms too complex to deduce pitch successfully, silent means no significant signal is detected. If you decide not to keep this information (i.e. you prefer the pitch data to be continuously pitched, for some musical purpose, rather than to contain gaps) REPITCH INTERP will cause the pitch data to interpolate across the unpitched gaps. Using one or the other Mode, you can do this in two ways: either you create pitch glissandi across the unpitched areas, moving ('gliding') from the pitch before the noise or silence to the pitch after it, or you sustain the pitch before the noise or silence until you reach the pitch after it. There are also facilities within the Sound Loom Pitch Editor to smooth across areas of noise or silence manually. Musical applications; This is particularly important when working with two different files in which continuity of pitch is important. For example, REPITCH COMBINE in Mode 1 enables you to combine two different pitch traces, creating a transposition file (.trn) that 'contains the difference between the two input pitch data files translated into the transposition ratios needed to move from the pitch shape of the first file to the pitch shape of the second file'. In other words, you are imposing the pitch contour of file 2 onto file 1. If, for example, you have recorded two instruments one or both of which is playing staccato, there will be silent gaps in your pitch files. Without interpolation, these gaps will make it impossible for the pitch traces to interact properly, unless they were perfectly in sync." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
}
 
dsp["Repitch Invert 1 - Invert pitch contour of a pitch data file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "invert 1", tip = " The no mapping option (map set to 0) will give a mirror inversion of the original contour, i.e., all intervals will be identical, but in the opposite direction. This is fine for various styles, such as atonal and serial music, but may not be what is wanted in a tonal context. Here the inversion often needs to remain within the same scale or 'key'. The mapping function makes it possible to specify what's needed here. The Mode 1 output (binary pitch data file) can be fed back into another pitch modification process. The Mode 2 output (binary transposition file) can be given as input to TRANSPOSE/TRANSPOSEF Mode 4 to create an analysis file which can be resynthesised: i.e., to complete the process. Musical applications; Inversion is a very basic and universal musical technique. It is a way of extending material, introducing something new while retaining a good part of what is already familiar. Here in the spectral domain, there will also be timbral alterations as a result of the inversion. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "map", min = 0, max = 0, input = "brk", def = 0, tip = "set map to 0 if no mapping is required. Otherwise, map is a text file of paired values showing how intervals expressed in (possibly fractional) semitones are to be mapped onto their inversions." },
  arg4 = { name = "meanpitch", switch = "-m", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitch expressed as a MIDI note value around which the pitch line is to be inverted" },
  arg5 = { name = "bot", switch = "-b", min = 0, max = 1000, def = 0, tip = "bottom pitch (MIDI note value) permissible (Default: 0; Range: MIDI equivalents of 9Hz to Nyquist)" },
  arg6 = { name = "top", switch = "-t", min = 0, max = 1000, def = 127, tip = "top pitch (MIDI note value) permissible (Default: 127; Range: MIDI equivalents of 9Hz to Nyquist)" },
}
 
dsp["Repitch Invert 2 - Invert pitch contour of a pitch data file - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "invert 1", tip = " The no mapping option (map set to 0) will give a mirror inversion of the original contour, i.e., all intervals will be identical, but in the opposite direction. This is fine for various styles, such as atonal and serial music, but may not be what is wanted in a tonal context. Here the inversion often needs to remain within the same scale or 'key'. The mapping function makes it possible to specify what's needed here. The Mode 1 output (binary pitch data file) can be fed back into another pitch modification process. The Mode 2 output (binary transposition file) can be given as input to TRANSPOSE/TRANSPOSEF Mode 4 to create an analysis file which can be resynthesised: i.e., to complete the process. Musical applications; Inversion is a very basic and universal musical technique. It is a way of extending material, introducing something new while retaining a good part of what is already familiar. Here in the spectral domain, there will also be timbral alterations as a result of the inversion. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "map", min = 0, max = 0, input = "brk", def = 0, tip = "set map to 0 if no mapping is required. Otherwise, map is a text file of paired values showing how intervals expressed in (possibly fractional) semitones are to be mapped onto their inversions." },
  arg4 = { name = "meanpitch", switch = "-m", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitch expressed as a MIDI note value around which the pitch line is to be inverted" },
  arg5 = { name = "bot", switch = "-b", min = 0, max = 1000, def = 0, tip = "bottom pitch (MIDI note value) permissible (Default: 0; Range: MIDI equivalents of 9Hz to Nyquist)" },
  arg6 = { name = "top", switch = "-t", min = 0, max = 1000, def = 127, tip = "top pitch (MIDI note value) permissible (Default: 127; Range: MIDI equivalents of 9Hz to Nyquist)" },
}
 
dsp["Repitch Noisetosil - Replace unpitched windows by silence - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "noisetosil", tip = "When you've made a pitch-extraction of a real sound, it will have moments of pitch, moments of silence, and moments when it can't decide what the pitch is but knows it does not have silence. It marks these indeterminate areas as 'noise'. They may be noise in the conventional sense, or complex tones where a single pitch cannot be determined. If your original source contains pitched and unpitched (noise) elements, REPITCH NOISETOSIL allows you to make a 'pitch' file where all the unpitched (noise) data is eliminated. The binary pitch data file (.frq) is made with REPITCH GETPITCH. Its default mode sets pitch extraction so that it interpolates the pitch over any unpitched (noise) material. In this way you get a continuously pitched readout from a sound that may not be continuously pitched. Therefore, to make use of REPITCH NOISETOSIL and the related functions in REPITCH (INSERTSIL and INTERP), you need to set the -z flag in REPITCH GETPITCH so that interpolation does NOT take place, and the noise components are retained. REPITCH NOISETOSIL then replaces these noise components in a .frq file with silence, i.e., leaving pauses without breaking the pitch continuity of the sound. Musical applications; This contour with pauses where the noise components were is compositionally useful because it allows you transfer e.g., the pitch contour of speech onto another sound (e.g. a flute) without having to worry about the unpitched areas in the speech, and still obtain a continuous line. With NOISETOSIL (replace noise by silence), your flute output will have pauses where noise (e.g. sibilants) occur in the original speech. This is quite a different result than interpolating in REPITCH GETPITCH when extracting the pitch trace, because then the pitch contour is continuous, without pauses where the noise components occurred. Again, you may well want to skip these undecidable areas in reconstructing a sound, such as when combining with formants in COMBINE MAKE or in COMBINE MAKE2. REPITCH NOISETOSIL therefore enables you to mask out the noise. You can reconstruct sounds with the noise left in if you would like to, but REPITCH NOISETOSIL provides an option because at present, reconstruction of signals from noise and format data does not work ideally (especially for speech sibilants)." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
}
 
dsp["Repitch PchToText - Convert binary pitch data .frq to text file"] = {
  cmds = { exe = "repitch", mode = "pchtotext", tip = "The binary pitch data is encapsulating the pitch contour of a sound. In its binary format it can be manipulated by most of the REPITCH functions, including combining with other binary pitch data files so that the contour of one file is used to reshape that of another. Once this data is in text format, it can not only be altered 'by hand' but also by the many numerical facilities provided in COLUMNS, such as adding a random value between min and max limits to each number in the file (i.e., column of numbers), thus randomising the pitch trace of a sound. The result can be reapplied to the original sound or to another sound. Given the range of operations in COLUMNS, the potential for reshaping pitch contours is enormous." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.txt", output = "txt", tip = "Select an output .txt file" },
}
 
dsp["Repitch Pchshift - Transpose pitches in a binary pitch data file by a constant number of semitones (becomes inharmonic) - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "pchshift", tip = "This is a straightforward upward or downward shift by a constant amount, in this case applied directly to a binary pitch data file (.frq), as made by REPITCH GETPITCH, or even by REPITCH COMBINE, Mode 2. Musical applications; As part of REPITCH, the intention here is to provide a function to change the pitch of a binary pitch data file prior to further processing with another REPITCH function. The output of REPITCH PCHSHIFT is a binary pitch data file (.frq) with the requested transposition. This .frq output must then be run through REPITCH COMBINE to create a transposition (.trn) file proper. It can be combined with the pitch trace (.frq) from the original analysis file (.ana) used with PCHSHIFT or with a pitch trace from some other analysis file (.ana). Note how a transposed pitch trace from one file can be applied to another one, an example of cross-file processing. The sound is then made with REPITCH TRANSPOSE or REPITCH TRANSPOSEF, using the .trn output of REPITCH COMBINE as the input. Though the use of all these different functions may seem cumbersome, the purpose is to provide opportunities for cross-file processing. For straightforward transposition in the spectral domain – transposition without change of duration – you can use the other modes of REPITCH TRANSPOSE and REPITCH TRANSPOSEF, which allow the use of both transposition constants and time-varying files." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "transposition", min = -1000, max = 1000, def = 0, tip = "amount of (constant) transposition in (fractions of) semitones; negative numbers transpose downwards" },
}
 
dsp["Repitch Pitchtosil - Replace pitched windows by silence - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "pitchtosil", tip = "The opposite of REPITCH NOISETOSIL, REPITCH PITCHTOSIL may be useful if you want to do something different with the noise segments of the speech – because in this case the pitch components become pauses, leaving only the noise components. Again, it is important that the binary pitch data file extracted via REPITCH GETPITCH retains the unpitched material so that REPITCH PITCHTOSIL can separate out the pitch from the noise. Note that retaining the unpitched material is the option (which would need to be selected), and interpolating across the unpitched material (keeping the pitch contour going) is the default when running REPITCH GETPITCH. Musical applications; The spoken word contains vowels and consonants. The vowels tend to be pitched to some degree, while the consonants contain noise. With REPITCH PITCHTOSIL the vowel content is removed, leaving the consonants. The application comes when combining this consonants-only pitch data file (the essential components of speech) with a formant file to generate a new, speech-like sound. See COMBINE MAKE and COMBINE MAKE2. REPITCH COMBINE offers the possibility of other types of file combinations to make new spectra." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
}
 
dsp["Repitch Quantise 1 - Quantise pitches in a pitch data file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "quantise 1", tip = "Operating on extracted pitch contour data saved as a binary pitch data file, this function 'snaps' the pitches of the contour to the intervallic grid specified in q_set. Note that this grid can be defined for a single octave and then duplicated in all the octaves. The pitch or transposition outfile can be used by other REPITCH functions as appropriate." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "Q Set", input = "txt", tip = "a file of (possibly fractional) MIDI pitch values over which the pitch is to be quantised" },
  arg4 = { name = "-o", switch = "-o", tip = "duplicates q_set in all octave transpositions" },
}
 
dsp["Repitch Quantise 2 - Quantise pitches in a pitch data file - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "quantise 2", tip = "Operating on extracted pitch contour data saved as a binary pitch data file, this function 'snaps' the pitches of the contour to the intervallic grid specified in q_set. Note that this grid can be defined for a single octave and then duplicated in all the octaves. The pitch or transposition outfile can be used by other REPITCH functions as appropriate." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "Q Set", input = "txt", tip = "a file of (possibly fractional) MIDI pitch values over which the pitch is to be quantised" },
  arg4 = { name = "-o", switch = "-o", tip = "duplicates q_set in all octave transpositions" },
}
 
dsp["Repitch Randomise 1 - Randomise pitch line - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "randomise 1", tip = "The provision of both maxinterval and timestep parameters provide ways to vary a process which is inherently unpredictable. The Mode 1 output (binary pitch data file) can be fed back into another pitch modification process. The Mode 2 output (binary transposition file) can be given as input to TRANSPOSE/TRANSPOSEF Mode 4 to create an analysis file which can be resynthesised: i.e., to complete the process. Musical applications; With this process you can produce strange and unpredictable pitch movements/distortions. It can only be used on an experimental basis while searching for wierd effects." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "maxinterval", min = 0, max = 96, input = "brk", def = 0, tip = "number of semitones over which the pitches can plus or minus randomly vary (Range: 0 to 96; single value or breakpoint file)" },
  arg4 = { name = "timestep", switch = "-m", min = 0, max = length, input = "brk", tip = "maximum timestep in milliseconds between random pitch fluctuations." },
  arg5 = { name = "slew", switch = "-s", min = -1000, max = 1000, def = 0, tip = "e.g. 2: the range of the upward variation will be twice that of the downward variation e.g. -3: the range of the downward variation will be 3 times that of the upward variation. (Range: > 1 OR < -1)" },
}
 
dsp["Repitch Randomise 2 - Randomise pitch line - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "randomise 1", tip = "The provision of both maxinterval and timestep parameters provide ways to vary a process which is inherently unpredictable. The Mode 1 output (binary pitch data file) can be fed back into another pitch modification process. The Mode 2 output (binary transposition file) can be given as input to TRANSPOSE/TRANSPOSEF Mode 4 to create an analysis file which can be resynthesised: i.e., to complete the process. Musical applications; With this process you can produce strange and unpredictable pitch movements/distortions. It can only be used on an experimental basis while searching for wierd effects." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "maxinterval", min = 0, max = 96, input = "brk", def = 0, tip = "number of semitones over which the pitches can plus or minus randomly vary (Range: 0 to 96; single value or breakpoint file)" },
  arg4 = { name = "timestep", switch = "-m", min = 0, max = length, input = "brk", tip = "maximum timestep in milliseconds between random pitch fluctuations." },
  arg5 = { name = "slew", switch = "-s", min = -1000, max = 1000, def = 0, tip = "e.g. 2: the range of the upward variation will be twice that of the downward variation e.g. -3: the range of the downward variation will be 3 times that of the upward variation. (Range: > 1 OR < -1)" },
}
 
dsp["Repitch Smooth 1 - Smooth pitch contour in a pitch data file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "smooth 1", tip = "The smoothing operation is applied to the pitch contour. If the time frame is 8 windows, REPITCH SMOOTH takes the average pitch of those 8 windows, then interpolates to the average pitch of the next 8 windows. This is the PITCH equivalent of BLUR BLUR.With the -p flag set, it takes the peak value (the highest above the specified mean, or the lowest below the specified mean), rather than the average pitch, in each window, and interpolates as before. Musical applications; The averaging/interpolating of pitch data process will produce results which will vary according to the spectral content of the infile, so the degree of pitch focus and differentiation in the source will greatly affect the result. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "timeframe", min = 0, max = length, input = "brk", def = 0, tip = "number of milliseconds over which to interpolate pitch values (Range: duration of 1 analysis window to the duration of the entire file)" },
  arg4 = { name = "meanpitch", switch = "-p", min = 0, max = 10000, input = "brk", tip = "interpolate between the peak value in each timeframe block of pitch values" },
  arg5 = { name = "-h", switch = "-h", tip = "At the end of the file, hold the last interpolated pitch value calculated" },
}
 
dsp["Repitch Smooth 2 - Smooth pitch contour in a pitch data file - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "smooth 2", tip = "The smoothing operation is applied to the pitch contour. If the time frame is 8 windows, REPITCH SMOOTH takes the average pitch of those 8 windows, then interpolates to the average pitch of the next 8 windows. This is the PITCH equivalent of BLUR BLUR.With the -p flag set, it takes the peak value (the highest above the specified mean, or the lowest below the specified mean), rather than the average pitch, in each window, and interpolates as before. Musical applications; The averaging/interpolating of pitch data process will produce results which will vary according to the spectral content of the infile, so the degree of pitch focus and differentiation in the source will greatly affect the result. " },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "timeframe", min = 0, max = length, input = "brk", def = 0, tip = "number of milliseconds over which to interpolate pitch values (Range: duration of 1 analysis window to the duration of the entire file)" },
  arg4 = { name = "meanpitch", switch = "-p", min = 0, max = 10000, input = "brk", tip = "interpolate between the peak value in each timeframe block of pitch values" },
  arg5 = { name = "-h", switch = "-h", tip = "At the end of the file, hold the last interpolated pitch value calculated" },
}
 
dsp["Repitch Transpose - 1 Transposition as a frequency ratio - Transpose spectrum (spectral envelope also moves) "] = {
  cmds = { exe = "repitch", mode = "transpose 1", tip = "1 Transposition as a frequency ratio - REPITCH TRANSPOSE does fixed or time varying transposition in the spectral domain. It accepts a variety of breakpoint file inputs as well as a binary data file. The spectral envelope moves with the transposition (as opposed to TRANSPOSEF, where it does not), meaning that the timbres associated with formants are going to change. Musical applications; The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract a pitch contour with REPITCH GETPITCH, saving as a breakpoint (.brk) file and use this output to combine with another pitch contour or transposition file in COMBINE/B to form another (time varying) transposition file to apply to an infile. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSE in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSE. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. Modify pitchfile data in some way, save as a transposition file and use as an input to TRANSPOSE/F. The following REPITCH pitchfile modification functions have a transposition file output mode: APPROX, EXAG, INVERT, QUANTISE, RANDOMISE, SMOOTH and VIBRATO. The results of these transpositions can be used to alter timbre, especially with TRANSPOSE. Also, some use of transposition could form an important step when realising a morph: to draw one sound closer to that of another in frequency region or timbre, or to create a aurally transitional stage in the morphing process" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "transpos", min = 0.00383, max = 256, input = "brk", def = 2, tip = "a transposition factor or time varying breakpoint file in a format according with the mode being used" },
  arg4 = { name = "minfrq", switch = "-l", min = 0, max = 22050, def = 50, tip = "minimum frequency, below which data is filtered out" },
  arg5 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, def = 10000, tip = "maximum frequency, above which data is filtered out" },
  arg6 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch Transpose - 2 Transposition in (fractions of) octaves - Transpose spectrum (spectral envelope also moves) "] = {
  cmds = { exe = "repitch", mode = "transpose 2", tip = "2 Transposition in (fractions of) octaves - REPITCH TRANSPOSE does fixed or time varying transposition in the spectral domain. It accepts a variety of breakpoint file inputs as well as a binary data file. The spectral envelope moves with the transposition (as opposed to TRANSPOSEF, where it does not), meaning that the timbres associated with formants are going to change. Musical applications; The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract a pitch contour with REPITCH GETPITCH, saving as a breakpoint (.brk) file and use this output to combine with another pitch contour or transposition file in COMBINE/B to form another (time varying) transposition file to apply to an infile. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSE in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSE. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. Modify pitchfile data in some way, save as a transposition file and use as an input to TRANSPOSE/F. The following REPITCH pitchfile modification functions have a transposition file output mode: APPROX, EXAG, INVERT, QUANTISE, RANDOMISE, SMOOTH and VIBRATO. The results of these transpositions can be used to alter timbre, especially with TRANSPOSE. Also, some use of transposition could form an important step when realising a morph: to draw one sound closer to that of another in frequency region or timbre, or to create a aurally transitional stage in the morphing process" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "transpos", min = -8, max = 8, input = "brk", def = 2, tip = "a transposition factor or time varying breakpoint file in a format according with the mode being used" },
  arg4 = { name = "minfrq", switch = "-l", min = 0, max = 22050, def = 50, tip = "minimum frequency, below which data is filtered out" },
  arg5 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, def = 10000, tip = "maximum frequency, above which data is filtered out" },
  arg6 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch Transpose - 3 Transposition in (fractions of) semitones - Transpose spectrum (spectral envelope also moves) "] = {
  cmds = { exe = "repitch", mode = "transpose 3", tip = "3 Transposition in (fractions of) semitones - REPITCH TRANSPOSE does fixed or time varying transposition in the spectral domain. It accepts a variety of breakpoint file inputs as well as a binary data file. The spectral envelope moves with the transposition (as opposed to TRANSPOSEF, where it does not), meaning that the timbres associated with formants are going to change. Musical applications; The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract a pitch contour with REPITCH GETPITCH, saving as a breakpoint (.brk) file and use this output to combine with another pitch contour or transposition file in COMBINE/B to form another (time varying) transposition file to apply to an infile. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSE in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSE. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. Modify pitchfile data in some way, save as a transposition file and use as an input to TRANSPOSE/F. The following REPITCH pitchfile modification functions have a transposition file output mode: APPROX, EXAG, INVERT, QUANTISE, RANDOMISE, SMOOTH and VIBRATO. The results of these transpositions can be used to alter timbre, especially with TRANSPOSE. Also, some use of transposition could form an important step when realising a morph: to draw one sound closer to that of another in frequency region or timbre, or to create a aurally transitional stage in the morphing process" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "transpos", min = -96, max = 96, input = "brk", def = 2, tip = "a transposition factor or time varying breakpoint file in a format according with the mode being used" },
  arg4 = { name = "minfrq", switch = "-l", min = 0, max = 22050, def = 50, tip = "minimum frequency, below which data is filtered out" },
  arg5 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, def = 10000, tip = "maximum frequency, above which data is filtered out" },
  arg6 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch TransposeF - 1 Transposition as a frequency ratio - Transpose spectrum: but retain original spectral envelope"] = {
  cmds = { exe = "repitch", mode = "transposef 1", tip = "1 Transposition as a frequency ratio - For this process to work, you need to enable one of the N toggles first! Here, formants are preserved (see TRANSPOSE, where they are not). Thus, for example, the vowel characteristics of the original source should be preserved. Musical applications; This process may be used when one wants to retain the original sound as much as possible, though in a different register. On the other hand, it creates an interesting play between the unfamiliar (appearing in a different, and perhaps unnatural, register), and the familiar (original formants are preserved). The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSEF in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSEF. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "-fN", switch = "-f", min = 1, max = 256, def = 2, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally spaced frequency channels" },
  arg4 = { name = "-pN", switch = "-p", min = 0, max = 12, def = 2, tip = "extract formant envelope linear pitchwise, using N equally spaced pitch bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "transpos", min = 0, max = 256, input = "brk", def = 2, tip = "transposition factor or time varying breakpoint file, with data in a form according with one of the four modes." },
  arg7 = { name = "minfrq", switch = "-l", min = 0, max = 22050, input = "brk", def = 50, tip = "fuller spectrum" },
  arg8 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, input = "brk", def = 10000, tip = "fuller spectrum" },
  arg9 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch TransposeF - 2 Transposition in (fractions of) octaves - Transpose spectrum: but retain original spectral envelope"] = {
  cmds = { exe = "repitch", mode = "transposef 2", tip = "2 Transposition in (fractions of) octaves - For this process to work, you need to enable one of the N toggles first! Here, formants are preserved (see TRANSPOSE, where they are not). Thus, for example, the vowel characteristics of the original source should be preserved. Musical applications; This process may be used when one wants to retain the original sound as much as possible, though in a different register. On the other hand, it creates an interesting play between the unfamiliar (appearing in a different, and perhaps unnatural, register), and the familiar (original formants are preserved). The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSEF in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSEF. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "-fN", switch = "-f", min = 1, max = 256, def = 2, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally spaced frequency channels" },
  arg4 = { name = "-pN", switch = "-p", min = 0, max = 12, def = 2, tip = "extract formant envelope linear pitchwise, using N equally spaced pitch bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "transpos", min = 0, max = 256, input = "brk", def = 2, tip = "transposition factor or time varying breakpoint file, with data in a form according with one of the four modes." },
  arg7 = { name = "minfrq", switch = "-l", min = 0, max = 22050, input = "brk", def = 50, tip = "fuller spectrum" },
  arg8 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, input = "brk", def = 10000, tip = "fuller spectrum" },
  arg9 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch TransposeF - 3 Transposition in (fractions of) semitones - Transpose spectrum: but retain original spectral envelope"] = {
  cmds = { exe = "repitch", mode = "transposef 3", tip = "3 Transposition in (fractions of) semitones - For this process to work, you need to enable one of the N toggles first! Here, formants are preserved (see TRANSPOSE, where they are not). Thus, for example, the vowel characteristics of the original source should be preserved. Musical applications; This process may be used when one wants to retain the original sound as much as possible, though in a different register. On the other hand, it creates an interesting play between the unfamiliar (appearing in a different, and perhaps unnatural, register), and the familiar (original formants are preserved). The process of transposition can be approached in various ways: Transpose a file by a constant amount. Create a transposition breakpoint file by hand and apply it to any sound. Extract pitch contours from two files with REPITCH GETPITCH. Then create a transposition file with REPITCH COMBINE/COMBINEB Mode 1 which contains the information needed to change the pitch contour of the first file into that of the second. Then apply this transposition data to the first file here with TRANSPOSEF in order to effect the transposition. Similarly the output of COMBINE/COMBINEB Mode 3 can be used as the transposition input to TRANSPOSEF. Summing transposition data can be an interestingly serendipitous (unpredictable) exercise. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "-fN", switch = "-f", min = 1, max = 256, def = 2, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally spaced frequency channels" },
  arg4 = { name = "-pN", switch = "-p", min = 0, max = 12, def = 2, tip = "extract formant envelope linear pitchwise, using N equally spaced pitch bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "transpos", min = 0, max = 256, input = "brk", def = 2, tip = "transposition factor or time varying breakpoint file, with data in a form according with one of the four modes." },
  arg7 = { name = "minfrq", switch = "-l", min = 0, max = 22050, input = "brk", def = 50, tip = "fuller spectrum" },
  arg8 = { name = "maxfrq", switch = "-h", min = 0, max = 22050, input = "brk", def = 10000, tip = "fuller spectrum" },
  arg9 = { name = "-x", switch = "-x", tip = "fuller spectrum" },
}
 
dsp["Repitch Vibrato 1 - Add vibrato to pitch in a pitch data file - Output a .frq file"] = {
  cmds = { exe = "repitch", mode = "vibrato 1", tip = "This operates like a LFO (low frequency oscillator). Vibfreq is the speed of oscillation and vibrange is the depth (interval swing) of the oscillation." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.frq", output = "frq", tip = "Select an output .frq file" },
  arg3 = { name = "vibfreq", min = 0, max = 10000, input = "brk", def = 10, tip = "frequency of vibrato itself in Hz (Range: values > 0)" },
  arg4 = { name = "vibrange", min = 0, max = 10000, input = "brk", def = 1, tip = "maximum interval over which the vibrato moves away from the central pitch (in semitones) (Range: values > 0)" },
}
 
dsp["Repitch Vibrato 2 - Add vibrato to pitch in a pitch data file - Output a .trn file"] = {
  cmds = { exe = "repitch", mode = "vibrato 1", tip = "This operates like a LFO (low frequency oscillator). Vibfreq is the speed of oscillation and vibrange is the depth (interval swing) of the oscillation." },
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output.trn", output = "trn", tip = "Select an output .trn file" },
  arg3 = { name = "vibfreq", min = 0, max = 10000, input = "brk", def = 10, tip = "frequency of vibrato itself in Hz (Range: values > 0)" },
  arg4 = { name = "vibrange", min = 0, max = 10000, input = "brk", def = 1, tip = "maximum interval over which the vibrato moves away from the central pitch (in semitones) (Range: values > 0)" },
}

dsp["Repitch Synth - Create the spectrum of a sound following the pitch contour in the pitch file"] = { 
  cmds = { exe = "repitch", mode = "synth", input = "data_to_pvoc", tip = "REPITCH SYNTH synthesises the spectrum of a sound while following the pitch trace in the input pitch data file. Once the pitch contour of a sound (e.g. a stream of speech, with a complex changing pitchline) has been extracted, new sounds can be generated which follow the same pitchline. Furthermore, the pitchline can be modified (inverted, quantised etc.) using the other pitch data file manipulation programs in REPITCH and then the new spectrum added to it." }, 
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output", output = "ana", tip = "Select an output sound" },
  arg3 = { name = "Harmonics", input = "txt", tip = "A text file that just lists the amplitudes of each harmonic in sequence, from 1 upwards. These amplitude values must lie in the range 0 to 1." },
}

dsp["Repitch Vowels - Create spectrum of vowel sound(s) following pitch contour in pitch file"] = { 
  cmds = { exe = "repitch", mode = "vowels", input = "data_to_pvoc", tip = "This process allows a sequence of vowels to be synthesized over a pitch-contour extracted from a source sound. Musical applications; You can combine pitch data (.frq) from one speaker with formant data (.for) from another ( COMBINE MAKE). In contrast, REPITCH VOWELS allows you to define the vowel (formant) data from scratch, by writing out the vowels in a breakpoint file, before combining them with the binary pitch data. The overall effect is the semblance of speech added to a sound." }, 
  arg1 = { name = "Input.frq", input = "frq", tip = "Select an input .frq file" },
  arg2 = { name = "Output", output = "ana", tip = "Select an output sound" },
  arg3 = { name = "vowel-data", input = "txt", tip = "text file containg a vowel, OR a file of paired time vowel values. See docs for the strings" },
  arg4 = { name = "halfwidth", min = 0.01, max = 10,  input = "brk", tip = "the half-width of a formant, in Hz, as a fraction of the formant centre-frequency. Range: 0.01 to 10" },
  arg5 = { name = "curve", min = 0.1, max = 10, input = "brk", tip = "the steepness of the formant peak. Range: 0.1 to 10.0" },
  arg6 = { name = "pk_range", min = 0, max = 1, input = "brk", tip = "the ratio of the (maximum) amplitude range of the formant peaks to the (maximum) total range. Range: 0 to 1" },
  arg7 = { name = "fweight", min = 0, max = 1, input = "brk", tip = "the amplitude weighting of the fundamental. Range: 0 to 1" },
  arg8 = { name = "foffset", min = 0, max = 1, input = "brk", tip = "the range of scattering of the frequencies of the harmonics from their true value. Range: 0 to 1" },
}
 
------------------------------------------------
-- retime
------------------------------------------------
 
dsp["Retime 1 - Rearrange and retime events within a soundfilet - Specify the times of peaks in the input text file. Output these at a regular pulse in the given tempo"] = {
  cmds = { exe = "retime", mode = "retime 1", channels = "any", tip = "1 - Specify the times of peaks in the input. Output these at a regular pulse in the given tempo  Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "refpoints", input = "txt", tip = "times of peaks in the input sound file which will become on-the-beat events in the output" },
  arg4 = { name = "tempo", min = 0, max = 400, tip = "tempo of the output soundfile, as a Metronome Mark (MM) for values > 20, OR as a beat duration for values less than 1." },
}

--[[ mode 2 not available on the command line, only in soundloom --]]
 
dsp["Retime 10 - Rearrange and retime events within a soundfilet. Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 10", channels = "any", tip = "10 - Adjust the levels of events in the input sound, changing the pattern of accents. This process can be used to change the accenting pattern of the input events. It assumes these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "minsil", min = 0.04535, max = 1000, input = "brk", def = 5, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg4 = { name = "equalise", min = 0, max = 1, tip = "the value of equalise determines the level balance between accented and unaccented beats." },
  arg5 = { name = "meter", switch = "-m", min = 0, max = 50, def = 10, tip = "specifies the pattern of accented beats" },
  arg6 = { name = "pregain", switch = "-p", min = 0.01, max = 1, tip = "gain on the input signal " },
}

dsp["Retime 6 - Position events in the input sound at specified beats in the output. Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 6", channels = "any", tip = "6 - Position events in the input sound at specified beats in the output. This process assumes these events are separated by silences, however short.. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "retempodata", input = "txt", tip = "textfile contains positions of events in the output. Events are counted in beats at the defined Tempo, and are assumed to start at beat zero." },
  arg4 = { name = "tempo", min = 0, max = 1000, def = 60, tip = "tempo of the output soundfile, as a Metronome Mark (MM) for values > 20, OR as a beat duration for values less than 1." },
  arg5 = { name = "offset", min = 0, max = length/1000, def = 0, tip = "time of the first sounding event in the output file" },
  arg6 = { name = "minsil", min = 0.045351, max = 10000.0, def = 10, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg7 = { name = "pregain", min = 0, max = 1, tip = "gain on the input signal (Range > 0 to 1)." },
}

dsp["Retime 7 - Position events in the input sound at specified times in the output. Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 7", channels = "any", tip = "7 - Position events in the input sound at specified times in the output. This process assumes these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "retempodata", input = "txt", tip = "textfile contains positions of events in the output. Events are counted in beats at the defined Tempo, and are assumed to start at beat zero." },
  arg4 = { name = "offset", min = 0, max = length/1000, def = 0, tip = "time of the first sounding event in the output file" },
  arg5 = { name = "minsil", min = 0.045351, max = 10000.0, def = 10, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg6 = { name = "pregain", min = 0, max = 1, tip = "gain on the input signal (Range > 0 to 1)." },
}

dsp["Retime 9 - Replace some events in the input sound by silence, using a specified pattern of 'deletions'. Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 9", channels = "any", tip = "9 - Replace some events in the input sound by silence, using a specified pattern of 'deletions'. This process can be used to change the rhythmic pattern of the input events. It assumes these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "maskdata", input = "txt", tip = "A textfile with a sequence of zeros and ones: 0 means masks an event (replace it by silence). 1 leaves an event unmasked. The pattern of masking is repeated once its end is reached." },
  arg4 = { name = "minsil", min = 0.045351, max = 10000.0, def = 10, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
}

dsp["Retime 11 - Finds durations of shortest and longest events in the input sound (not silences). Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 11", channels = "any", tip = "11 - Finds durations of shortest and longest events in the input sound (not silences). This process assumes these events are separated by silences, however short. Output is to the Console. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "minsil", min = 0.045351, max = 10000.0, def = 10, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
}

dsp["Retime 12 - Finds durations of shortest and longest events in the input sound (not silences). Adjust the levels of events in the input sound, changing the pattern of accents."] = {
  cmds = { exe = "retime", mode = "retime 12", channels = "any", tip = "12 - Find the start of the sound in the file (the 1st non-zero sample). The starttime of the event is written to a new, or an existing datafile. The latter allows several sounds to be run through the process to the same datafile, accumulating a list of the starttimes of each sound processed. – On the Sound Loom a list of sounds on the Chosen Files can be processed via the Bulk Process mechanism, to produce a single output datafile. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "outfile.txt", output = "txt", tip = "output datafile for Mode 12 The output datafile must have '.txt' extension. If the file already exists, the output is appended to that file." },
}

dsp["Retime 13 - Rearrange and retime events within a soundfilet - Find the peak in the input sound and move all the data so the peak is at a specified time."] = {
  cmds = { exe = "retime", mode = "retime 13", channels = "any", tip = "13 - Find the peak in the input sound and move all the data so the peak is at a specified time. This process allows the peak within a soundfile to be placed at a specific time after the file start. To achieve this, the entire sound is moved by inserting or removing silence. Note that, if the peak is moved to an earlier time, and this causes the initial sound in the file to be placed 'before zero', the process will fail. Moving the peak backwards in time should only be attempted if there is sufficient silence at the start of the sound. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "goalpeaktime", min = 0, max = 30, def = 6, tip = "Time to which the peak found in the input file is to be moved " },
}
 
dsp["Retime 14 - Rearrange and retime events within a soundfilet - Specify an event in the input sound and move all the data so the event is at a specified time."] = {
  cmds = { exe = "retime", mode = "retime 14", channels = "any", tip = "14 - Specify an event in the input sound and move all the data so the event is at a specified time. This process allows the event you specify within the input soundfile to be placed at a specific time after the file start. To achieve this, the entire sound is moved by inserting or removing silence.  Note that, if the peak is moved to an earlier time, and this causes the initial sound in the file to be placed 'before zero', the process will fail. Moving the peak backwards in time should only be attempted if there is sufficient silence at the start of the sound. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "goalpeaktime", min = 0, max = 30, def = 6, tip = "Time to which the peak found in the input file is to be moved " },
  arg4 = { name = "peaktime", min = 0, max = length/1000, tip = "Time of the event to be moved, within the input soundfile" },
}
 
dsp["Retime 3 - Rearrange and retime events within a soundfilet - Shorten existing events in the input sound."] = {
  cmds = { exe = "retime", mode = "retime 3", channels = "any", tip = "3 - Shorten existing events in the input sound. This process shortens the events in the infile. It assumes these events are separated by silences, however short.  Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "minsil", min = 0.04535, max = 1000, input = "brk", def = 5, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg4 = { name = "inpkwidth", min = 1, max = 1000, tip = "(minimum) width (in milliseconds) of events in the input file." },
  arg5 = { name = "outpkwidth", min = 1, max = 1000, tip = "required width (in milliseconds) of events in the output file." },
  arg6 = { name = "splicelen", min = 0, max = length, tip = "the duration of splices (in milliseconds) which cut the events to be placed in the output sound" },
}
 
dsp["Retime 4 - Rearrange and retime events within a soundfilet - Find existing events in the input sound and output them at a regular tempo (MM)."] = {
  cmds = { exe = "retime", mode = "retime 4", channels = "any", tip = "4 Find existing events in the input sound and output them at a regular tempo (MM). This process assumes that these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tempo", min = 0, max = 6000, def = 120, tip = "tempo of the output soundfile, as a Metronome Mark (MM) for values > 20, OR as a beat duration for values less than 1." },
  arg4 = { name = "minsil", min = 0.04535, max = 1000, input = "brk", def = 5, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg5 = { name = "pregain", min = 0, max = 1, tip = "gain on the input signal (Range > 0 to 1)." },
}
 
dsp["Retime 5 - Rearrange and retime events within a soundfilet - Find events in the input sound and change their speed by a factor."] = {
  cmds = { exe = "retime", mode = "retime 5", channels = "any", tip = "5 Find events in the input sound and change their speed by a factor. This process assumes these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "factor", min = 0.01, max = 100, input = "brk", tip = "speed-change factor (which can vary over time)." },
  arg4 = { name = "minsil", min = 0.04535, max = 1000, input = "brk", def = 5, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
  arg5 = { name = "start", switch = "-s", min = 0, max = length/1000, tip = "time at which the speed changing becomes effective" },
  arg6 = { name = "end", switch = "-e", min = 0, max = length/1000, tip = "time at which the speed changing ceases to be effective " },
  arg7 = { name = "sync", switch = "-a", min = 0, max = length/1000, tip = "approximate time of any infile-event which synchronises with its copy in the output." },
}
 
dsp["Retime 8 - Rearrange and retime events within a soundfilet - Specify an event within the input sound and repeat it at a specified tempo."] = {
  cmds = { exe = "retime", mode = "retime 8", channels = "any", tip = "8 Specify an event within the input sound and repeat it at a specified tempo. This process assumes these events are separated by silences, however short. Musical Applications; The overall application of RETIME is to (re)rhythmicise material. This suite of processes can be used to change the time-pattern within an input sound, such as the rhythm of a melodic phrase or the prosody of speech. They were originally developed to slightly shift the true rhythm of spoken phrases of natural speech onto an idealised rhythmic frame, permitting different vocal phrases to be rhythmically locked to a particular tempo, without radically altering the speech prosody. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "tempo", min = 0.01, max = 1000, tip = "tempo of the output soundfile, as a Metronome Mark (MM) for values > 20, OR as a beat duration for values less than 1." },
  arg4 = { name = "eventtime", min = 0, max = length/1000, def = 0, tip = "tart time (roughly) of event to repeat (time specified must be inside the event)." },
  arg5 = { name = "beats", min = 1, max = 24, tip = "number of beats (at the specified tempo) within the event-to-repeat – effectively the duration of the event-to-repeat." },
  arg6 = { name = "repeats", min = 1, max = 100, def = 5, tip = "number of times to repeat the event (1 or more)" },
  arg7 = { name = "minsil", min = 0.04535, max = 1000, input = "brk", def = 5, tip = "duration in milliseconds of the minimum silence between events. (Range: 0.045351 to 10000.0)." },
}
 
------------------------------------------------
-- reverb
------------------------------------------------
 
dsp["Reverb - Multi-Channel reverb "] = {
  cmds = { exe = "reverb", mode = "", channels = "1in2out", tip = " REVERB implements the now classic Schroeder/Moorer model, consisting of six comb filters in parallel, followed by four allpass filters in series. The comb filters generate the dense reverberation, and the allpass filters (with much shorter delay times) apply further smearing of the echoes to minimize the spectral colouration of the comb filters. A further allpass is applied to each output channel, each with a different randomly-chosen delay time. Each comb filter contains a simple low-pass filter to simulate high-frequency absorption - this also affects the overall reverberation time. A preset set of early reflections as defined by Moorer is incorporated; this can be replaced by a user-defined set either hand-written or created using ROOMRESP. The delay times for the filters are preset to suit a 'medium room' model, as suggested by Moorer. In most situations the user will not want to change these; however the option is provided to change these times by means of a simple text file." },
  arg1 = { name = "N", switch = "-c", min = 1, max = 16, def = 2, tip = "create outfile with N channels (Range: 1 <= N < 16; Default = 2)" },
  arg2 = { name = "FILE", switch = "-e", input = "data", tip = "load early reflections data from breakpoint file as made with rmresp" },
  arg3 = { name = "-f", switch = "-f", tip = "write output as floating point (Default: format of infile)" },
  arg4 = { name = "highcut filter", switch = "-H", min = 0, max = 22050, def = 10000, tip = "Apply Highcut filter to infile with cutoff frequency NHz(6dB per octave)" },
  arg5 = { name = "lowcut filter", switch = "-L", min = 0, max = 22050, def = 400, tip = "apply Lowcut filter to infile with cutoff frequency NHz (12dB per octave)" },
  arg6 = { name = "N", switch = "-p", min = 0, max = 1000, tip = "set reverb predelay to N msecs (shifts early reflections)" },  
  arg7 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg8 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg9 = { name = "egain", min = 0, max = 1, tip = "set level of early reflections (Range: 0.0 to 1.0)" },
  arg10 = { name = "mix", min = 0, max = 1, tip = "balance of direct and reverb signal (Range: 1.0 [weighted towards direct signal] to 0.0 [weighted towards reverb signal])" },
  arg11 = { name = "rvbtime", min = 0, max = 20, tip = "reverb decay time (to -60dB) in seconds" },
  arg12 = { name = "absorb", min = 0, max = 1, tip = "degree of hf damping to suggest roomsize (Range: 0.0 to 1.0)" },
  arg13 = { name = "lpfreq", min = 0, max = 22050, tip = "lowpass filter cutoff frequency in Hz applied at input to reverb" },
  arg14 = { name = "trailertime", min = 0, max = 30, def = 0, tip = "time in seconds added to outfile for reverb tail" },
}

------------------------------------------------
-- rmsinfo
------------------------------------------------ 

dsp["Rmsinfo - Scan file and report RMS and average power level statistics "] = {
cmds = { exe = "rmsinfo", mode = "", tip = "Standard output shows: RMS level, Average level, DC level (bipolar average). RMSINFO complements SFPROPS by determining the overall power level (loudness) of each channel of the input soundfile. It uses two types of computation: RMS (Root-Mean-Square) and raw average (normalised sum of all samples). In addition, it displays the average level of any DC present. All levels are calculated relative to digital peak at 0dBFS. Thus a full-range sinusoid will have a reported RMS level of -3dB. For sustained sounds, the RMS and average levels will be very similar. For percussive, speech-based, and other sounds with widely differing amplitudes, they may be markedly different – the average level will be lower than the RMS level. In such cases, the information reported gives an overall indication of the 'peakiness' of a file, and may be a guide to the choice of compressor thresholds. Note that for static B-Format soundfiles – i.e., no motion of sources – the information will demonstrate the standard -3dB reduction of the W signal, as employed in all the MCTOOLKIT programs and in keeping with the Furse-Malham conventions. For mobile sources, e.g., from using ABFPAN2, the levels may well be reported as equal, depending on the overall bias (if any) of the panning. The final reported value represents the average over the whole file. Other conventions for B-Format streams are under consideration within the surround sound community, in particular, normalisation schemes that avoid the traditional -3dB reduction of W. This will in principle lead to RMS values for W that are closely similar to those of other channels. The program recognises that it can take a long time to scan a large file. The optional arguments startpos and endpos (which can be invoked separately) can be used to select a selection of a file to be scanned. Also, CTRL-C can be used to terminate the program, in which event the levels up to that point will be displayed." },
  arg1 = { name = "-n", switch = "-n", tip = "include equivalent 0dBFS-normalised RMS and AVG levels. Use this flag to see the equivalent levels if the file is normalised to 0dBFS." }, 
  arg2 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg3 = { name = "startpos", min = 0, max = length/1000, def = 0, tip = "start position in seconds at which to begin the scan" },
  arg4 = { name = "endpos", min = 0, max = length/1000, def = length/1000, tip = "position in seconds at which to end the scan" },
}

------------------------------------------------
-- rmverb
------------------------------------------------
 
dsp["Roomverb - Multi-channel reverb with room simulation "] = {
  cmds = { exe = "rmverb", mode = "", channels = "1in2out", tip = "Although many of the parameters are the same as those for REVERB (q.v), the program operates very differently. Rather than use comb filters in parallel, ROOMVERB uses a variable network of 'nested' allpass filters inside an overall feedback loop. This has the effect of increasing the density of the reverberation over time, as is characteristic of most 'real' acoustic spaces. It is therefore dedicated more specifically to that task than is REVERB. Nevertheless, as the roomsize and feedback parameters are independently controllable, some unusal effects can still be created. For authentic reverb, it is important that the feedback level is not set too high, otherwise audible pulsations can be heard at the start of the reverberation. These can be mitigated to a degree by running REVERB several times in parallel, with slightly different reverb times, and mixing the results, or combining into a multi-channel file. Conversely, it is also possible to create an 'infinite reverb' effect by setting the feedback level to 1.0.The preset early reflections for a given room size are the same as those in REVERB." },
  arg1 = { name = "predelay", switch = "-p", min = 0, max = 1000, def = 0, tip = "override early reflections delay in milliseconds" },
  arg2 = { name = "N", switch = "-c", min = 1, max = 2, def = 2, tip = "create N-channel outfile (Default:2)" },
  arg3 = { name = "early.brk", switch = "-e", input = "txt", tip = "load early reflections data from breakpoint file as made with rmresp" },
  arg4 = { name = "-f", switch = "-f", tip = "write output as floating point (Default: format of infile)" },
  arg5 = { name = "lowcut filter", switch = "-L", min = 0, max = 22050, def = 150, tip = "apply Lowcut filter to infile with cutoff frequency NHz (12dB per octave)" },
  arg6 = { name = "highcut filter", switch = "-H", min = 0, max = 22050, def = 3000, tip = "Apply Highcut filter to infile with cutoff frequency NHz(6dB per octave)" },
  arg7 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg8 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg9 = { name = "rmsize", min = 1, max = 3, def = 1, tip = "1 (small), 2 (medium), or 3 (large)" },
  arg10 = { name = "egain", min = 0, max = 1, tip = "set level of early reflections (Range: 0.0 to 1.0)" },
  arg11 = { name = "mix", min = 0, max = 1, tip = "balance of direct and reverb signal (Range: 1.0 [weighted towards direct signal] to 0.0 [weighted towards reverb signal])" },
  arg12 = { name = "fback", min = 0, max = 1, tip = "reverb feedback level: controls decaytime (Range: 0.0 to 1.0)" },
  arg13 = { name = "absorb", min = 0, max = 1, tip = "degree of hf damping to suggest roomsize (Range: 0.0 to 1.0)" },
  arg14 = { name = "lpfreq", min = 0, max = 22050, tip = "lowpass filter cutoff frequency in Hz applied at input to reverb" },
  arg15 = { name = "trailertime", min = 0, max = 30, def = 0, tip = "time in seconds added to outfile for reverb tail" },
}

------------------------------------------------
-- rmresp
------------------------------------------------

dsp["Roomresp - Create early reflections data file for REVERB, ROOMVERB and TAPDELAY"] = {
  cmds = { exe = "rmresp", mode = "", tip = "This program uses a simple ray-tracing model as widely used in computer graphics, based on the dimensions of a rectangular room. Reverberation arises when a sound, as well as reaching listeners directly, also reaches them after bouncing several times off one or more surfaces (floor, walls, ceiling). Depending on the degree of reflectivity or liveness of these surfaces (e.g. stone walls reflect almost all the sound, whereas soft furnishings reflect much less) many or few reflections will reach the listeners, who will perceive discrete echoes or a smooth ambience, and a longer or shorter reverberation time. Since the room model used is a simple rectangular shape, with parallel walls, it is possible for reflections to form regular patterns. For smooth reverberation this needs to be minimised; this can be achieved by placing the listener (and possibly the source too) assymetrically in the room – i.e., not equidistant between two walls. On the other hand, it is easy to generate the sorts of highly coloured reflections experienced in bathrooms, tanks and similar spaces. Thus a typical arrangement for a medium room might be (all dimensions in metres); ROOM:  length = 20  width = 11  height = 3.5. SOURCE:  length = 4.5  width = 5  height = 2.5. LISTENER:  length = 17  width = 6  height = 2. The number of reflections generated depends on nrefs. Useful values are between 2 and 5;  the latter will generate approximately 876 taps. In theory there should be even more, but many echoes arrive virtually at the same time, and these are averaged together to reduce the overall data dize. Thelivenessparameter does not affect the number of reflections, but their amplitude. When a lowish level for liveness is set, many of the generated taps will have very low amplitudes (they would be submerged in the dense reverberation), and it will be reasonable to delete these to save processing time in the reverb programs. The amplitude of the output taps is internally scaled ('normalized') to 1.0. The data can be evaluated by applying it to TAPDELAY, and once a satisfactory level is found (using the tapgain parameter in TAPDELAY), this can be used with the -a flag to generate a new data file suitable to apply to the reverb programs. The egain parameter in REVERB and ROOMVERB can be used to the same end. As ever, experimentation is the rule!" },
  arg1 = { name = "maxamp", switch = "-a", min = 0.0, max = 1.0, def = 1.0, tip = "peak amplitude for data (Range: 0.0 to 1.0; Default: 1.0)" },
  arg2 = { name = "res", switch = "-r", min = 0.1, max = 2.0, def = 0.1, tip = "time resolution in milliseconds for reflections (Range: 0.1 to <= 2; Default: 0.1)" },
  arg3 = { name = "txout.dat", output = "txt", tip = "text output file containing early reflections in breakpoint format suitable for input to ROOMVERB, REVERB or TAPDELAY" },
  arg4 = { name = "liveness", min = 0, max = 1, def = 0.95, tip = "degree of reflection from each surface (Range: 0 to 1; typical value: 0.95)" },
  arg5 = { name = "nrefs", min = 0, max = 50, def = 2, tip = "number of reflections from each surface (> 0; typical value: 2 to 5). Warning: high values will create extremely long data files!" },
  arg6 = { name = "roomL", min = 0, max = 100, def = 3, tip = "room size (Length, Width, Height)" },
  arg7 = { name = "roomW", min = 0, max = 100, def = 3, tip = "room size (Length, Width, Height)" },
  arg8 = { name = "roomH", min = 0, max = 100, def = 3, tip = "room size (Length, Width, Height)" },
  arg9 = { name = "sourceL", min = 0, max = 100, def = 2, tip = "position of sound source (as above)" },
  arg10 = { name = "sourceW", min = 0, max = 100, def = 2, tip = "position of sound source (as above)" },
  arg11 = { name = "sourceH", min = 0, max = 100, def = 2, tip = "position of sound source (as above)" },
  arg12 = { name = "listenerL", min = 0, max = 100, def = 1, tip = "position of listener (as above" },
  arg13 = { name = "listenerW", min = 0, max = 100, def = 1, tip = "position of listener (as above" },
  arg14 = { name = "listenerH", min = 0, max = 100, def = 1, tip = "position of listener (as above" },
}
 
------------------------------------------------
-- selfsim
------------------------------------------------
 
dsp["Selfsim - Replaces spectral windows with the most similar, louder window(s) of the same analysis file "] = {
  cmds = { exe = "selfsim", mode = "selfsim", tip = "This process systematically replaces spectral windows that are less prominent (i.e.,not as loud, no special peaks ...) with spectral windows in the same file that are more prominent in some way and are the most similar to them in spectral envelope . At the same time it scales amplitudes for any overall difference between the two windows. Musical Applications; It is important to observe the effect of higher values for the self-similarity-index. You might describe this program as a feature repeater. On the one hand it is looking for prominent windows and extending their presence in the sound. On the other hand, these prominent windows are also examined for similarity of spectral envelope. Thus the features of this spectral envelope are repeated more often in the resulting sound – hence the 'self-similar' idea. The overall effect is going to depend on the nature of the prominent windows, and one can expect that a fairly steady-state sound will not be much affected by this process. Try it on exciting sounds! " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "self-similarity-index", min = 0, max = 1000, def = 10, tip = "the number of similar windows to replace." },
}
 
------------------------------------------------
-- sfecho
------------------------------------------------

dsp["Sfecho Echo - Repeat a sound with timing and level adjustments between repeats"] = {
  cmds = { exe = "sfecho", mode = "echo", url = "http://www.ensemble-software.net/CDPDocs/html/cgroextd.htm#ECHOES", tip = "CDP's EXTEND LOOP enables you to step through a soundfile while adding each step-segment to an output soundfile. It does not allow you to specify an endtime beyond the end of the input sound (it cuts off). EXTEND REPETITIONS enables you to repeat a whole soundfile, whether overlapping or with a gap between repetitions: i.e., the time of repetition is beyond the end of the input sound. This new ECHO function complements these two features by placing the repeats after the end of the input soundfile." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "delay", min = length/1000, max = 3600, input = "brk", def = length/1000 + 1,  tip = "time in seconds between echo repeats (Range: greater than the length of insndfile to 3600 sec. [one hour]; thus delay cannot be less than the input duration)" },
  arg4 = { name = "attenuation", min = 0, max = 1, input = "brk", def = 0.2, tip = "relative (diminishing) level of each repeat (Range: 0 to 1)" },
  arg5 = { name = "totaldur", min = 0, max = 3600, def = 10 , tip = "maximum output duration (actual duration may be less); it must be a minimum of 2 x delay." },
  arg6 = { name = "-r", switch = "-r", min = 0, max = 1, input = "brk", tip = "randomisation of echo times (Range: 0 to 1)" },
  arg7 = { name = "-c", switch = "-c", min = 0, max = -96, def = 0, tip = "dB level at which decaying echoes cut off (Range: 0 to -96dB, Default: -96dB, i.e., silence)" },
}
 
------------------------------------------------
-- sfedit
------------------------------------------------
 
dsp["Sfedit Cut - 1 Cut and keep a segment of a sound (Time in seconds)"] = {
  cmds = { exe = "sfedit", mode = "cut 1", channels = "any", tip = "The start and end locations for a block of sound are specified, and that block is saved as a new soundfile. A long splice will give a smooth cutoff, a short splice an abrupt cutoff, and a zerio splice will usually produce a click. The splice window is applied to both the beginning and the end of the sound, so cannot be larger than half the length of the block. This function presumes the use of a graphic sound editor in order precisely to locate the places at which to begin end the block to be cut. Musical Applications; simply save a favoured portion of a soundfile, select a short, timbrally evolving, section of a soundfile and time-stretch it, cut part of the result and time-stretch again, etc. process a portion of a sound, and then reinsert it into the original. For example, isolate key portions of two sounds and pre-process each of them in preparation for a morph-transition; then reinsert them into their respective original soundfiles and do the morph, collect segments of various soundfiles in preparation for making a musical collage" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000, tip = "time in infile where segment to keep begins" },
  arg4 = { name = "end", min = 0, max = length/1000, tip = "time in infile where segment to keep ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Cut - 2 Cut and keep a segment of a sound (Time as sample count (rounded to multiples of channel count))"] = {
  cmds = { exe = "sfedit", mode = "cut 2", channels = "any", tip = "2 Time as sample count (rounded to multiples of channel count). The start and end locations for a block of sound are specified, and that block is saved as a new soundfile. A long splice will give a smooth cutoff, a short splice an abrupt cutoff, and a zerio splice will usually produce a click. The splice window is applied to both the beginning and the end of the sound, so cannot be larger than half the length of the block. This function presumes the use of a graphic sound editor in order precisely to locate the places at which to begin end the block to be cut. Musical Applications; simply save a favoured portion of a soundfile, select a short, timbrally evolving, section of a soundfile and time-stretch it, cut part of the result and time-stretch again, etc. process a portion of a sound, and then reinsert it into the original. For example, isolate key portions of two sounds and pre-process each of them in preparation for a morph-transition; then reinsert them into their respective original soundfiles and do the morph, collect segments of various soundfiles in preparation for making a musical collage" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000*srate, tip = "time in infile where segment to keep begins" },
  arg4 = { name = "end", min = 0, max = length/1000*srate, tip = "time in infile where segment to keep ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Cut - 3 Cut and keep a segment of a sound (Time as grouped sample count (e.g., 3 = 3 stereo pairs)"] = {
  cmds = { exe = "sfedit", mode = "cut 3", channels = "any", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). The start and end locations for a block of sound are specified, and that block is saved as a new soundfile. A long splice will give a smooth cutoff, a short splice an abrupt cutoff, and a zerio splice will usually produce a click. The splice window is applied to both the beginning and the end of the sound, so cannot be larger than half the length of the block. This function presumes the use of a graphic sound editor in order precisely to locate the places at which to begin end the block to be cut. Musical Applications; simply save a favoured portion of a soundfile, select a short, timbrally evolving, section of a soundfile and time-stretch it, cut part of the result and time-stretch again, etc. process a portion of a sound, and then reinsert it into the original. For example, isolate key portions of two sounds and pre-process each of them in preparation for a morph-transition; then reinsert them into their respective original soundfiles and do the morph, collect segments of various soundfiles in preparation for making a musical collage" },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000*srate, tip = "time in infile where segment to keep begins" },
  arg4 = { name = "end", min = 0, max = length/1000*srate, tip = "time in infile where segment to keep ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit Cutend - 1 Cut and keep the end portion of a sound (Time in seconds)"] = {
  cmds = { exe = "sfedit", mode = "cutend 1", channels = "any", tip = "This function enables you to use a specified length of the last section of a sound without having to work out where that length begins. You just specify the length you want. Needless to say, it has to be shorter than the whole soundfile. Musical Applications; The way a sound ends varies a great deal and often has extra attributes, such as resonance, reverb or echoes. This material can therefore be useful in itself. For example, a piano tone starts percussively and ends gradually if left to ring on. When reversed, the sound swells, sounding very much like an organ. SFEDIT CUTEND can quickly cut the end portion, starting automatically after the beginning. Given a generous splice envelope and reversed with MODIFY RADICAL Mode 1, it can become quite a different sound altogether." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "length", min = 0, max = length/1000, tip = "length of sound to keep, ending at the end of infile" },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Cutend - 2 Cut and keep the end portion of a sound (Time as sample count (rounded to multiples of channel count))"] = {
  cmds = { exe = "sfedit", mode = "cutend 2", channels = "any", tip = "2 Time as sample count (rounded to multiples of channel count). This function enables you to use a specified length of the last section of a sound without having to work out where that length begins. You just specify the length you want. Needless to say, it has to be shorter than the whole soundfile. Musical Applications; The way a sound ends varies a great deal and often has extra attributes, such as resonance, reverb or echoes. This material can therefore be useful in itself. For example, a piano tone starts percussively and ends gradually if left to ring on. When reversed, the sound swells, sounding very much like an organ. SFEDIT CUTEND can quickly cut the end portion, starting automatically after the beginning. Given a generous splice envelope and reversed with MODIFY RADICAL Mode 1, it can become quite a different sound altogether." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "length", min = 0, max = length/1000*srate, tip = "length of sound to keep, ending at the end of infile" },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Cutend - 3 Cut and keep the end portion of a sound (Time as grouped sample count (e.g., 3 = 3 stereo pairs))"] = {
  cmds = { exe = "sfedit", mode = "cutend 3", channels = "any", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). This function enables you to use a specified length of the last section of a sound without having to work out where that length begins. You just specify the length you want. Needless to say, it has to be shorter than the whole soundfile. Musical Applications; The way a sound ends varies a great deal and often has extra attributes, such as resonance, reverb or echoes. This material can therefore be useful in itself. For example, a piano tone starts percussively and ends gradually if left to ring on. When reversed, the sound swells, sounding very much like an organ. SFEDIT CUTEND can quickly cut the end portion, starting automatically after the beginning. Given a generous splice envelope and reversed with MODIFY RADICAL Mode 1, it can become quite a different sound altogether." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "length", min = 0, max = length/1000*srate, tip = "length of sound to keep, ending at the end of infile" },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit Insil - 1 Insert silence into a sound (Time in seconds, overwriting or spreading the sound apart)"] = {
  cmds = { exe = "sfedit", mode = "insil 1", channels = "any", tip = "1 Time in seconds. This process will create a gap in the infile at a specified point in time, either pushing apart or overwriting the original sound for the duration of the silence. Musical Applications; This can be used at the beginning of a sound to 'hard-wire' a gap into a mix. Another application would be to spread the timing of two events in a sound by a specified amount." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000, tip = "time in seconds in infile at which the silence is to begin" },
  arg4 = { name = "duration", min = 0, max = length/1000, tip = "length of silence in seconds" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
  arg6 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the silence pushes the infile apart)" },
  arg7 = { name = "-s", switch = "-s", tip = "retains any silence written over file end (Default: rejects silence added at file end) " },
}

dsp["Sfedit Insil - 2 Insert silence into a sound (Time as sample count, rounded to multiples of channel count), overwriting or spreading the sound apart)"] = {
  cmds = { exe = "sfedit", mode = "insil 2", channels = "any", tip = "2 Time as sample count (rounded to multiples of channel count). This process will create a gap in the infile at a specified point in time, either pushing apart or overwriting the original sound for the duration of the silence. Musical Applications; This can be used at the beginning of a sound to 'hard-wire' a gap into a mix. Another application would be to spread the timing of two events in a sound by a specified amount." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000*srate, tip = "time in seconds in infile at which the silence is to begin" },
  arg4 = { name = "duration", min = 0, max = length/1000*srate, tip = "length of silence in seconds" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
  arg6 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the silence pushes the infile apart)" },
  arg7 = { name = "-s", switch = "-s", tip = "retains any silence written over file end (Default: rejects silence added at file end) " },
}

dsp["Sfedit Insil - 3 Insert silence into a sound (Time as grouped sample count), overwriting or spreading the sound apart)"] = {
  cmds = { exe = "sfedit", mode = "insil 3", channels = "any", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). This process will create a gap in the infile at a specified point in time, either pushing apart or overwriting the original sound for the duration of the silence. Musical Applications; This can be used at the beginning of a sound to 'hard-wire' a gap into a mix. Another application would be to spread the timing of two events in a sound by a specified amount." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000*srate, tip = "time in seconds in infile at which the silence is to begin" },
  arg4 = { name = "duration", min = 0, max = length/1000*srate, tip = "length of silence in seconds" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
  arg6 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the silence pushes the infile apart)" },
  arg7 = { name = "-s", switch = "-s", tip = "retains any silence written over file end (Default: rejects silence added at file end) " },
}

dsp["Sfedit Noisecut - Noisecut, suppress noise in a (mono) sound file, replacing with silence"] = {
  cmds = { exe = "sfedit", mode = "noisecut", tip = "This process was developed after many attempts to automatically separate the noise constituents (sibilants etc.) from speech whilst trying to track the pitch of the other material. It uses a filter to recognise the presence of sibilants in the speech and allows vowels (and strongly pitched iteratives) to be separated – in place (i.e., remaining at their original time) – from the speech stream. Alternatively, the sibilants (i.e., noise) can be similarly extracted, in place. This is the -n option. Musical applications; One way to apply this function is to use it to treat the pitched and unpitched components in a stream of events in different ways. You could first separate the pitched and noise elements (using this program in its two different senses). Then you might add tremolo to the pitched elements and after this reintroduce the unmodified noise elements by mixing them with the undulating tones." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "splicelen", min = 0, max = 50, tip = "duration of splice slopes, in milliseconds" },
  arg4 = { name = "noisfrq", min = 0, max = 22050, tip = "frequency above which the signal is regarded as noise (try 6000 Hz)" },
  arg5 = { name = "maxnoise", min = 0, max = 50, tip = "the maximum duration in milliseconds of any noise segments permitted to remain." },
  arg6 = { name = "mintone", min = 0, max = 50, tip = "the minimum duration in milliseconds of any non-noise segments to be retained. Range: 0 to 50ms" },
  arg7 = { name = "-n", switch = "-n", tip = "option to retain noise rather than non-noise " },
}
 
dsp["Sfedit Excise - 1 Remove a segment from a soundfile and close up the gap (time in seconds)"] = {
  cmds = { exe = "sfedit", mode = "excise 1", channels = "any", tip = "1 Time in seconds. Here the block start and end points mark a block to be removed. The size of the splice determines the smoothness (long splice) or abruptness (short splice) of the cuts. A zero splice usually creates a click at the splice point, except in the special case where the signal is zero. The splice 'window' enables the use to reshape the amplitude envelope at the point where the cuts are made. Musical Applications; A frequent use of this program will be to remove silence, glitches, or otherwise unwanted material from a sound. It could also be used to chop up a sound in a rough sort of way in order to create unexpected juxtapositions of material, e.g., words." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000, tip = "time in infile where segment to remove begins" },
  arg4 = { name = "end", min = 0, max = length/1000, tip = "time in infile where segment to remove ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Excise - 2 Remove a segment from a soundfile and close up the gap (Time as sample count (rounded to multiples of channel count))"] = {
  cmds = { exe = "sfedit", mode = "excise 2", channels = "any", tip = "2 Time as sample count (rounded to multiples of channel count). Here the block start and end points mark a block to be removed. The size of the splice determines the smoothness (long splice) or abruptness (short splice) of the cuts. A zero splice usually creates a click at the splice point, except in the special case where the signal is zero. The splice 'window' enables the use to reshape the amplitude envelope at the point where the cuts are made. Musical Applications; A frequent use of this program will be to remove silence, glitches, or otherwise unwanted material from a sound. It could also be used to chop up a sound in a rough sort of way in order to create unexpected juxtapositions of material, e.g., words." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000*srate, tip = "time in infile where segment to remove begins" },
  arg4 = { name = "end", min = 0, max = length/1000*srate, tip = "time in infile where segment to remove ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}

dsp["Sfedit Excise - 3 Remove a segment from a soundfile and close up the gap (Time as grouped sample count (e.g., 3 = 3 stereo pairs))"] = {
  cmds = { exe = "sfedit", mode = "excise 3", channels = "any", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). Here the block start and end points mark a block to be removed. The size of the splice determines the smoothness (long splice) or abruptness (short splice) of the cuts. A zero splice usually creates a click at the splice point, except in the special case where the signal is zero. The splice 'window' enables the use to reshape the amplitude envelope at the point where the cuts are made. Musical Applications; A frequent use of this program will be to remove silence, glitches, or otherwise unwanted material from a sound. It could also be used to chop up a sound in a rough sort of way in order to create unexpected juxtapositions of material, e.g., words." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000*srate, tip = "time in infile where segment to remove begins" },
  arg4 = { name = "end", min = 0, max = length/1000*srate, tip = "time in infile where segment to remove ends" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit - Zcut, keep a segment of a MONO soundfile, cutting at zero crossings"] = {
  cmds = { exe = "sfedit", mode = "zcut 1", tip = "This process uses an alternative method to 'splice' the sound segments, cutting them at the nearest zero-crossings in the signal, rather than making a splice slope of default or specified duration . Nevertheless the cuts should be clickless. Musical Applications; This is a different way of cutting out a segment of sound. The start and end times you give are approximate because the nearest zero points will probably not be precisely at those times." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "start", min = 0, max = length/1000, tip = "(approximate) time in the infile where the segment to keep begins" },
  arg4 = { name = "end", min = 0, max = length/1000, tip = "(approximate) time in the infile where the segment to keep ends" },
}
 
dsp["Sfedit Excises - 1 Time in seconds - Remove segments of a soundfile and close up the gaps "] = {
  cmds = { exe = "sfedit", mode = "excises 1", tip = "1 Time in seconds. The times in excisefile are given in seconds, a pair on each line, separated by a space or a tab. Musical Applications; Multiple cuts may be useful when removing a series of glitches, or when chopping up a sound as mentioned above in SFEDIT EXCISE to create unexpected juxtapositions: collage techniques." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "text file with (paired) start and end times of chunks to be removed. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit Excises - 2 Time as sample count - Remove segments of a soundfile and close up the gaps "] = {
  cmds = { exe = "sfedit", mode = "excises 2", tip = "2 Time as sample count (rounded to multiples of channel count). The times in excisefile are given in seconds, a pair on each line, separated by a space or a tab. Musical Applications; Multiple cuts may be useful when removing a series of glitches, or when chopping up a sound as mentioned above in SFEDIT EXCISE to create unexpected juxtapositions: collage techniques." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "text file with (paired) start and end times of chunks to be removed. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit Excises - 3 Time as grouped sample count - Remove segments of a soundfile and close up the gaps "] = {
  cmds = { exe = "sfedit", mode = "excises 3", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). The times in excisefile are given in seconds, a pair on each line, separated by a space or a tab. Musical Applications; Multiple cuts may be useful when removing a series of glitches, or when chopping up a sound as mentioned above in SFEDIT EXCISE to create unexpected juxtapositions: collage techniques." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "text file with (paired) start and end times of chunks to be removed. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms) " },
}
 
dsp["Sfedit Insert 1 - Insert 2nd sound into 1st (overwriting or spreading first sound) - Time in seconds"] = {
  cmds = { exe = "sfedit", mode = "insert 1", tip = "1 Time in seconds. Note the difference between placing a sound into the midst of another sound (pushing apart the two separated portions of the original), and actually overwriting the original. In the first instance, none of the original is lost. In the second, that part of the original which lasts until the end of the insert is lost – but if there is still more original soundfile after this point, it will carry on after the insert has finished. Musical Applications; Besides normal joinings and juxtapositions, SFEDIT INSERT can be used with more far-reaching objectives in mind. For example, a soundfile could be constructed out of widely diverse materials in order to pave the way for timbral transformations which will greatly alter the original sources (making them unrecognisable). E.g., blur, trace, extract spectral envelope, spread peaks, invert spectrum .." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "time", min = 0, max = length/1000, def = 0, tip = "time in seconds in infile at which the insert is to begin" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms)" },
  arg6 = { name = "level", switch = "-l", min = 0, max = 100, def = 1, tip = "gain multiplier on inserted file (Default: 1.0)" },
  arg7 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the insert pushes the infile apart)" },
}
 
dsp["Sfedit Insert 2 - Insert 2nd sound into 1st (overwriting or spreading first sound) - Time as sample count (rounded to multiples of channel count)"] = {
  cmds = { exe = "sfedit", mode = "insert 2", tip = "2 Time as sample count (rounded to multiples of channel count). Note the difference between placing a sound into the midst of another sound (pushing apart the two separated portions of the original), and actually overwriting the original. In the first instance, none of the original is lost. In the second, that part of the original which lasts until the end of the insert is lost – but if there is still more original soundfile after this point, it will carry on after the insert has finished. Musical Applications; Besides normal joinings and juxtapositions, SFEDIT INSERT can be used with more far-reaching objectives in mind. For example, a soundfile could be constructed out of widely diverse materials in order to pave the way for timbral transformations which will greatly alter the original sources (making them unrecognisable). E.g., blur, trace, extract spectral envelope, spread peaks, invert spectrum .." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "time", min = 0, max = length/1000, def = 0, tip = "time in seconds in infile at which the insert is to begin" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms)" },
  arg6 = { name = "level", switch = "-l", min = 0, max = 100, def = 1, tip = "gain multiplier on inserted file (Default: 1.0)" },
  arg7 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the insert pushes the infile apart)" },
}
 
dsp["Sfedit Insert 3 - Insert 2nd sound into 1st (overwriting or spreading first sound) - Time as grouped sample count (e.g., 3 = 3 stereo pairs) "] = {
  cmds = { exe = "sfedit", mode = "insert 3", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs). Note the difference between placing a sound into the midst of another sound (pushing apart the two separated portions of the original), and actually overwriting the original. In the first instance, none of the original is lost. In the second, that part of the original which lasts until the end of the insert is lost – but if there is still more original soundfile after this point, it will carry on after the insert has finished. Musical Applications; Besides normal joinings and juxtapositions, SFEDIT INSERT can be used with more far-reaching objectives in mind. For example, a soundfile could be constructed out of widely diverse materials in order to pave the way for timbral transformations which will greatly alter the original sources (making them unrecognisable). E.g., blur, trace, extract spectral envelope, spread peaks, invert spectrum .." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "time", min = 0, max = length/1000, def = 0, tip = "time in seconds in infile at which the insert is to begin" },
  arg5 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds (Default: 15ms)" },
  arg6 = { name = "level", switch = "-l", min = 0, max = 100, def = 1, tip = "gain multiplier on inserted file (Default: 1.0)" },
  arg7 = { name = "-o", switch = "-o", tip = "overwrite the original file with the inserted file (Default: the insert pushes the infile apart)" },
}
 
dsp["Sfedit Join - Join 2 files together, one after another  "] = {
  cmds = { exe = "sfedit", mode = "join", tip = "Splicing is joining soundfiles together. The joins can be 'butt' or sloped. A butt join means that the sounds are butted up against each other just as they are, with no overlap and no slope other than what they may already possess. This can be done by specifying 0 for splice. Unless your sounds are already spliced at the start and end, this will almost always produce a click at the edit point. Sloped joins either use the default overlap of 15 ms or specify another splicelen time. Longer times mean more overlap and more gradual changes in relative amplitude, the preceding sound getting softer while the following sound gets louder. Splice times are not restricted by the software, but if both -b and -e are used, splice cannot be greater than half the length of the sound. [Thanks to Gustav Ciamaga for pointing out the following.] SFEDIT JOIN requires at least 2 infiles, usually different but they can also be the same. In this instance, one could also taper (fade-in/fade-out) the beginning and/or end of the resultant repeated sound. To be more specific: you would use the same soundfile as both infile1 and infile2. In this instance the beginning and end of the resultant repeated sound can both be tapered (fade-in / fade-out). Use the -b flag to set the fade-in at the beginning and the -e flag to set the fade-out at the end (of the 2nd, repeated, sound). You can confirm this with a splice window of 1000 ms (-w1000). Thus, when the same sound is used several times: as infile1, infile2 or more times, this is one way to achieve repetitions or pulsations, depending on the nature of the sound. Musical Applications Splicing is one of the basic assembly procedures used in electroacoustic music. A butt join achieves maximum contrast and/or a join with no loss of time (no overlap), but there is a danger of clicks. Sometimes a portion of soundfile is removed, processed and replaced. In this case, it is best to use VIEWSF in single sample view mode in order to edit with sample accuracy as close to zero as possible. Overlaps smooth the joins and reduce the possibility of clicks or other unwanted 'bumps'. Long splice times achieve a smooth flow of one sound into the other without going as far as a full-length crossfade or morph. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "splicelen", switch = "-w", min = 0, max = length, def = 15, tip = "duration of splice in milliseconds (Default: 15ms)" },
  arg5 = { name = "-b", switch = "-b", tip = "splice slope at start of first file" },
  arg6 = { name = "-e", switch = "-e", tip = "splice slope at end of last file" },
}
 
dsp["Sfedit Joindyn - Join soundfiles in loudness-patterned sequence, specified in pattern text file  "] = {
  cmds = { exe = "sfedit", mode = "joindyn", tip = "As with SFEDIT JOINSEQ, this function splices together soundfiles in the order in which they are listed. In addition, it specifies the relative loudness of each soundfile in the sequence. Note that in Sound Loom similar patterns can be created as mixfiles, giving you the possibility to change the entry times of the sounds. This involves an advanced use of the TABLE EDITOR. Also note that EXTEND SEQUENCE2 allows you to put several sounds into a patterned sequence using any timing sequence (and patterns of levels). Musical applications; SFEDIT JOINDYN provides a quick and direct way to create a group of soundfiles in any order, with patterns of dynamic level" },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "pattern", min = 0, max = 0, input = "brk", def = 0, tip = "text file containing a pattern of soundfile level pairs. The soundfiles are identified by numbers, with the numbering following the order in which they are listed, starting with the number 1. Level range is 0 to 1. Example (repeating and fading)" },
  arg5 = { name = "splicelen", switch = "-w", min = 0, max = length, def = 15, tip = "duration of splice in milliseconds (Default: 15ms)" },
  arg6 = { name = "-b", switch = "-b", tip = "splice slope at start of first file" },
  arg7 = { name = "-e", switch = "-e", tip = "splice slope at end of last file" },
}
 
dsp["Sfedit Joinseq - Join soundfiles in patterned sequence specified in a text file"] = {
  cmds = { exe = "sfedit", mode = "joinseq", tip = "SFEDIT JOINSEQ allows a set of sounds to be joined, end to end, in a pattern. Any sound in the sequence can be repeated any number of times. The pattern is specified in a text file, with the sound pattern value separated by spaces or newlines. The soundfiles are identified by numbers, with the numbering following the order in which they are listed, starting with the number 1. Thus only the numbers are needed in the pattern file, as illustrated above. Maxlen enables you to use an existing pattern with fewer active components (i.e., apply data reduction). Musical applications; This would be a way to generating patterns of events, such as sequences of vocal syllables, with a melodic flavour – given that the source material has sufficiently different pitch levels and that you are going the use the whole length of each sound before the next sound begins(this is only a splice operation)." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "pattern", input = "txt", tip = " text file containing a pattern of numbers specifying the sequence (ordering) of soundfiles to use. The soundfiles are identified by numbers, with the numbering following the order in which they are listed, starting with the number 1." },
  arg5 = { name = "splicelen", switch = "-w", min = 0, max = length, def = 15, tip = "duration of splice in milliseconds (Default: 15ms)" },
  arg6 = { name = "maxlen", switch = "-m", min = 0, max = length, def = 15, tip = "duration of splice in milliseconds (Default: 15ms)" },
  arg7 = { name = "-b", switch = "-b", tip = "splice slope at start of first file" },
  arg8 = { name = "-e", switch = "-e", tip = "splice slope at end of last file" },
}
 
dsp["Sfedit Masks 1 - Mask specified chunks of a sound, with silence specified in a text file (Time in seconds)"] = {
  cmds = { exe = "sfedit", mode = "masks 1", channels = "any", tip = "1 Time in seconds - The excisefile takes into consideration the overall length of the input soundfile. With paired start and end times on separate lines, it specifies the start and end of silences. These are inserted into the infile replacing what was there previously. The resultant soundfile is therefore the same length as the original input. It is convenient to be able to create several silences at once. The splice parameter enables you to smooth the edges of these silences to varying degrees. Musical applications; Some ideas: create pulses of sound and silence,design excise times for two different soundfiles such that they overlap or interlock exactly. Then you can SUBMIX MIX the two soundfiles or SUBMIX INTERLEAVE the two soundfiles so that the two sounds alternate. The latter merges mono files into a multichannel file, so the sound and silence of each input would end up on different channels." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "a textfile with (paired) start and end times of chunks to be masked. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = length, def = 15, tip = "splice window in milliseconds (Default: 15)" },
}

dsp["Sfedit Masks 2 - Mask specified chunks of a sound, with silence specified in a text file (Time as sample count)"] = {
  cmds = { exe = "sfedit", mode = "masks 2", channels = "any", tip = "2 Time as sample count (rounded to multiples of channel-count) - The excisefile takes into consideration the overall length of the input soundfile. With paired start and end times on separate lines, it specifies the start and end of silences. These are inserted into the infile replacing what was there previously. The resultant soundfile is therefore the same length as the original input. It is convenient to be able to create several silences at once. The splice parameter enables you to smooth the edges of these silences to varying degrees. Musical applications; Some ideas: create pulses of sound and silence,design excise times for two different soundfiles such that they overlap or interlock exactly. Then you can SUBMIX MIX the two soundfiles or SUBMIX INTERLEAVE the two soundfiles so that the two sounds alternate. The latter merges mono files into a multichannel file, so the sound and silence of each input would end up on different channels." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "a textfile with (paired) start and end times of chunks to be masked. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = length, def = 15, tip = "splice window in milliseconds (Default: 15)" },
}

dsp["Sfedit Masks 3 - Mask specified chunks of a sound, with silence specified in a text file (Time as grouped sample count)"] = {
  cmds = { exe = "sfedit", mode = "masks 3", channels = "any", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs) - The excisefile takes into consideration the overall length of the input soundfile. With paired start and end times on separate lines, it specifies the start and end of silences. These are inserted into the infile replacing what was there previously. The resultant soundfile is therefore the same length as the original input. It is convenient to be able to create several silences at once. The splice parameter enables you to smooth the edges of these silences to varying degrees. Musical applications; Some ideas: create pulses of sound and silence,design excise times for two different soundfiles such that they overlap or interlock exactly. Then you can SUBMIX MIX the two soundfiles or SUBMIX INTERLEAVE the two soundfiles so that the two sounds alternate. The latter merges mono files into a multichannel file, so the sound and silence of each input would end up on different channels." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "excisefile", input = "txt", tip = "a textfile with (paired) start and end times of chunks to be masked. These must be in increasing time order." },
  arg4 = { name = "splice", switch = "-w", min = 0, max = length, def = 15, tip = "splice window in milliseconds (Default: 15)" },
}
 
dsp["Sfedit Replace - Insert a 2nd sound into an existing sound, replacing part of the original sound "] = {
  cmds = { exe = "sfedit", mode = "replace", tip = "There is already a process 'insert sound' (SFEDIT INSERT) which allows you to insert a 2nd sound into an existing sound, either by overwriting the original sound at the point of insertion, or cutting the original sound at the point of insertion, inserting the 2nd sound, and then continuing with the first sound from the place where it was cut. This process allows you to overwrite a SPECIFIED SEGMENT of the original sound with the new sound, even where this is not the same length as the inserted sound. Note however, that the 2nd sound must be AT LEAST AS LONG as the gap created in the original sound . Musical applications; This function provides a overwrite facility." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "time", min = 0, max = length/1000, def = 0, tip = "the time at which the 2nd soundfile is to be inserted into the 1st soundfile" },
  arg5 = { name = "endtime", switch = "-w", min = 0, max = length/1000, def = 0, tip = "the endtime of the segment in the original soundfile to be replaced" },
  arg6 = { name = "splice", switch = "-w", min = 0, max = 1000, def = 15, tip = "splice window in milliseconds. Default: 15 ms" },
  arg7 = { name = "level", switch = "-l", min = 0, max = 1000, def = 1, tip = "gain multiplier on the inserted soundfile. Default: 1.0 " },
}
 
dsp["Sfedit Sphinx 1 - In Sequence - switch between several files, with different switch times, to make a new sound, uses text file "] = {
  cmds = { exe = "sfedit", mode = "sphinx 1", tip = "SPHINX provides a way to mix up the contents of several soundfiles in a semi-controlled way. An example from Gustav Ciamaga provides a useful illustration. He has determined the start-time of each syllable in soundfiles containing spoken text. He then used SPHINX to rearrange these syllables in various ways, maintaining their integrity. If segment start-times were to be chosen at random, then the input material would be divided up with no regard for internal shapes, perhaps picking out silences. The effect of this would vary depending on the nature of the sonic material. Note that the modes and options can be thought of as progressive degrees of randomisation: In Mode 1 both soundfiles and times remain in sequential order – though the weight flag (-w) could be used to make the first soundfile predominate, and the soundfile-randomisation flag (-r) could be used to mix up the order of the soundfiles. For the purposes of this list of progressive randomisations, let us say that it the latter flag is saved until last." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing the times in seconds at which the input sound(s) are divided into segments" },
  arg5 = { name = "splicelen", min = 2, max = 15, def = 7, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg7 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}
 
dsp["Sfedit Sphinx 2 - Permutated - switch between several files, with different switch times, to make a new sound, uses text file "] = {
  cmds = { exe = "sfedit", mode = "sphinx 2", tip = "SPHINX provides a way to mix up the contents of several soundfiles in a semi-controlled way. An example from Gustav Ciamaga provides a useful illustration. He has determined the start-time of each syllable in soundfiles containing spoken text. He then used SPHINX to rearrange these syllables in various ways, maintaining their integrity. If segment start-times were to be chosen at random, then the input material would be divided up with no regard for internal shapes, perhaps picking out silences. The effect of this would vary depending on the nature of the sonic material. Note that the modes and options can be thought of as progressive degrees of randomisation:  In Mode 2 the order of rows is set by a random permutation. The program then selects a time from each row and produces a segment accordingly, until all rows have been used. Then a new row-order is chosen, etc. All the rows are used once before a new permutation of the row-order begins. Note that (unless the -r flag is set) the input soundfiles are still chosen strictly in order." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing the times in seconds at which the input sound(s) are divided into segments" },
  arg5 = { name = "splicelen", min = 2, max = 15, def = 7, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "segcnt", min = 0, max = 100, def = 10, tip = "the number of segments to use in the output" },
  arg7 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg8 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}
 
dsp["Sfedit Sphinx 3 - Random Choice - switch between several files, with different switch times, to make a new sound, uses text file "] = {
  cmds = { exe = "sfedit", mode = "sphinx 3", tip = "SPHINX provides a way to mix up the contents of several soundfiles in a semi-controlled way. An example from Gustav Ciamaga provides a useful illustration. He has determined the start-time of each syllable in soundfiles containing spoken text. He then used SPHINX to rearrange these syllables in various ways, maintaining their integrity. If segment start-times were to be chosen at random, then the input material would be divided up with no regard for internal shapes, perhaps picking out silences. The effect of this would vary depending on the nature of the sonic material. Note that the modes and options can be thought of as progressive degrees of randomisation:   In Mode 3 the rows are selected entirely at random – i.e., not even a reordering by permutation. The program thus selects its time from any row and produces the corresponding segment. The input soundfiles are still chosen in strict order (the order in which the user supplied them) unless the -r flag is set. In this case, the time in each row (and therefore the order of the input soundfiles) is randomly permutated. This reordering of the soundfiles repeats each time all the input soundfiles have been used, similar to the way the rows are changed round in Mode 2." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing the times in seconds at which the input sound(s) are divided into segments" },
  arg5 = { name = "splicelen", min = 2, max = 15, def = 7, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "segcnt", min = 0, max = 100, def = 10, tip = "the number of segments to use in the output" },
  arg7 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg8 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}
 
dsp["Sfedit Twixt 1 - In Sequence - Switch between several files, to make a new sound , uses text file "] = {
  cmds = { exe = "sfedit", mode = "twixt 1", tip = "1 In Sequence – Imagine all soundfiles are running in parallel on a multitrack. Switch from one sound to another at switch times, where the Nth switch-time in one file corresponds to the Nth in another file, and the times are the same on all the tracks. The applications for TWIXT are the same as for SPHINX, but with the difference that the same times and lengths apply to all the input soundfiles – only the soundfile to which they apply changes (cycles round the inputs in various ways by changing the order of the times), and the soundfile order itself can be randomised. Note that while HOUSEKEEP EXTRACT Mode 6 results in a list of onset times, as noted above, SFEDIT TWIXT Mode 4 is one way to use these times to create separate soundfiles that start with these times and end with the next time in the list – or the end of the input soundfile. On this point, note the advice provided by Gustav Ciamaga." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing a single column of ascending times in seconds at which the output soundfile switches between the input soundfile(s)" },
  arg5 = { name = "splicelen", min = 0, max = 1000, def = 15, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg7 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}
 
dsp["Sfedit Twixt 2 - Permutated - Switch between several files, to make a new sound to make a new sound, uses text file "] = {
  cmds = { exe = "sfedit", mode = "twixt 2", tip = "2 Permuted – Similar to Mode 1 but the time-segment order is randomly permutated. The applications for TWIXT are the same as for SPHINX, but with the difference that the same times and lengths apply to all the input soundfiles – only the soundfile to which they apply changes (cycles round the inputs in various ways by changing the order of the times), and the soundfile order itself can be randomised. Note that while HOUSEKEEP EXTRACT Mode 6 results in a list of onset times, as noted above, SFEDIT TWIXT Mode 4 is one way to use these times to create separate soundfiles that start with these times and end with the next time in the list – or the end of the input soundfile. On this point, note the advice provided by Gustav Ciamaga." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing a single column of ascending times in seconds at which the output soundfile switches between the input soundfile(s): " },
  arg5 = { name = "splicelen", min = 0, max = 1000, def = 10, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "segcnt", min = 0, max = 100, def = 10, tip = "the number of segments to use in the output" },
  arg7 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg8 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}
 
dsp["Sfedit Twixt 3 - Random choice - Switch between several files, to make a new sound to make a new sound, uses text file "] = {
  cmds = { exe = "sfedit", mode = "twixt 3", tip = "3 Random Choice – Similar to Mode 1 but choose any time-segment at random as the next segment. The applications for TWIXT are the same as for SPHINX, but with the difference that the same times and lengths apply to all the input soundfiles – only the soundfile to which they apply changes (cycles round the inputs in various ways by changing the order of the times), and the soundfile order itself can be randomised. Note that while HOUSEKEEP EXTRACT Mode 6 results in a list of onset times, as noted above, SFEDIT TWIXT Mode 4 is one way to use these times to create separate soundfiles that start with these times and end with the next time in the list – or the end of the input soundfile. On this point, note the advice provided by Gustav Ciamaga." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "switch-times", input = "txt", tip = "text file containing a single column of ascending times in seconds at which the output soundfile switches between the input soundfile(s): " },
  arg5 = { name = "splicelen", min = 0, max = 1000, def = 10, tip = "the duration of the splices, in milliseconds (Range: 2 to 15 milliseconds)" },
  arg6 = { name = "segcnt", min = 0, max = 100, def = 10, tip = "the number of segments to use in the output" },
  arg7 = { name = "weight", switch = "-w", min = 0, max = 10, def = 0, tip = "when set, infile1 occurs weight times more often than the other infiles" },
  arg8 = { name = "-r", switch = "-r", tip = "when set, the order of files used is randomly permutated, otherwise the files retain the order in which they were invoked." },
}

--[[

dsp["[Doesn't work - multiple outputs] Sfedit - Randchunks, cut chunks from a soundfile, randomly - multiple sound outputs"] = { 
  cmds = { exe = "sfedit", mode = "randchunks", tip = "SFEDIT RANDCHUNKS is like SFEDIT RANDCUTS but enables you to be more specific about the number of chunks (chunkcnt) and their length (minchunk and the optional maxchunk). Each chunk is saved as a new soundfile, with a name derived from the name of the infile. Musical applications; The number and length controls make it possible to make a controlled number of chunks of random length within a specified range. The ability to focus on the start of the soundfile enables you to explore the qualities of the attack portion of the sound." },
  arg1 = { name = "chunkcnt", min = 0, max = 100, tip = "the number of chunks to cut" },
  arg2 = { name = "minchunk", min = 0, max = length / 1000, tip = "the minimum length of the chunks, in seconds" },
  arg3 = { name = "maxchunk", switch = "-m", min = 0, max = length / 1000, tip = "the maximum lengths of the chunks, in seconds" },
  arg4 = { name = "-l", switch = "-l", tip = "chunks chosen are evenly distributed over the file (Default: random distribution)" },
  arg5 = { name = "-s", switch = "-s", tip = "all chunks start at the beginning of the file " },
} 

--]]

--[[

dsp["[Doesn't work - multiple outputs] Sfedit - Randcuts - Cut soundfile into pieces with cuts at random times - multiple sound outputs"] = { 
  cmds = { exe = "sfedit", mode = "randcuts", tip = "SFEDIT RANDCUTS provides a way to cut up a soundfile into several portions of a specified average length, saving each as a separate soundfile. The amount of difference in the lengths can be adjusted with the scattering parameter: the regularity of the lengths gets less and less as scatter increases. Musical applications; This could be a way of multiplying source material when a given soundfile has sufficient variation in its contents to justify the procedure." },
  arg1 = { name = "average-chunklen", min = 0, max = length / 1000, tip = "the average length of the chunks to cut" },
  arg2 = { name = "scattering", min = 0, max = 8, def = 4, tip = "controls the amount of variation in the length of the cuts (Range: 0 to 8)" },
}

--]]

--[[ audio input, multiple audio outputs
dsp["Sfedit Cutmany - 1 Time in seconds - Cut and keep several segments of a sound."] = {
cmds = { exe = "sfedit", mode = "cutmany 1", tip = "1 Time in seconds. This is an extension of the basic "edit cutout and keep" (SFEDIT CUT) to allow several segments to be cut from a file at a single pass. The start and end times of the cuts are placed in a textfile which the process reads. Musical applications; This function is useful if you want to extract several interesting features from a source sound. You can search the sound first, noting down the edit times of the sections you want, write them in a textfile, and then use the textfile to cut those segments from the source in a single pass, saving them to a generic name. For example, if your generic name is pop, the various cuts will be named pop1, pop2 etc.)" }, 
arg1 = { name = "cuttimes", input = "txt", tip = "text file of time-pairs for the start and end of each segment" },
arg2 = { name = "splicelen", min = 0, max = 1000, def = 15, tip = "the duration of the splice window in milliseconds: i.e., the amount of time to rise from and fall back to zero amplitude. NB: REQUIRED (not optional as in the other CUT functions)." },
}
--]]

--[[ audio input, multiple audio outputs
dsp["Sfedit Cutmany - 2 Time as sample count (rounded to multiples of channel-count"] = {
cmds = { exe = "sfedit", mode = "cutmany 2", tip = "2 Time as sample count (rounded to multiples of channel-count. This is an extension of the basic "edit cutout and keep" (SFEDIT CUT) to allow several segments to be cut from a file at a single pass. The start and end times of the cuts are placed in a textfile which the process reads. Musical applications; This function is useful if you want to extract several interesting features from a source sound. You can search the sound first, noting down the edit times of the sections you want, write them in a textfile, and then use the textfile to cut those segments from the source in a single pass, saving them to a generic name. For example, if your generic name is pop, the various cuts will be named pop1, pop2 etc.)" }, 
arg1 = { name = "cuttimes", input = "txt", tip = "text file of time-pairs for the start and end of each segment" },
arg2 = { name = "splicelen", min = 0, max = 1000, def = 15, tip = "the duration of the splice window in milliseconds: i.e., the amount of time to rise from and fall back to zero amplitude. NB: REQUIRED (not optional as in the other CUT functions)." },
}
--]]

--[[ audio input, multiple audio outputs
dsp["Sfedit Cutmany - 3 Time as grouped-sample count (e.g., 3 = 3 stereo pairs)"] = {
cmds = { exe = "sfedit", mode = "cutmany 3", tip = "3 Time as grouped-sample count (e.g., 3 = 3 stereo pairs). This is an extension of the basic "edit cutout and keep" (SFEDIT CUT) to allow several segments to be cut from a file at a single pass. The start and end times of the cuts are placed in a textfile which the process reads. Musical applications; This function is useful if you want to extract several interesting features from a source sound. You can search the sound first, noting down the edit times of the sections you want, write them in a textfile, and then use the textfile to cut those segments from the source in a single pass, saving them to a generic name. For example, if your generic name is pop, the various cuts will be named pop1, pop2 etc.)" }, 
arg1 = { name = "cuttimes", input = "txt", tip = "text file of time-pairs for the start and end of each segment" },
arg2 = { name = "splicelen", min = 0, max = 1000, def = 15, tip = "the duration of the splice window in milliseconds: i.e., the amount of time to rise from and fall back to zero amplitude. NB: REQUIRED (not optional as in the other CUT functions)." },
}
--]]

--[[ doesn't work, multiple audio files as output
dsp["Sfedit - Zcuts -  1 Time in seconds - Cut out and keep segments of a MONO soundfile, cutting at zero crossings (no splices) "] = { 
  cmds = { exe = "sfedit", mode = "zcuts 1", tip = "1 Time in seconds. This process uses an alternative method to 'splice' the sound segments, cutting them at the nearest zero-crossings in the signal, rather than making a splice slope of default or specified duration . Nevertheless the cuts should be clickless. Musical Applications; This is a different way of cutting out a segment of sound. The start and end times you give are approximate because the nearest zero points will probably not be precisely at those times. The cuttimes data file enables you to specify the start and end times for several cuts. As many new soundfiles are made as there are data pairs in the file, with a number added to your generic name to create the output filenames. For example, if your generic name is pop, the various cuts will be named pop1, pop2 etc.)." },
  arg1 = { name = "cuttimes", input = "txt", tip = "a textfile of time-pairs for the start and end of each segment to cut " },
}
--]]

--[[ doesn't work, multiple audio files as output
dsp["Sfedit - Zcuts -  2 Time as sample count (rounded to multiples of channel-count) - Cut out and keep segments of a MONO soundfile, cutting at zero crossings"] = { 
  cmds = { exe = "sfedit", mode = "zcuts 2", tip = "2 Time as sample count (rounded to multiples of channel-count). This process uses an alternative method to 'splice' the sound segments, cutting them at the nearest zero-crossings in the signal, rather than making a splice slope of default or specified duration . Nevertheless the cuts should be clickless. Musical Applications; This is a different way of cutting out a segment of sound. The start and end times you give are approximate because the nearest zero points will probably not be precisely at those times. The cuttimes data file enables you to specify the start and end times for several cuts. As many new soundfiles are made as there are data pairs in the file, with a number added to your generic name to create the output filenames. For example, if your generic name is pop, the various cuts will be named pop1, pop2 etc.)." },
  arg1 = { name = "cuttimes", input = "txt", tip = "a textfile of time-pairs for the start and end of each segment to cut " },
}
--]]

--[[ audio in, multiple audio outputs
dsp["Sfedit Syllables - 1 Time in seconds - Separate out vocal syllables "] = { 
  cmds = { exe = "sfedit", mode = "syllables 1", tip = "1 Time in seconds. The syllables in speech are difficult to separate one from the other simply by editing. By their very nature, the sounds of speech flow naturally one into another, and there is no 'natural' cutting point between them. This process compensates for this problem by shaving a little bit from the end of the previous syllable and a little bit from the start of the following syllable, thereby, for every syllable, giving separated syllables that are more convincing. The cuttimes file needs to be carefully constructed by noting in an appropriate sound editor the start and end times of the syllables you want to extract . They are given with each start end time pair given on separate lines: 0.0058 0.215 0.319  0.720 1.01   1.56 1.72   2.00 2.1    2.467 2.7    3.04 . You could then excise the syllables right away, but the advantage of using SFEDIT SYLLABLES is ability to allow for syllable overlap, thus capture endings and beginnings of syllables that overlap and would otherwise be lost. Graphic sound editors should work for this purpose, such as Audition, Sound Forge or the new graphic display and editing facilities on the Sound Loom, as you can block out and hear a portion of soundfile. In Sound Loom, you ALT Mouse Click on a soundfile on the Workspace to access these facilities. Using ears only, Soundshaper's 'Play using Markers' facility (i.e., Play FROM .. TO) is also a straightforward way to find the edit points, and gives a time display accurate to three decimal places. Gaps between times need to be large enough to accommodate the dovetail (overlap) and the splice (join slope). The maximum dovetail is 20 ms, i.e., 0.02 seconds, so this time distance should be regarded as the minimum. This applies to the length of the syllables as well as the time between syllables. Syllables that are too short, it should be added, will not be useful as soundfiles because the sound will disappear in the splice. As a rule of thumb, regard 200 ms as the shortest practical length of a syllable. A series of soundfiles using the generic name as the base are created. If the generic name is 'speechsyl', the separate soundfiles containing each syllable will be 'speechsy1', 'speechsy2' etc. The soundfile extension is appended by the program, as usual. A variant of SFEDIT CUTMANY, all the soundfiles are created in one pass. Musical applications;  SFEDIT SYLLABLES can be used to separate the syllables of speech, the individual note events from a melody performed on an instrument with strong transitional characteristics where it passes from one note to another" },
  arg1 = { name = "cuttimes", input = "txt", tip = "a textfile of time-pairs for the start and end of each segment to cut " },
  arg2 = { name = "dovetail", min = 1, max = 20, def = 5, , tip = "the time in milliseconds to allow for syllable overlap. Range: 1 to 20 ms" },
  arg3 = { name = "splicelen", min = 0, max = 1000, def = 10, , tip = "the duration of the splice window in milliseconds (cannot be shorter than the time between any two times)" },
  arg4 = { name = "-p", tip = "forces the process to cut PAIRS of syllables " },
}
--]]

--[[ audio in, multiple audio outputs
dsp["Sfedit Syllables - 2 Time as sample count (rounded to multiples of channel-count) - Separate out vocal syllables "] = { 
  cmds = { exe = "sfedit", mode = "syllables 2", tip = "2 Time as sample count (rounded to multiples of channel-count). The syllables in speech are difficult to separate one from the other simply by editing. By their very nature, the sounds of speech flow naturally one into another, and there is no 'natural' cutting point between them. This process compensates for this problem by shaving a little bit from the end of the previous syllable and a little bit from the start of the following syllable, thereby, for every syllable, giving separated syllables that are more convincing. The cuttimes file needs to be carefully constructed by noting in an appropriate sound editor the start and end times of the syllables you want to extract . They are given with each start end time pair given on separate lines: 0.0058 0.215 0.319  0.720 1.01   1.56 1.72   2.00 2.1    2.467 2.7    3.04 . You could then excise the syllables right away, but the advantage of using SFEDIT SYLLABLES is ability to allow for syllable overlap, thus capture endings and beginnings of syllables that overlap and would otherwise be lost. Graphic sound editors should work for this purpose, such as Audition, Sound Forge or the new graphic display and editing facilities on the Sound Loom, as you can block out and hear a portion of soundfile. In Sound Loom, you ALT Mouse Click on a soundfile on the Workspace to access these facilities. Using ears only, Soundshaper's 'Play using Markers' facility (i.e., Play FROM .. TO) is also a straightforward way to find the edit points, and gives a time display accurate to three decimal places. Gaps between times need to be large enough to accommodate the dovetail (overlap) and the splice (join slope). The maximum dovetail is 20 ms, i.e., 0.02 seconds, so this time distance should be regarded as the minimum. This applies to the length of the syllables as well as the time between syllables. Syllables that are too short, it should be added, will not be useful as soundfiles because the sound will disappear in the splice. As a rule of thumb, regard 200 ms as the shortest practical length of a syllable. A series of soundfiles using the generic name as the base are created. If the generic name is 'speechsyl', the separate soundfiles containing each syllable will be 'speechsy1', 'speechsy2' etc. The soundfile extension is appended by the program, as usual. A variant of SFEDIT CUTMANY, all the soundfiles are created in one pass. Musical applications;  SFEDIT SYLLABLES can be used to separate the syllables of speech, the individual note events from a melody performed on an instrument with strong transitional characteristics where it passes from one note to another" },
  arg1 = { name = "cuttimes", input = "txt", tip = "a textfile of time-pairs for the start and end of each segment to cut " },
  arg2 = { name = "dovetail", min = 1, max = 20, def = 5, , tip = "the time in milliseconds to allow for syllable overlap. Range: 1 to 20 ms" },
  arg3 = { name = "splicelen", min = 0, max = 1000, def = 10, , tip = "the duration of the splice window in milliseconds (cannot be shorter than the time between any two times)" },
  arg4 = { name = "-p", tip = "forces the process to cut PAIRS of syllables " },
}
--]]

--[[ audio in, multiple audio outputs
dsp["Sfedit Syllables - 3 Time as grouped sample count (e.g., 3 = 3 stereo pairs)  - Separate out vocal syllables "] = { 
  cmds = { exe = "sfedit", mode = "syllables 3", tip = "3 Time as grouped sample count (e.g., 3 = 3 stereo pairs) . The syllables in speech are difficult to separate one from the other simply by editing. By their very nature, the sounds of speech flow naturally one into another, and there is no 'natural' cutting point between them. This process compensates for this problem by shaving a little bit from the end of the previous syllable and a little bit from the start of the following syllable, thereby, for every syllable, giving separated syllables that are more convincing. The cuttimes file needs to be carefully constructed by noting in an appropriate sound editor the start and end times of the syllables you want to extract . They are given with each start end time pair given on separate lines: 0.0058 0.215 0.319  0.720 1.01   1.56 1.72   2.00 2.1    2.467 2.7    3.04 . You could then excise the syllables right away, but the advantage of using SFEDIT SYLLABLES is ability to allow for syllable overlap, thus capture endings and beginnings of syllables that overlap and would otherwise be lost. Graphic sound editors should work for this purpose, such as Audition, Sound Forge or the new graphic display and editing facilities on the Sound Loom, as you can block out and hear a portion of soundfile. In Sound Loom, you ALT Mouse Click on a soundfile on the Workspace to access these facilities. Using ears only, Soundshaper's 'Play using Markers' facility (i.e., Play FROM .. TO) is also a straightforward way to find the edit points, and gives a time display accurate to three decimal places. Gaps between times need to be large enough to accommodate the dovetail (overlap) and the splice (join slope). The maximum dovetail is 20 ms, i.e., 0.02 seconds, so this time distance should be regarded as the minimum. This applies to the length of the syllables as well as the time between syllables. Syllables that are too short, it should be added, will not be useful as soundfiles because the sound will disappear in the splice. As a rule of thumb, regard 200 ms as the shortest practical length of a syllable. A series of soundfiles using the generic name as the base are created. If the generic name is 'speechsyl', the separate soundfiles containing each syllable will be 'speechsy1', 'speechsy2' etc. The soundfile extension is appended by the program, as usual. A variant of SFEDIT CUTMANY, all the soundfiles are created in one pass. Musical applications;  SFEDIT SYLLABLES can be used to separate the syllables of speech, the individual note events from a melody performed on an instrument with strong transitional characteristics where it passes from one note to another" },
  arg1 = { name = "cuttimes", input = "txt", tip = "a textfile of time-pairs for the start and end of each segment to cut " },
  arg2 = { name = "dovetail", min = 1, max = 20, def = 5, , tip = "the time in milliseconds to allow for syllable overlap. Range: 1 to 20 ms" },
  arg3 = { name = "splicelen", min = 0, max = 1000, def = 10, , tip = "the duration of the splice window in milliseconds (cannot be shorter than the time between any two times)" },
  arg4 = { name = "-p", tip = "forces the process to cut PAIRS of syllables " },
}
--]]

------------------------------------------------
-- sfprops
------------------------------------------------

dsp["Sfprops - Display soundfile details, with WAVE-EX speaker positions "] = {
  cmds = { exe = "sfprops", mode = "", tip = "This is a purely informative program. In addition to reporting the standard properties of a soundfile, it gives details of any WAVE_EX speaker positions and identifies the layout when it is one of the standard ones as supported by other Multi-channel Toolkit programs. It prints all PEAK information if present. Use CHXFORMAT to find low-level information on the WAVE_EX speaker mask. Note that it will identify 7.1 and cube speaker layouts. The PEAK chunk report includes a level report in dB. SFPROPS displays the soundfile properties on the console: duration, number of sample frames, sample rate, etc." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
}
 
------------------------------------------------
-- shifter
------------------------------------------------
 
dsp["Shifter - Generate simultaneous repetition streams, shifting rhythmic pulse from one to another specified in cycles text file "] = {
  cmds = { exe = "shifter", mode = "shifter 1", channels = "1in2out", tip = "SHIFTER sets up several sound streams. In each sound stream the source sound repeats at a fixed tempo in (specified) cycles. The repetition-times for each cycle are arranged so that the streams will resynchronise (all start at the same instant) after a specified number of cycles in each stream. For example, with cycles 11,12,13, three streams are set up which repeat the sound 11,12 and 13 times, respectively, before the streams resynchronise. (This represents three streams with their tempi in the relationship 11:12:13.) Note that the sounds themselves are NOT time-stretched; only the timings-between-repetitions are different in the different cycles. The various 'focusing' parameters determine which of the simultaneous tempi is the most prominent at any time. Care should be taken to keep the input level very low, to avoid overflow as sounds are mixed in the SHIFTER process. Modify loudness 4 (Force level) might be set to e.g. 0.05." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "cycles", input = "txt", tip = "a textfile listing the number of beats in each cycle." },
  arg4 = { name = "cycdur", min = 0.01, max = cycles, def = 0.01, tip = "the duration of one complete cycle" },
  arg5 = { name = "dur", min = 0, max = 60, def = 10, tip = "the required duration of the output sound" },
  arg6 = { name = "ochans", min = 2, max = 2, def = 2, tip = "number of channels in the output sound" },
  arg7 = { name = "subdiv", min = 4, max = 400, def = 4, tip = "the minimum division of the beat: it needs to be > 4 and a multiple of 2 and/or 3" },
  arg8 = { name = "linger", min = 3, max = 64, def = 4, tip = "the number of cycles that are to remain in a fixed focus" },
  arg9 = { name = "transit", min = 1, max = 1000, def = 10, tip = " the number of cycles that are to make a transition to the next focus. The sum of linger and transit must be >= 1." },
  arg10 = { name = "boost", min = 0, max = 100, def = 0.1, tip = "with standard stream level 'L', add boost * L to focus stream level." },
  arg11 = { name = "-z", switch = "-z", tip = "This flag causes focus to ZIGZAG through the cycles." },
  arg12 = { name = "-r", switch = "-r", tip = "This flag causes focus to select a RANDOM order of the cycles." },
  arg13 = { name = "-l", switch = "-l-", tip = "If the number of output channels is greater than 2, the loudspeaker layout is assumed to be surround-sound. The -l flag changes the loudspeaker arrangement to a linear array, with a leftmost and rightmost loudspeaker." },
}
 
dsp["Shifter 2 - Generate simultaneous repetition streams, shifting rhythmic pulse from one to another specified in cycles text file "] = {
  cmds = { exe = "shifter", mode = "shifter 2", channels = "1in2out", tip = "2 The number of input files must equal the number of cycles. The program assigns the input files, in order, to the cycles, in order. SHIFTER sets up several sound streams. In each sound stream the source sound repeats at a fixed tempo in (specified) cycles. The repetition-times for each cycle are arranged so that the streams will resynchronise (all start at the same instant) after a specified number of cycles in each stream. For example, with cycles 11,12,13, three streams are set up which repeat the sound 11,12 and 13 times, respectively, before the streams resynchronise. (This represents three streams with their tempi in the relationship 11:12:13.) Note that the sounds themselves are NOT time-stretched; only the timings-between-repetitions are different in the different cycles. The various 'focusing' parameters determine which of the simultaneous tempi is the most prominent at any time. Care should be taken to keep the input level very low, to avoid overflow as sounds are mixed in the SHIFTER process. Modify loudness 4 (Force level) might be set to e.g. 0.05." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "cycles", input = "txt", tip = "a textfile listing the number of beats in each cycle." },
  arg5 = { name = "cycdur", min = 0.01, max = cycles, def = 0.01, tip = "the duration of one complete cycle" },
  arg6 = { name = "dur", min = 0, max = 60, def = 10, tip = "the required duration of the output sound" },
  arg7 = { name = "ochans", min = 2, max = 2, def = 2, tip = "number of channels in the output sound" },
  arg8 = { name = "subdiv", min = 4, max = 400, def = 4, tip = "the minimum division of the beat: it needs to be > 4 and a multiple of 2 and/or 3" },
  arg9 = { name = "linger", min = 3, max = 64, def = 4, tip = "the number of cycles that are to remain in a fixed focus" },
  arg10 = { name = "transit", min = 1, max = 1000, def = 10, tip = " the number of cycles that are to make a transition to the next focus. The sum of linger and transit must be >= 1." },
  arg11 = { name = "boost", min = 0, max = 100, def = 0.1, tip = "with standard stream level 'L', add boost * L to focus stream level." },
  arg12 = { name = "-z", switch = "-z", tip = "This flag causes focus to ZIGZAG through the cycles." },
  arg13 = { name = "-r", switch = "-r", tip = "This flag causes focus to select a RANDOM order of the cycles." },
  arg14 = { name = "-l", switch = "-l-", tip = "If the number of output channels is greater than 2, the loudspeaker layout is assumed to be surround-sound. The -l flag changes the loudspeaker arrangement to a linear array, with a leftmost and rightmost loudspeaker." },
}
 
------------------------------------------------
-- shrink
------------------------------------------------
 
dsp["Sfedit - 2 Shrink around midpoint - Repeat a sound, shortening it on each repetition"] = {
  cmds = { exe = "shrink", mode = "shrink 2", channels = "any", tip = "With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "shrinkage", min = 0, max = 1, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg4 = { name = "gap", min = length/1000, max = 60, def = length/1000, tip = "initial timestep between output events" },
  arg5 = { name = "contract", min = 0, max = 1, def = 0.6, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg6 = { name = "dur", min = length/1000*2, max = 32767, def = length/1000*2, tip = "the (minimum) duration of the output" },
  arg7 = { name = "spl", min = 2, max = 50, def = 6, tip = "splice length in milliseconds" },
  arg8 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg9 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg10 = { name = "rnd", switch = "-r", min = 0, max = 1, tip = "randomisation of timings after which events are regular in time" },
  arg11 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg12 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." },
}
 
dsp["Sfedit - 1 Shrink from end - Repeat a sound, shortening it on each repetition"] = {
  cmds = { exe = "shrink", mode = "shrink 1", channels = "any", tip = "With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "shrinkage", min = 0, max = 1, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg4 = { name = "gap", min = length/1000, max = 60, def = length/1000, tip = "initial timestep between output events" },
  arg5 = { name = "contract", min = 0, max = 1, def = 0.5, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg6 = { name = "dur", min = length/1000*2, max = 32767, def = length/1000*2, tip = "the (minimum) duration of the output" },
  arg7 = { name = "spl", min = 2, max = 50, def = 10, tip = "splice length in milliseconds" },
  arg8 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg9 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg10 = { name = "rnd", switch = "-r", min = 0, max = 1, tip = "randomisation of timings after which events are regular in time" },
  arg11 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg12 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." },
}
 
dsp["Sfedit - 4 Shrink from specified time - Repeat a sound, shortening it on each repetition"] = {
  cmds = { exe = "shrink", mode = "shrink 4", channels = "any", tip = "With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = 1, def = 0.3, tip = "time around which shrinkage takes place" },
  arg4 = { name = "shrinkage", min = 0, max = 1, def = 0.3, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg5 = { name = "gap", min = length/1000, max = 60, def = length/1000, tip = "initial timestep between output events" },
  arg6 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg7 = { name = "dur", min = length/1000*2, max = 32767, def = length/1000*2, tip = "the (minimum) duration of the output" },
  arg8 = { name = "spl", min = 2, max = 50, def = 8, tip = "splice length in milliseconds" },
  arg9 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg10 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg11 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg12 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg13 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." },
}
 
dsp["Sfedit - 3 Shrink from start - Repeat a sound, shortening it on each repetition"] = {
  cmds = { exe = "shrink", mode = "shrink 3", channels = "any", tip = "With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "shrinkage", min = 0, max = 1, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg4 = { name = "gap", min = length/1000, max = 60, def = length/1000, tip = "initial timestep between output events" },
  arg5 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg6 = { name = "dur", min = length/1000*2, max = 32767, def = length/1000*2, tip = "the (minimum) duration of the output" },
  arg7 = { name = "spl", min = 2, max = 50, def = 5, tip = "splice length in milliseconds" },
  arg8 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg9 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg10 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg11 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg12 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." },
}

--[[

dsp["[Doesn't work - multiple outputs] Shrink from specified time - 5 Shrink around found peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them"] = { 
  cmds = { exe = "shrink", mode = "shrink 5", tip = "5 - With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "shrinkage", min = 0.0, max = 100.0, def = 1.5, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once events become too short for splices" },
  arg2 = { name = "wsiz", min = 0, max = 100, def = 99, tip = "windowsize in milliseconds for extracting the envelope (Range: 1 to 100, Default: 100)" },
  arg3 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together. Events cannot overlap, so the minimum contraction is the maximum shrinkage." },
  arg4 = { name = "aft", min = 0, max = length/1000, tip = "time after which the shrinkage begins" },
  arg5 = { name = "spl", min = 0, max = length, tip = "splice length in milliseconds" },
  arg6 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg7 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg8 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg9 = { name = "len", switch = "-l", min = 0, max = length/1000, tip = "the minimum segment length before sound squeezing can begin; used with the -e flag" },
  arg10 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "the level relative to max below which found peaks are ignored (Range: 0 to 1, Default: 0)" }, 
  arg11 = { name = "skew", switch = "-g", min = 0, max = 1, def = 0.25, tip = "how the envelope is centred on the segment (Range: 0 to 1, Default 0.25" },
  arg12 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg13 = { name = "-i", switch = "-i", tip = "Inverse: reverse each segment in the output. Note that then reversing the outfile creates a stream of unreversed segments where segments expande/accelerate rather than shrink/contract." },
  arg14 = { name = "-e", switch = "-e", tip = "Even Squeeze: sounds shorten in a regular manner starting with the first squeezed segment. Note that squeezed sound lengths are not dependent on the length of the input segments." },
  arg15 = { name = "-o", switch = "-o", tip = "omit any events that are too quiet once a fixed end tempo has been reached" },
}

--]]

--[[

dsp["[Doesn't work - multiple outputs] Shrink from specified time - 6 Shrink around specified peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them"] = { 
  cmds = { exe = "shrink", mode = "shrink 6", tip = "Shrink around specified peaks and output each segment as a separate soundfile. With SHRINK a sound is repeated, and at each repetition it gets shorter in duration because some of it is removed." },
  arg1 = { name = "peaktimes", min = 0, max = 0, def = 0, input = "brk", tip = "a textfile list of the times where peaks occur in the input soundfile" },
  arg2 = { name = "shrinkage", min = 0.0, max = 100.0, def = 1.5, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once events become too short for splices" },
  arg3 = { name = "wsiz", min = 0, max = 100, def = 99, tip = "windowsize in milliseconds for extracting the envelope (Range: 1 to 100, Default: 100)" },
  arg4 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together. Events cannot overlap, so the minimum contraction is the maximum shrinkage." },
  arg5 = { name = "aft", min = 0, max = length/1000, tip = "time after which the shrinkage begins" },
  arg6 = { name = "spl", min = 0, max = length, tip = "splice length in milliseconds" },
  arg7 = { name = "small", switch = "-s", min = 0, max = length/1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg8 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg9 = { name = "len", switch = "-l", min = 0, max = length/1000, tip = "the minimum segment length before sound squeezing can begin; used with the -e flag" },
  arg10 = { name = "gate", switch = "-g", min = 0, max = 1, def = 0, tip = "the level relative to max below which found peaks are ignored (Range: 0 to 1, Default: 0)" }, 
  arg11 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg12 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg13 = { name = "-i", switch = "-i", tip = "Inverse: reverse each segment in the output. Note that then reversing the outfile creates a stream of unreversed segments where segments expande/accelerate rather than shrink/contract." },
  arg14 = { name = "-e", switch = "-e", tip = "Even Squeeze: sounds shorten in a regular manner starting with the first squeezed segment. Note that squeezed sound lengths are not dependent on the length of the input segments." },
  arg15 = { name = "-o", switch = "-o", tip = "omit any events that are too quiet once a fixed end tempo has been reached" },
}

--]]

--[[ audio input, multiple audio results, aparently also outputs a mixfile (text?)
dsp["Shrink 5 - Shrink around found peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them"] = { 
  cmds = { exe = "shrink", mode = "shrink 5", tip = "5 - Shrink around found peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them" },
  arg1 = { name = "shrinkage", min = 0.0, max = 1.0, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg2 = { name = "wsiz", min = 0, max = 100, def = 100, tip = "windowsize in milliseconds for extracting the envelope (Range: 1 to 100, Default: 100)" },
  arg3 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg4 = { name = "aft", min = 0, max = length/1000, def = 0, tip = "time after which the shrinkage begins" },
  arg5 = { name = "spl", min = 0, max = 1000, def = 5, tip = "splice length in milliseconds" },
  arg6 = { name = "small", switch = "-s", min = 0, max = length / 1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg7 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg8 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg9 = { name = "len", switch = "-l", min = 0, max = length/1000, tip = "the minimum segment length before sound squeezing can begin; used with the -e flag" },
  arg10 = { name = "-gate", switch = "-g", min = 0, max = 1, def = 0, tip = "the level relative to max below which found peaks are ignored (Range: 0 to 1, Default: 0)" },
  arg11 = { name = "-skew", switch = "-q", min = 0, max = 1, def = 0.25, tip = "how the envelope is centred on the segment (Range: 0 to 1, Default 0.25; 0.5 = central position and a zero value switches the flag off.)" },
  arg12 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg13 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." }, 
  arg14 = { name = "-e", switch = "-e", tip = "Even Squeeze: sounds shorten in a regular manner starting with the first squeezed segment." },
  arg15 = { name = "-o", switch = "-o", tip = "omit any events that are too quiet once a fixed end tempo has been reached" },
}
--]]

--[[ audio input, multiple audio results, aparently also outputs a mixfile (text?)
dsp["Shrink 6 - Shrink around specified peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them "] = { 
  cmds = { exe = "shrink", mode = "shrink 6", tip = "6 - Shrink around specified peaks and output each segment as a separate soundfile, also creating a mixfile with which to assemble them " },
  arg1 = { name = "peaktimes", input = "txt", tip = "a textfile list of the times where peaks occur in the input soundfile" },
  arg2 = { name = "shrinkage", min = 0.0, max = 1.0, tip = "shortening factor of sound from one repeat to the next. Shrinkage stops once envents become too short short splices." },
  arg3 = { name = "wsiz", min = 0, max = 100, def = 100, tip = "windowsize in milliseconds for extracting the envelope (Range: 1 to 100, Default: 100)" },
  arg4 = { name = "contract", min = 0, max = 1, tip = "shortening of gaps between output events: 1.0 = events are equally spaced, < 1.0 = events become closer together." },
  arg5 = { name = "aft", min = 0, max = length/1000, def = 0, tip = "time after which the shrinkage begins" },
  arg6 = { name = "spl", min = 0, max = 1000, def = 5, tip = "splice length in milliseconds" },
  arg7 = { name = "small", switch = "-s", min = 0, max = length / 1000, tip = "the minimum sound length, after which sounds are of equal length" },
  arg8 = { name = "min", switch = "-m", min = 0, max = 10, tip = "the minimum event separation, after which events are regular in time" },
  arg9 = { name = "len", switch = "-l", min = 0, max = length/1000, tip = "the minimum segment length before sound squeezing can begin; used with the -e flag" },
  arg10 = { name = "-gate", switch = "-g", min = 0, max = 1, def = 0, tip = "the level relative to max below which found peaks are ignored (Range: 0 to 1, Default: 0)" },
  arg11 = { name = "rnd", switch = "-r", min = 0, max = 10, tip = "randomisation of timings after which events are regular in time" },
  arg12 = { name = "-n", switch = "-n", tip = "equalise the maximum level of output events (if possible)" },
  arg13 = { name = "i", switch = "-i", tip = "Inverse: reverse each segment in the output." }, 
  arg14 = { name = "-e", switch = "-e", tip = "Even Squeeze: sounds shorten in a regular manner starting with the first squeezed segment." },
  arg15 = { name = "-o", switch = "-o", tip = "omit any events that are too quiet once a fixed end tempo has been reached" },
}
--]]
 
------------------------------------------------
-- silend
------------------------------------------------
 
dsp["Silend 1 - Add silence to the end of a soundfile - specify duration of the silence "] = {
  cmds = { exe = "silend", mode = "silend 1", channels = "any", tip = " Musical Applications; One application is to add silence at the end of each soundfile when assembling a list of soundfiles to be played as one sequence. This puts pauses between the sounds, pauses that are more appropriately placed at the end rather than the beginning." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "sildur", min = 0, max = 10, def = 2, tip = "duration of silence to add" },
}
 
dsp["Silend 2 - Add silence to the end of a soundfile 2 - specify total output duration"] = {
  cmds = { exe = "silend", mode = "silend 2", channels = "any", tip = " Musical Applications; One application is to add silence at the end of each soundfile when assembling a list of soundfiles to be played as one sequence. This puts pauses between the sounds, pauses that are more appropriately placed at the end rather than the beginning." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "outdur", min = 0, max = 30, def = 2, tip = "duration of silence to add" },
}

------------------------------------------------
-- sndinfo
------------------------------------------------

dsp["Sndinfo Chandiff - Compare channels in a stereo soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "chandiff", channels = "any", tip = " This process is a simple utility to compare the two channels of a stereo soundfile sample by sample. Musical applications; The presence of differences between the channels will indicate that it is a true stereo file: i.e., some differences in data. If the two channels are identical – report no differences – it will really be a mono source which has been 'spread' across two channels. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "threshold", switch = "-t", min = 0.0, max = 1, def = 0, tip = "maximum permissible difference in data values (Range: 0.0 to 1.0)" },
  arg3 = { name = "count", switch = "-n", min = 0, max = 44100, def = 1, tip = "maximum number of differences to accept (Default: 1)" },
}

dsp["Sndinfo Diff - Compare two sound, analysis, pitch, transposition, envelope or formant files"] = { 
  cmds = { exe = "sndinfo", mode = "diff", input = "2audio", tip = "SNDINFO DIFF compares the data in the two input files and reports any differences. The value given for threshold will be determined by the type of files being compared: e.g., amplitude in a 0 to 1 range. Musical applications; Sometimes we make copies of soundfiles and give the copy a new name. Later, we forget whether or not the two files are really the same. SNDINFO DIFF provides the answer. Note that SNDINFO DIFF can compare a variety of files in a binary, non-text format: soundfiles, analysis files, pitch data files (from REPITCH GETPITCH) etc. " }, 
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
  arg3 = { name = "threshold", switch = "-t", min = 0.0, max = 1, def = 0, tip = "maximum permissible difference in data values (Range: 0.0 to 1.0)" },
  arg4 = { name = "count", switch = "-n", min = 0, max = 44100, def = 1, tip = "maximum number of differences to accept (Default: 1)" },
  arg5 = { name = "-l", switch = "-l", tip = "continue, even if the files are not the same length" },
  arg6 = { name = "-c", switch = "-c", tip = "continue, even if the (sound)files do not have the same number of channels" },
}

dsp["Sndinfo Findhole - Find largest low level hole in a soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "findhole", tip = "Reports the length of the hole and the time at which it begins. If it displays the message 'Maximum holesize 0.0 at time 0.0', try raising the threshold. Musical applications; This utility enables us to cut out areas of low signal in a soundfile without having to access a soundfile viewer. It helps to make the command line environment more practical. The presence of the basic editing facilities (cut, paste (insert), splice and mix in command line form also supports our partially-sighted users. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "threshold", switch = "-t", min = 0.0, max = 1, def = 0, tip = "amplitude value: consider there to be a hole only if the level falls and stays below threshold (Range: floating point value: 0.0 < threshold <= 1.0) (Default: 0.0)" },
}

dsp["Sndinfo Len - Display duration of a soundfiling-system file"] = { 
  cmds = { exe = "sndinfo", mode = "len", tip = " Displays the type of file, the duration in seconds, the number samples and bytes for a soundfile, the number of windows, floats and bytes for an analysis or analysis-derived file. Musical applications; SNDINFO LEN provides a quick check on length for a specific soundfile. I have used this most often when preparing breakpoint files or mixing (Ed.). DIRSF will provide the same information and is useful when a little unsure about the name: you can get close with wildcards (e.g., 'cr*.wav' to see all .wav files beginning with 'cr') and then spot the one you want in the list displayed. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
}

dsp["Sndinfo Lens - List durations of several soundfiling-system files"] = { 
  cmds = { exe = "sndinfo", mode = "lens", tip = "  Here you see a display of the soundfile name and its duration (only). Musical applications; You are able to check on several (sound)files at once." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
}

dsp["Sndinfo Loudchan - Find loudest channel in a stereo soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "loudchan", channels = "any", tip = " Displays which channel is loudest. Musical applications; Knowing which channel is loudest can help in setting levels in the mixfile. If stereo files are present and there is some slight overload, it may be useful to check which specific channel might be causing the problem. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
}

dsp["Sndinfo Maxi - List levels of several soundfiles"] = { 
  cmds = { exe = "sndinfo", mode = "maxi", tip = " SNDINFO MAXI lists levels within the normalised range of 0 to 1, e.g., 0.421491 and 0.717514, i.e., in reference to 1 as full volume (0 dB). It displays both the soundfile names and their levels on the screen and writes this information to a (mandatory) text file. You can convert these values to dB using Mode 24 of SNDINFO UNITS. E.g., 0.421491 is -7.504dB and 0.717514 is -2.896dB. Note that .7 is closer to 1 than 0.4 and therefore a higher level. Also see the Gain-to-dB Chart. Musical applications; You are able to check on several soundfiles at once. This can be useful for example to compare a soundfile to which gain has been applied and the original." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
  arg3 = { name = "Output.txt", output = "txt" },
}

dsp["Sndinfo Maxsamp - Find maximum sample in a soundfile or binary data file"] = { 
  cmds = { exe = "sndinfo", mode = "maxsamp", tip = "SNDINFO MAXSAMP provides a good deal of useful information. Here is a typical output and what it all means: SNDINFO MAXSAMP Display, with Commentary Info labels   Info values   Commentary maximum abs value:   32380   Value belongs to the 0-32767 amplitude range maximum level:   -0.1032dB   0 dB is full volume, so this will be just under at:   1758 samples   Sample location of the sample with the highest level time:   0.0399 sec   Corresponding time location of the sample with the highest level repeated:   1 times   Number of times this highest value occurs in the file max possible gain:   1.0120   Gain factor which could be used to 'normalise' the sound, i.e., to bring it to full volume – values less than 1 reduce volume max dB gain:   0.1032   You might want to use a dB value in a mixfile. This information is also written to the header of a CDP soundfile, for possible future use by the system. When using SNDINFO MAXSAMP again on the same file, it will just read the header unless you force another look at the sound data in the file by using -f. Musical applications; Sometimes you just want the sound to be a bit louder (or softer). Or the Phase Vocoder may report a bit of overload that needs to be reduced. Or some process has introduced overload and you need to start with a slightly softer input. Use MODIFY LOUDNESS to make the changes. It is wise to apply gain to a (digital) soundfile with discretion and only as needed. Being too heavy-handed, such as applying a large amount of gain to a rather soft sound, can introduce distortion artefacts." }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "-f", switch = "-f", tip = "force file to be scanned (ignoring any information in the header about max sample which may already be present)" },
}

dsp["Sndinfo Maxsamp2 - Find maximum sample within a specified timerange in a soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "maxsamp2", tip = "This function returns information about maximum amplitude and recommended gain factor as does SNDINFO MAXSAMP above, but enables you to specify a time period in the soundfile that you would like to check. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "start", min = 0, max = length/1000, def = 0, tip = "start time of search in file" },
  arg3 = { name = "end", min = 0, max = length/1000, def = length/1000, tip = "end time of search in file " },
}

dsp["Sndinfo Prntsnd - Print sound sample data to a textfile (CARE!!! large amounts of data!!) "] = { 
  cmds = { exe = "sndinfo", mode = "prntsnd", tip = " The textfile generated by the process contains data for every sample within the time specified in the form [sample_count] amplitude level. Each value is displayed on a separate line. You will therefore have sample_rate number of samples for every second of sound. At a sample rate of 44100, even 1/100th of a second will contain 441 values per channel. Musical applications; I think more than anything else, this little utility shows what can be done. Anyone wanting to develop this towards a more particular application or type of display is free to have the source code! " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt" },
  arg3 = { name = "starttime", min = 0, max = length/1000, def = 0, tip = "time in infile at which to begin printing data" },
  arg4 = { name = "end", min = 0, max = length/1000, def = length/1000, tip = "time in infile at which to end printing data" },
}

dsp["Sndinfo Props - Display properties of a soundfiling-system file"] = { 
  cmds = { exe = "sndinfo", mode = "props", tip = "The 'properties' are the key defining features of a soundfile. They usually include: its size in bytes and samples, the type of number used to store the sample data (SHORT or FLOAT), the sample rate (number of samples per second), and the number of channels.  A soundfile sample is (normally) stored as a 16-bit number. A byte is 8-bits. This is why there are twice as many bytes as there are samples in a soundfile. An analysis file sotres its data floating-point numbers, with 4 bytes per sample. Some programs are now also able to store soundfiles in floating-point format. This information is written in the 'header' of the file, a text portion that precedes the data. This information can be altered with HOUSEKEEP RESPEC. A CDP header may also have some other information, as we have seen with SNDINFO MAXSAMP, but the structure and use of the header conforms to the .wav standard. Many CDP functions 'interrogate' the header and make use of the information it contains. This for example, is how SUBMIX DUMMY puts the correct number of channels in the file it creates. Musical applications; SNDINFO PROPS provides a quick confirmation of the header information in a soundfile. It is normally used before and after HOUSEKEEP RESPEC " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
}

dsp["Sndinfo Smptime - Convert sample count to time in soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "smptime", tip = "There may be occasions when you would like to know how much time a given number of samples comprises. This function provides the answer in seconds, to 6 decimal places of precision. You need to specify an infile because the number of samples used for a given length of time differs according to sample rate and number of channels. The function looks at the header of infile to get the information it needs to make the calculation – saving you a bit of time. If your value for sampcount is greater than the number of samples in the file, an 'out of range' error message is issued, showing as the maximum value the number of samples in the file. The reverse operation, from time to samples, is handled by SNDINFO TIMESMP. Musical applications; Most software which displays soundfiles will show time both in seconds and as a sample count. But if you come across one which just uses sample count, this will convert to the equivalent time in seconds. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "sampcount", min = 0, max = length, def = 0, tip = "number of samples (an integer)" },
  arg3 = { name = "-g", switch = "-g", tip = "the sample count is taken as a count of multi-channel samples. For example, in a stereo file, sample-pairs are counted." },
}

dsp["Sndinfo Sumlen - Sum durations of several soundfiling-system files "] = { 
  cmds = { exe = "sndinfo", mode = "sumlen", tip = " Unless using butt edits exclusively, splicing soundfiles together involves overlapping the files by the length of the splices. Thus the total duration of the spliced files is not the same as the total duration of all the files taken individually.SNDINFO SUMLEN will tell you what the total duration of the spliced files is, taking into account the splice overlaps. If a splice length is not specified, it will use the Default 15ms. You can enter a value of 0 for splice and get a total duration with no overlaps: i.e., for all the files taken individually Musical applications; When splicing and mixing, you sometimes want to have an idea how long a certain combinations of soundfiles will become. This function will tell you that without your having to look up each file individually. One restriction is that files with different numbers of channels can be mixed, but their durations cannot be summed by SNDINFO SUMLEN." }, 
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
  arg3 = { name = "splicelen", switch = "-s", min = 0, max = length, def = 15, tip = "length of splice in milliseconds. (Default: 15ms)." },
}

dsp["Sndinfo Timediff - Find difference in duration of two soundfiles "] = { 
  cmds = { exe = "sndinfo", mode = "timediff", input = "2audio", tip = "The inputs must be soundfiles. The function displays the amount of time difference there is between their respective durations. It does this calculation based on information it finds in the headers of the soundfiles. Musical applications; Suppose you wanted to offset a file in a mix, so that it ended at the same time as another file. SNDINFO TIMEDIFF would tell you the size of the offset. It might also be relevant when using the stagger option of MORPH MORPH." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the 1st input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the 2nd input sound to the process" },
}

dsp["Sndinfo Timesmp - Convert time to sample count in a soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "timesmp", tip = " There may be occasions when you would like to know how many samples are contained in a given length of time in a soundfile. This function provides the answer. You need to specify an infile because the number of samples used for a given length of time differs according to sample rate and number of channels. The function looks at the header of infile to get the information it needs to make the calculation – saving you a bit of time. A stereo soundfile has twice as many samples per unit time as does a mono soundfile. For example 1.52 seconds in a mono soundfile at a sample rate of 44100 comprises 67032 samples, but 1.52 seconds in a stereo soundfile at the same sample rate comprises 134064 samples. With the -g flag, the two channels are 'grouped' (treated as a single pair), so SNDINFO TIMESMP will display 67032. The reverse operation, from samples to time, is handled by SNDINFO SMPTIME. Musical applications; Most software which displays soundfiles will show time both in seconds and as a sample count. But if you come across one which just uses time, and for some reason you need to know the sample count equivalent to this duration, SNDINFO TIMESMP will display the number of samples." }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "time", min = 0, max = length/1000, def = 0, tip = "(floating point) time value in seconds" },
  arg3 = { name = "-g", switch = "-g", tip = "the sample count is taken as a count of multi-channel samples. For example, in a stereo file, sample-pairs are counted." },
}

dsp["Sndinfo Units - 1 MIDI to FRQ - Pitch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 1", tip = "MIDI to FRQ -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", min = 0, max = 1000, def = 80, tip = "Integer representing a MIDI note-value" },
}

dsp["Sndinfo Units - 2 FRQ to MIDI - Pitch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 2", tip = "2 FRQ to MIDI -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0, max = 44100, def = 1200, tip = "Frequency in Hertz" },
}

dsp["Sndinfo Units - 3 NOTE to FRQ - Pitch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 3", tip = "3 NOTE to FRQ -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", input = "string", def = "", tip = "Note like A0 or ebu4 et cetera" },
}

dsp["Sndinfo Units - 4 NOTE to MIDI - Pitch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 4", tip = "4 NOTE to MIDI -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", input = "string", def = "", tip = "Note like A0 or ebu4 et cetera" },
}

dsp["Sndinfo Units - 5 FRQ to NOTE - Pitch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 5", tip = "5 FRQ to NOTE -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", min = 8.175799, max = 12543.853951, def = 10, tip = "A frequency in Hz" },
}

dsp["Sndinfo Units - 6 MIDI to NOTE - - Ptch - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 6", tip = "6 MIDI to NOTE -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", min = 0.0, max = 127.000000, def = 80, tip = "Integer representing a MIDI note-value" },
}

dsp["Sndinfo Units - 7 FRQ RATIO to SEMITONES - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 7", tip = "7 FRQ RATIO to SEMITONES -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", min = 0.003906, max = 256.0, def = 1, tip = "A fractional multiplier used to transpose a frequency (see chart in cdp docs)" },
}

dsp["Sndinfo Units - 8 FRQ RATIO to INTERVAL - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 8", tip = "8 FRQ RATIO to INTERVAL -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", min = 0.003906, max = 256.000000, tip = "A fractional multiplier used to transpose a frequency (see chart in cdp docs)" },
}

dsp["Sndinfo Units - 9 INTERVAL to FRQ RATIO - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 9", tip = "9 INTERVAL to FRQ RATIO -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
  arg1 = { name = "value", input = "txt", tip = "example; 3 = a (major) third up,-m3 = a minor third down, m3u = a minor third plus Up a ¼-tone" },
}

dsp["Sndinfo Units - 10 SEMITONES to FRQ RATIO - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 10", tip = "10 SEMITONES to FRQ RATIO -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0, max = 1000, def = 6, tip = "Number of semitones and fractions of semitones" },
}

dsp["Sndinfo Units - 11 OCTAVES to FRQ RATIO - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 11", tip = "11 OCTAVES to FRQ RATIO -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -12, max = 12, def = 2, tip = "Number of octaves and fractions of octaves" },
}

dsp["Sndinfo Units - 12 OCTAVES to SEMITONES - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 12", tip = "12 OCTAVES to SEMITONES -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -12, max = 12, def = 2, tip = "Number of octaves and fractions of octaves" },
}

dsp["Sndinfo Units - 13 FRQ RATIO to OCTAVESS - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 13", tip = "13 FRQ RATIO to OCTAVES -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 64, tip = "A fractional multiplier used to transpose a frequency (see cdp docs)" },
}

dsp["Sndinfo Units - 14 SEMITONES to OCTAVES - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 14", tip = "14 SEMITONES to OCTAVES -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -24, max = 24, def = 6, tip = "Number of semitones and fractions of semitones" },
}

dsp["Sndinfo Units - 15 SEMITONES to INTERVAL - Interval - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 15", tip = "15 SEMITONES to INTERVAL -  References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -24, max = 24, def = 6, tip = "Number of semitones and fractions of semitones" },
}

dsp["Sndinfo Units - 16 FRQ RATIO to TIME RATIO - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 16", tip = "16 FRQ RATIO to TIME RATIO - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 64, tip = "A fractional multiplier used to transpose a frequency (see cdp docs)" },
}

dsp["Sndinfo Units - 17 SEMITONES to TIME RATIO - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 17", tip = "17 SEMITONES to TIME RATIO - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -96.000000, max = 96.000000, def = 0.0, tip = "A fractional multiplier used to transpose a frequency (see cdp docs)" },
}

dsp["Sndinfo Units - 18 OCTAVES to TIME RATIO - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 18", tip = "18 OCTAVES to TIME RATIO - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -8.000000, max = 8.000000, def = 0.0, tip = "Number of octaves and fractions of octaves" },
}

dsp["Sndinfo Units - 19 INTERVAL to TIME RATIO - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 19", tip = "19 INTERVAL to TIME RATIO - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", input = "string", def = "", tip = "for example; 3 = a (major) third up, -m3 = a minor third down, m3u = a minor third plus Up a ¼-tone" },
}

dsp["Sndinfo Units - 20 TIME RATIO to FRQ RATIO - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 20", tip = "20 TIME RATIO to FRQ RATIO - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 1, tip = "Timestretch/compression = reciprocal of the frequency ratio: period = 1/frq_ratio. E.g., twice the frequency will be ½ the duration: 1/2 = 0.5" },
}

dsp["Sndinfo Units - 21 TIME RATIO to SEMITONES - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 21", tip = "21 TIME RATIO to SEMITONES - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 1, tip = "Timestretch/compression = reciprocal of the frequency ratio: period = 1/frq_ratio. E.g., twice the frequency will be ½ the duration: 1/2 = 0.5" },
}

dsp["Sndinfo Units - 22 TIME RATIO to OCTAVES - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 22", tip = "22 TIME RATIO to OCTAVES - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 1, tip = "Timestretch/compression = reciprocal of the frequency ratio: period = 1/frq_ratio. E.g., twice the frequency will be ½ the duration: 1/2 = 0.5" },
}

dsp["Sndinfo Units - 23 TIME RATIO to INTERVAL - Speed - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 23", tip = "23 TIME RATIO to INTERVAL - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.003906, max = 256, def = 1, tip = "Timestretch/compression = reciprocal of the frequency ratio: period = 1/frq_ratio. E.g., twice the frequency will be ½ the duration: 1/2 = 0.5" },
}

dsp["Sndinfo Units - 24 GAIN FACTOR to DBGAIN - Loudness - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 24", tip = "24 GAIN FACTOR to DBGAIN - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = 0.000000, max = 128.000000, def = 0.5, tip = "Amplitude multiplier: each integer above zero represents an increase of ca. 3dB. Fractions reduce amplitude: 0.7 is down ca. 3dB, 0.5 is down ca. 6dB etc." },
}

dsp["Sndinfo Units - 25 DBGAIN to GAIN FACTOR - Loudness - Convert between different units "] = { 
  cmds = { exe = "sndinfo", mode = "units 25", tip = "25 DBGAIN to GAIN FACTOR - References to octaves can be confusing. Note that here 'Octave 0' is the middle octave on the keyboard (Middle-C) and that Octave -1 means the octave below this. When entering 'Notes' in the form A-1, for example, this would be the A below Middle-C. The Middle-C octave is represented by 8.xx in Csound, and one also comes across Middle-C referred to as C3, probably because it's the 3rd octave on most piano keyboards. I've made up a handwritten chart for myself which gathers together these octave differences, together with the note name and MIDI note number into boxes for each note over the several octaves I use most of the time. (Ed.) The precision of the units may sometimes seem extreme, but are usually more significant than they at first appear. Frequencies and amplitudes cover a very wide numerical range, and rounding errors usually do need to be minimised. You will notice that even with 6 decimal places, corresponding conversions don't come out exactly the same. Musical applications; Exploring some of the conversions, such as Gain Factor & dB Gain, can be very instructive. There are a couple of charts below for quick reference in areas I tend to use frequently. You can use the program to work out your own charts for values you find you often require. We really ought to have a book of charts of various kinds at our fingertips, just as mathematicians used to have log tables before the advent of the calculator, e.g.,: the notes chart described above, a table of frequencies corresponding to equal-(or other)-tempered scales, a chart showing how (the logarithmic scale of) dB levels relates to sounds in the environment, a chart of transposition ratios (start of one is shown below), a chart showing how gain factors relate to dB (start of one is shown below). We can save a lot of time by preparing some tools for working in the electroacoustic medium. SNDINFO UNITS gives us answers for many of the specific questions which will inevitably come up." }, 
   arg1 = { name = "value", min = -96.000000, max = 42.144199, def = 0.0, tip = "Amplitude expressed in dB, the logarithmic scale in which every 6dB represents a doubling or halving of the amplitude – the range of hearing is enormous!" },
}

dsp["[doesn't work?] Sndinfo Zcross - Display fraction of zero-crossings in a soundfile"] = { 
  cmds = { exe = "sndinfo", mode = "zcross", channels = "2in2out", tip = "SNDINFO ZCROSS counts the number of zero-crossings in your file. Zero-crossings occur when the waveform crosses the zero amplitude centre-line, which represents the rest position of the speaker cone between positive and negative motions. But what do we mean here by 'fraction of' and what use can we make of this information? For a pure sinewave you will have 2 zero-crossings for every wavecycle. For a 441Hz wave, at a sample rate of 44100 samples per second, you would expect to find 100 wavecycles per second, and therefore 200 zero-crossings per second. The zero-crossing ratio is therefore defined here as 1:200 (= 0.005).With more complex sounds, the wave may cross the zero amplitude line more than twice in each wavecycle, and with noise, the signal crosses the zero line very frequently. Thus, more complex or noisy signals have a higher zero-crossing ratio, e.g. 1:3 (= 0.333). So the ratio gives some indication of how complex or noisy your sound is: the higher the ratio, the noisier the sound. 4 Examples, specifying 1 second segments within the sound: A fairly complex chord (with several adjacent semitones) made with SYNTH CHORD – the fraction of zero-crossings is 0.024442, or approximatgely 1:32. Text delivered by a male voice, speaking normally – the fraction of zero-crossings is 0.044444, or approximately 3:64. A riff played on a drum-kit – the fraction of zero-crossings is 0.176098, or approximately 11:64. Noise generated by SYNTH NOISE – the fraction of zero-crossings is 0.502540, or approximately 1:2. Musical applications SNDINFO ZCROSS is therefore a diagnostic tool for use with programs which use zero-crossing detection to modify a sound (e.g. the DISTORT programs). It gives you a numerical indication of the relative complexity of the sound and therefore the degree of distortion that may result from manipulating the 'pseudo-wavecycles' between zero-crossings. " }, 
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "starttime", min = 0, max = length/1000, tip = "time in infile at which to begin the count" },
  arg3 = { name = "endtime", min = 0, max = length/1000, def = length/1000, tip = "time in infile at which to end the count" },
}
 
------------------------------------------------
-- spec
------------------------------------------------
 
dsp["Spec Bare - Zero the data in channels which do not contain harmonics "] = {
  cmds = { exe = "spec", mode = "bare", tip = " BARE zeroes all the non-partial data, retaining the harmonics of the original tone. The harmonics are the integer multiples of the fundamental. When the original is a complex sound, the actual result is dependent on what the GETPITCH process works out this fundamental to be. If the original sound is a clearly pitched tone, it is probable that harmonic partials will be captured in some channels. SPEC BARE will zero out the data in the channels which do not contain harmonic partials, to provide a good, clean version of the original for further processing. If the original sound is a complex tone with many inharmonic partials, then the SPEC BARE process may not work. The windows will be flagged as having no pitch. However, I (Ed.) have used it on a donkey bray and found that the result somehow brought out an overtone series. This was then further emphasised by applying HILITE PLUCK to the SPEC BARE output. Musical applications As implied above, the musical uses of SPEC BARE begin when another process needs to work with clearly identified partials content, such as emphasising pitch content, overtone series, or harmonic colouration. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Input.frq", input = "frq", tip = "Select the data input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "-x", switch = "-x", tip = "more body in resulting spectrum" },
}
 
dsp["Spec Clean 1 - Remove noise from PVOC analysis file "] = {
  cmds = { exe = "spec", mode = "clean 1", tip = "1 deletes a channel (after skiptime) FROM THE TIME its level falls below the (noisgain adjusted) maximum level seen for that channel in nfile TO THE END of the file. The process works by comparing the level (for each channel in turn) in nfile (the noise file) with the level in the sound to be cleaned. To find the noise threshold in any channel, i.e., the level below which we decide the data is merely noise and not significant signal, the program looks at the whole nfile and takes the maximum level it finds (in each channel) as the noise threshold (for each particular channel). If noisgain is then set as e.g., 2, then the noise threshold (for each channel in turn) is set 2-times louder than the threshold extracted from the noise file. In Mode 1, as soon as the level in (the particular channel of) the file to be cleaned falls below the threshold (for that channel), the channel data is eliminated from that time to the end of the file. This is therefore useful for cleaning up the end of a file where the sound source is perhaps fading away, and falls below the noise threshold. In Mode 2, the channel data is eliminated ANYWHERE it falls below the noise threshold (for that channel), but the data is restored at any time it rises above that threshold again. Mode 3 works like Mode 2, but only on a specified high frequency area. Mode 4 attempts to look for channels which contribute 'nothing' to the spectrum of the sound required. It does this by comparing a noise segment with a good signal segment, and deciding which channels are 'redundant', i.e., are always below the noise threshold. The key to its use therefore lies in selecting effective sections of the soundfile for these reference levels. Listen carefully to the source soundfile, noting where appropriate sections begin and end, and use these times to cut the input (analysis) file with SPEC CUT to produce the required comparison files. This may take some trial and error. Listen for 'bubbling' effects; these may occur if the noise level or the noise comparison sample isn't quite right to achieve the desired noise reduction. Musical applications; This process can be useful in tidying up the signal of e.g., an analogue recording. However, the user is cautioned that the methods used here are relatively simple and are not sufficiently 'intelligent' to professionally digitally remaster analogue recordings." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "skiptime", min = 0, max = length/1000, def = 0, tip = " (seconds) may be set to time at which good source signal level has been established" },
  arg5 = { name = "noisgain", switch = "-g", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}
 
dsp["Spec Clean 2 - Remove noise from PVOC analysis file "] = {
  cmds = { exe = "spec", mode = "clean 2", tip = "2 deletes channel (after skiptime) ANYWHERE its level falls below the (noisgain adjusted) maximum level seen for that channel in nfile, RESTORING data when it rises above that level. The process works by comparing the level (for each channel in turn) in nfile (the noise file) with the level in the sound to be cleaned. To find the noise threshold in any channel, i.e., the level below which we decide the data is merely noise and not significant signal, the program looks at the whole nfile and takes the maximum level it finds (in each channel) as the noise threshold (for each particular channel). If noisgain is then set as e.g., 2, then the noise threshold (for each channel in turn) is set 2-times louder than the threshold extracted from the noise file. In Mode 1, as soon as the level in (the particular channel of) the file to be cleaned falls below the threshold (for that channel), the channel data is eliminated from that time to the end of the file. This is therefore useful for cleaning up the end of a file where the sound source is perhaps fading away, and falls below the noise threshold. In Mode 2, the channel data is eliminated ANYWHERE it falls below the noise threshold (for that channel), but the data is restored at any time it rises above that threshold again. Mode 3 works like Mode 2, but only on a specified high frequency area. Mode 4 attempts to look for channels which contribute 'nothing' to the spectrum of the sound required. It does this by comparing a noise segment with a good signal segment, and deciding which channels are 'redundant', i.e., are always below the noise threshold. The key to its use therefore lies in selecting effective sections of the soundfile for these reference levels. Listen carefully to the source soundfile, noting where appropriate sections begin and end, and use these times to cut the input (analysis) file with SPEC CUT to produce the required comparison files. This may take some trial and error. Listen for 'bubbling' effects; these may occur if the noise level or the noise comparison sample isn't quite right to achieve the desired noise reduction. Musical applications; This process can be useful in tidying up the signal of e.g., an analogue recording. However, the user is cautioned that the methods used here are relatively simple and are not sufficiently 'intelligent' to professionally digitally remaster analogue recordings." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "skiptime", min = 0, max = length/1000, def = 0, tip = " (seconds) may be set to time at which good source signal level has been established" },
  arg5 = { name = "noisgain", switch = "-g", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}
 
dsp["Spec Clean 3 - Remove noise from PVOC analysis file "] = {
  cmds = { exe = "spec", mode = "clean 3", tip = "3 deletes channel as in MODE 2 but ONLY for channels of frequency > freq. The process works by comparing the level (for each channel in turn) in nfile (the noise file) with the level in the sound to be cleaned. To find the noise threshold in any channel, i.e., the level below which we decide the data is merely noise and not significant signal, the program looks at the whole nfile and takes the maximum level it finds (in each channel) as the noise threshold (for each particular channel). If noisgain is then set as e.g., 2, then the noise threshold (for each channel in turn) is set 2-times louder than the threshold extracted from the noise file. In Mode 1, as soon as the level in (the particular channel of) the file to be cleaned falls below the threshold (for that channel), the channel data is eliminated from that time to the end of the file. This is therefore useful for cleaning up the end of a file where the sound source is perhaps fading away, and falls below the noise threshold. In Mode 2, the channel data is eliminated ANYWHERE it falls below the noise threshold (for that channel), but the data is restored at any time it rises above that threshold again. Mode 3 works like Mode 2, but only on a specified high frequency area. Mode 4 attempts to look for channels which contribute 'nothing' to the spectrum of the sound required. It does this by comparing a noise segment with a good signal segment, and deciding which channels are 'redundant', i.e., are always below the noise threshold. The key to its use therefore lies in selecting effective sections of the soundfile for these reference levels. Listen carefully to the source soundfile, noting where appropriate sections begin and end, and use these times to cut the input (analysis) file with SPEC CUT to produce the required comparison files. This may take some trial and error. Listen for 'bubbling' effects; these may occur if the noise level or the noise comparison sample isn't quite right to achieve the desired noise reduction. Musical applications; This process can be useful in tidying up the signal of e.g., an analogue recording. However, the user is cautioned that the methods used here are relatively simple and are not sufficiently 'intelligent' to professionally digitally remaster analogue recordings." },
  arg1 = { name = "infile", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "nfile", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "outfile", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "freq", min = 0, max = 22050, def = 6000, tip = "frequency in Hz above which noise is to be removed" },
  arg5 = { name = "noisgain", switch = "-g", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}

dsp["Spec Clean 4 - Remove noise from PVOC analysis file "] = {
  cmds = { exe = "spec", mode = "clean 4", tip = "4 deletes channel EVERYWHERE, whose level in gfile is ALWAYS below the (noisgain adjusted) maximum level seen for that channel in nfile. The process works by comparing the level (for each channel in turn) in nfile (the noise file) with the level in the sound to be cleaned. To find the noise threshold in any channel, i.e., the level below which we decide the data is merely noise and not significant signal, the program looks at the whole nfile and takes the maximum level it finds (in each channel) as the noise threshold (for each particular channel). If noisgain is then set as e.g., 2, then the noise threshold (for each channel in turn) is set 2-times louder than the threshold extracted from the noise file. In Mode 1, as soon as the level in (the particular channel of) the file to be cleaned falls below the threshold (for that channel), the channel data is eliminated from that time to the end of the file. This is therefore useful for cleaning up the end of a file where the sound source is perhaps fading away, and falls below the noise threshold. In Mode 2, the channel data is eliminated ANYWHERE it falls below the noise threshold (for that channel), but the data is restored at any time it rises above that threshold again. Mode 3 works like Mode 2, but only on a specified high frequency area. Mode 4 attempts to look for channels which contribute 'nothing' to the spectrum of the sound required. It does this by comparing a noise segment with a good signal segment, and deciding which channels are 'redundant', i.e., are always below the noise threshold. The key to its use therefore lies in selecting effective sections of the soundfile for these reference levels. Listen carefully to the source soundfile, noting where appropriate sections begin and end, and use these times to cut the input (analysis) file with SPEC CUT to produce the required comparison files. This may take some trial and error. Listen for 'bubbling' effects; these may occur if the noise level or the noise comparison sample isn't quite right to achieve the desired noise reduction. Musical applications; This process can be useful in tidying up the signal of e.g., an analogue recording. However, the user is cautioned that the methods used here are relatively simple and are not sufficiently 'intelligent' to professionally digitally remaster analogue recordings." },
  arg1 = { name = "infile", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "nfile", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "gfile", input = "ana", tip = "Select the third input sound to the process" },
  arg4 = { name = "outfile", output = "ana", tip = "Select the output sound to the process" },
  arg5 = { name = "noisgain", switch = "-g", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}
 
dsp["Spec Cut - Cut a section out of an analysis file, between starttime and endtime (seconds)"] = {
  cmds = { exe = "spec", mode = "cut", tip = "SPEC CUT makes it possible to extract a section of a PVOC analysis file and save it as a new analysis file. The cut points are specified in seconds. Musical applications; The primary application is simply to shorten an analysis file without having to go back to the original soundfile, cut that and re-analyze. Listening to the original soundfile should give a rough indication of where the cut points might best be located. It is also used to select portions of an analysis file containing 'typical noise' (the nfile) and 'good signal' (the gfile) for use with SPEC CLEAN. To do this, one uses a standard sound editor, blocking out sections of the original source sound, listening to them, and making a note of the time points. These time points are then used with SPEC CUT directly on the analysis file for that soundfile – the times of source and analysis match. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "starttime", min = 0, max = length/1000, tip = "starttime time in seconds at which the cut is to begin" },
  arg4 = { name = "endtime", min = 0, max = length/1000, tip = "endtime time in seconds at which the cut is to end" },
}
 
dsp["Spec Gain - Amplify or attenuate the spectrum "] = {
  cmds = { exe = "spec", mode = "gain", tip = "GAIN multiplies the amplitude of each partial with the gain factor. This will attenuate the amplitude if less than 1 and increase the amplitude if greater than 1. For any soundfile, there is an optimal gain factor, above which distortion is created. Musical applications; GAIN can be usefully applied to bring a sound up to full volume before running another process, or restoring it to full volume if it has lost amplitude as a result of the process. However, it does need to be used with some caution. Large or repeated amplifications of a poor signal will also amplify discontinuities and increase digital noise. Sometimes PVOC, the Phase Vocoder, recommends an increase or decrease in gain, esp. the latter, if an overflow has occurred. SPEC GAIN is used to make this adjustment. Excessive application of gain on a poor signal can introduce digital noise by creating sharp changes in amplitude levels. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "gain", min = 0, max = 1000, input = "brk", def = 2, tip = "gain numerical float value > 0.0 " },
}
 
dsp["Spec Gate - Eliminate channel data below a threshold amplitude "] = {
  cmds = { exe = "spec", mode = "gate", tip = " SPEC GATE very simply cuts out low level signal. Musical applications; It is intended as a facility by which various unwanted artefacts in the sound can be removed from the analysis file. The use of the phase vocoder with various types of input sound can be somewhat unpredictable, with the result that 'artefacts' may be produced, often heard as a light swishing sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "threshold", min = 0, max = 1, input = "brk", def = 0.5, tip = "lowest acceptable amplitude level (Range: 0 to 1)" },
}

dsp["Spec Grab - Grab a single analysis window at time point specified "] = { 
  cmds = { exe = "spec", mode = "grab", tip = "GRAB grabs a single window from an existing analysis data file. The idea behind this is to be able to take the information from a single window for various purposes. Musical applications One application has been implemented so far: SPEC PRINT, which prints the analysis data to the screen or to text file. SPEC MAGNIFY, which time-expands the data of the one window to any duration, is another application of this idea, but it selects the window for itself. MORPH GLIDE interpolates between two single analysis windows extracted by GRAB. " }, 
   arg1 = { name = "Input", input = "ana", tip = "sound input" },
   arg2 = { name = "Output", output = "brk", tip = "1 window .ana file for use in morph glide" }, 
   arg3 = { name = "time", min = 0, max = length/1000, def = 0, tip = "time location in seconds of the window to grab" },
}
 
dsp["Spec Magnify - Magnify (in duration) a single analysis window at time time to duration dur"] = {
  cmds = { exe = "spec", mode = "magnify", tip = "This function was previously named SPECWEXP. It synthesizes a steady sound from the analysis data at any single window within the analysis file. Listen for material in the source which might benefit from being drawn out for a longer period of time. A wide range of frequencies and rapidly changing timbral content are two things to listen for. Note that the resynthesized sound output will need to be topped and tailed. The Groucho function TAILOR can be used for this, but many sound editors also have this facility. Musical applications; The purpose of SPEC MAGNIFY is to generate a potentially interesting sound by expanding a moment of apparent timbral interest. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "time", min = 0, max = length/1000, tip = "time location in seconds where the single window to expand occurs in the analysis file" },
  arg4 = { name = "dur", min = 0, max = 30, def = 6, tip = "the required duration of oufile; it MUST BE > the analysis window length " },
}

 
------------------------------------------------
-- specinfo
------------------------------------------------
 
dsp["Specinfo Octvu - Text display of the time-varying amplitude of the spectrum, within octave bands"] = {
  cmds = { exe = "specinfo", mode = "octvu", tip = "The display shows frequency bands horizontally, and the readings at the time points for each band vertically, underneath each frequency band heading. The fundamental will influence the frequency starting point for the display. Scanning vertically down the time point readings will quickly show in which band and at what time the maximum energy is located. (Using a mathematically 'easy' division of the second (e.g., a time_step of 250 for ¼ sec, or of 500 for ½ sec) will make it easier to visualise the energy across the time of the sound.) Note that the reported values are relative levels only. The lowest band includes all energy down to '0Hz'. Musical applications; SPECINFO OCTVU provides an overview of how the spectrum is evolving. (Also see SPECINFO PEAK.) This information can be useful when making decisions, for example, about when and at which frequencies to apply filtering to a sound. Also, some functions, such as STRANGE SHIFT, have a 'frequency divide' parameter to locate processes above or below a certain frequency. SPECINFO OCTVU can be used to help work out what frequency value to give to this parameter. Note that the time_step parameter enables you to set the resolution in time of the information gathered ­ a 'zoom' control." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "time_step", min = 0, max = length, tip = "spread (in milliseconds) of the time points at which information is gathered; band energy is totalled over each time_step" },
  arg4 = { name = "fundamental", switch = "-f", min = 0, max = 44100, def = 1000, tip = "Octave bands are centred on octave transpositions of fundamental, given in Hz. Default: centred on divisions of the Nyquist frequency (i.e., ½ the sample rate)." },
}
 
dsp["Specinfo Peak - Locate time-varying energy centre of the spectrum "] = {
  cmds = { exe = "specinfo", mode = "peak", tip = " PEAK searches the spectrum for the time-varying peak energy, i.e., the highest amplitude within the given frequency band. You can define the size of the frequency bands you wish to compare (minimum is one channel width). You can also ask the program to average the energy calculations over a certain time-window, which you specify. This avoids tracking every micro-fluctuation of energy in the spectrum. The output text file lists the start time of each window (based on the window durations specified by you) and indicates the lower and upper frequency of the band of maximum energy in each of these windows. The window times shown in the outtextfile are generated automatically, starting at time zero and stepping through the file in time-chunks specified by you as timewindow. The timewindow parameter therefore operates as a zoom mechanism. PEB shows you the lower and upper frequency of the band of frequencies which has the peak energy in that particular window. Musical applications; The possibility of widening the time window chunks indicates how this function can return useful information. The objective is to locate which frequency band contains an energy peak in any given time period. Timewindow gives time resolution and frqwindow gives frequency resolution in parts of an octave. Note that the timewindow is not the same as the analysis window. The duration of an analysis window is the value given for -N divided by the sample rate: e.g., 1024/44100 = an analysis window of 0.0232 sec duration. A timewindow of e.g. 0.1 sec will therefore average over about 5 analysis windows and will give 10 data entries per second of sound. This will show where on average in those 5 windows, the peak energy occurs. The information in outtextfile will show which frequency bands contain high amplitude data, and where they occur, useful for CUT and FILTER operations. The cutoff_frq parameter enables you to avoid gathering unncessary information. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "cutoff_frq", switch = "-c", min = 0, max = 44100, def = 8, tip = "above which spectral search begins (Default = 8.00Hz)" },
  arg4 = { name = "timewindow", switch = "-t", min = 0, max = length/1000, def = 0.1, tip = "for energy averaging: in SECONDS (Default = 0.1000)" },
  arg5 = { name = "frqwindow", switch = "-f", min = 0.02, max = 1000, def = 0.5, tip = "for energy averaging: in OCTAVES (Min 0.02. Default = 0.50)" },
  arg6 = { name = "-h", switch = "-h", tip = " adjust result for sensitivity of ear" },
}
 
dsp["Specinfo Print - Print data in an analysis file as text to file  "] = {
  cmds = { exe = "specinfo", mode = "print", tip = "SPECINFO PRINT provides a way to examine in detail the contents of a PVOC analysis file. It displays amplitude and frequency data for each channel, window by window, starting at time. The sum of all values in a single window will give the total amplitude of the sound in that window as a value between 0 and 1. The value in each channel therefore indicates the amplitude contribution of that channel to the total amplitude of the window. Musical applications; This function gives a complete listing of the amplitude and frequency content of an analysis window. The more prominent frequencies will presumably have higher amplitude values. Scanning this data should indicate where key sonic events occur." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "time", min = 0, max = length/1000, def = 0, tip = "in file at which the printout begins" },
  arg4 = { name = "windowcnt", switch = "-w", min = 0, max = 1000, def = 3, tip = "number of windows to print" },
}
 
dsp["Specinfo Report - 1 Report on frequency and time of spectral peaks - Text report on location of frequency peaks in the evolving spectrum"] = {
  cmds = { exe = "specinfo", mode = "report 1", tip = "1 Report on frequency and time of spectral peaks.  The report shows the times on the left and the peaks peaks requested in columns to the right of each time. By glancing over the data, one can tell at what time and in which frequency bands the peaks occur. This shows the time and frequency location of the spectral energy of the sound. Musical applications; SPECINFO REPORT gives more detailed information on where the spectral peaks are (especially the persistent ones, if you use the -s option). The information about spectral peaks will probably also tell you something about where formants are, or where strong audible pitch components are located." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "N", switch = "-f", tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "peaks", min = 0, max = 116, def = 50, tip = "(maximum) number of peaks to find. Range: 1­16" },
  arg7 = { name = "bt", switch = "-b", min = 0, max = 44100, def = 30, tip = "bottom frequency to start peak search" },
  arg8 = { name = "tp", switch = "-t", min = 0, max = 44100, def = 8000, tip = "top frequency to stop peak search" },
  arg9 = { name = "val", switch = "-s", min = 0, max = 24097, def = 9, tip = "number of windows over which the peaks are averaged. Range 2­4097. Default 9. Warning: high values slow up the program." },
}
 
dsp["Specinfo Report - 2 Report spectral peaks in loudness order - Text report on location of frequency peaks in the evolving spectrum"] = {
  cmds = { exe = "specinfo", mode = "report 2", tip = "2 Report spectral peaks in loudness order. The report shows the times on the left and the peaks peaks requested in columns to the right of each time. By glancing over the data, one can tell at what time and in which frequency bands the peaks occur. This shows the time and frequency location of the spectral energy of the sound. Musical applications; SPECINFO REPORT gives more detailed information on where the spectral peaks are (especially the persistent ones, if you use the -s option). The information about spectral peaks will probably also tell you something about where formants are, or where strong audible pitch components are located." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "N", switch = "-f", tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "peaks", min = 0, max = 116, def = 50, tip = "(maximum) number of peaks to find. Range: 1­16" },
  arg7 = { name = "bt", switch = "-b", min = 0, max = 44100, def = 30, tip = "bottom frequency to start peak search" },
  arg8 = { name = "tp", switch = "-t", min = 0, max = 44100, def = 8000, tip = "top frequency to stop peak search" },
  arg9 = { name = "val", switch = "-s", min = 0, max = 24097, def = 9, tip = "number of windows over which the peaks are averaged. Range 2­4097. Default 9. Warning: high values slow up the program." },
}
 
dsp["Specinfo Report - 3 Report spectral peaks as frequency only (no time data) - Text report on location of frequency peaks in the evolving spectrum"] = {
  cmds = { exe = "specinfo", mode = "report 3", tip = "3 Report spectral peaks as frequency only (no time data). The report shows the times on the left and the peaks peaks requested in columns to the right of each time. By glancing over the data, one can tell at what time and in which frequency bands the peaks occur. This shows the time and frequency location of the spectral energy of the sound. Musical applications; SPECINFO REPORT gives more detailed information on where the spectral peaks are (especially the persistent ones, if you use the -s option). The information about spectral peaks will probably also tell you something about where formants are, or where strong audible pitch components are located." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "N", switch = "-f", tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "peaks", min = 0, max = 116, def = 50, tip = "(maximum) number of peaks to find. Range: 1­16" },
  arg7 = { name = "bt", switch = "-b", min = 0, max = 44100, def = 30, tip = "bottom frequency to start peak search" },
  arg8 = { name = "tp", switch = "-t", min = 0, max = 44100, def = 8000, tip = "top frequency to stop peak search" },
  arg9 = { name = "val", switch = "-s", min = 0, max = 24097, def = 9, tip = "number of windows over which the peaks are averaged. Range 2­4097. Default 9. Warning: high values slow up the program." },
}
 
dsp["Specinfo Report - 4 Report spectral peaks in loudness order, as frequency only (no time data) - Text report on location of frequency peaks in the evolving spectrum"] = {
  cmds = { exe = "specinfo", mode = "report 4", tip = "4 Report spectral peaks in loudness order, as frequency only (no time data). The report shows the times on the left and the peaks peaks requested in columns to the right of each time. By glancing over the data, one can tell at what time and in which frequency bands the peaks occur. This shows the time and frequency location of the spectral energy of the sound. Musical applications; SPECINFO REPORT gives more detailed information on where the spectral peaks are (especially the persistent ones, if you use the -s option). The information about spectral peaks will probably also tell you something about where formants are, or where strong audible pitch components are located." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output.txt", output = "txt", },
  arg3 = { name = "N", switch = "-f", tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "peaks", min = 0, max = 116, def = 50, tip = "(maximum) number of peaks to find. Range: 1­16" },
  arg7 = { name = "bt", switch = "-b", min = 0, max = 44100, def = 30, tip = "bottom frequency to start peak search" },
  arg8 = { name = "tp", switch = "-t", min = 0, max = 44100, def = 8000, tip = "top frequency to stop peak search" },
  arg9 = { name = "val", switch = "-s", min = 0, max = 24097, def = 9, tip = "number of windows over which the peaks are averaged. Range 2­4097. Default 9. Warning: high values slow up the program." },
}

dsp["Specinfo Channel - Returns PVOC channel number corresponding to a given frequency"] = { 
  cmds = { exe = "specinfo", mode = "channel", input = "pvoc", tip = "This function displays the number of the analysis channel in which the specified frequency is most likely to occur. Note however that in practice a frequency may be displaced by up to 8 channels on either side of the anticipated channel.Note that this does not mean that the frequency you give is actually present in that channel, only that this is the channel it would be in if it were present. To determine which frequencies in a soundfile for which you might want channel information, you could use REPITCH GETPITCH, extracting the pitch data as a breakpoint file so that it can be examined easily. However, do note that there will be a weighting towards harmonic partials, though not exclusively, because GETPITCH is focused on 'pitch' as such; 'pitches' usually sound as they do because of the focusing effect of harmonically related partials (i.e., integer multiples of the fundamental). Musical applications; This function may be largely redundant in the current Release 4 system. It was needed earlier to specify a 'frequency divide channel number', but this is now handled internally by the relevant functions. Still, it may be instructive to look at which channels various frequencies occur in, for example when deciding on the number of analysis bands to have (frequency resolution). " }, 
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "frequency", min = 0, max = 44100, def = 1000, tip = "frequency frequency in Hz" },
}

dsp["Specinfo Frequency - Returns the centre frequency of the PVOC channel specified "] = { 
  cmds = { exe = "specinfo", mode = "frequency", input = "pvoc", tip = "The same caution applies as was stated for SPECINFO CHANNEL: the frequency returned may or may not actually be present in that channel. This function simply identifies the frequencies that correspond to a given analysis channel. It's a mathematical correlation, not an actual analysis of a real sound. Musical applications; As with SPECINFO CHANNEL, largely redundant, but possibly useful for noting frequency and channel correlations." }, 
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "analysis_channel_number", min = 0, max = 44100, def = 512, tip = "number of the analysis channel for which you would like to know the corresponding frequency. (Range: from 1 to the number of channels in the analysis, as given for the -N flag of PVOC.)" },
}

dsp["Specinfo Level - Convert (varying) level of the analysis file to a pseudo-soundfile (1 window -> 1 sample)"] = { 
  cmds = { exe = "specinfo", mode = "level", input = "pvoc", tip = "LEVEL constructs a pseudo-soundfile in which the amplitude level of each analysis window becomes the amplitude level of each 'sample' in the display. This pseudo-soundfile can then be viewed with most sound waveform display programs. Each window is in fact a frame time: 1/analysis_rate. See the Phase Vocoder Reference Manual for a discussion of frames. Musical applications; The purpose of this program is to enable the user to get an overall picture of where in the analysis file the strongest signal lies. The CDP VIEWSF program, available on both the Falcon and the PC platforms, will display the level of each analysis window on the 0 to 32767 scale. On the Atari, use the a option to get a display accurate to the sample. On the PC, zoom to the sample level display.) The time shown is reckoned by the VIEWSF program to relate to samples, not to analysis windows, so it does not actually display the real time in the analysis file. Each vertical line in the SPECINFO LEVEL pseudo-soundfile actually relates to sample_rate/analysis_rate samples. For example, 22050 (the sample rate) divided by 172 (the analysis rate) = 128. (Note that in the DIRSF display, the analysis rate is shown in the number-of-channels column.) VIEWSF thinks that this is only one sample (of sound) and will show the time accordingly. Therefore, to convert to the real time in the soundfile, you need to multiply the time displayed by the number of samples it really represents: real_time=time_displayed * (sample_rate/analysis_rate). This information indicates where in the sound the frequencies have the greatest amplitude, which may be useful when determining how and when to apply filtering to the sound. " }, 
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },  
}

dsp["Specinfo Windowcnt - Returns the number of analysis windows in infile"] = { 
  cmds = { exe = "specinfo", mode = "windowcnt", input = "pvoc", tip = " SPECINFO WINDOWCNT is a simple utility which reports on the number of windows there are in the analysis file for the sound infile. Musical applications; Why might this information be useful? I don't know. [Ed.]" },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
}
 
------------------------------------------------
-- specnu
------------------------------------------------
 
dsp["Specnu Clean - Eliminate from the source file any persisting signal that falls below a threshold (defined by the noise file)"] = {
  cmds = { exe = "specnu", mode = "clean", tip = "This process enables sources (once analysed) to be cleaned, and is similar to the earlier version of SPEC CLEAN. It differs from SPEC CLEAN by having a persist parameter. Thus with SPECNU CLEAN you can specify how long a signal must persist before you will accept it as valid data (rather than simply noise to be eliminated). This avoids the 'bubbling' problem of the original SPEC CLEAN. With SPEC CLEAN, when a signal was near the noise threshold, the spectrum might be judged as acceptable signal, or as noise, from one window to another, possibly changing rapidly between (accepted) signal and (eliminated) noise. The result of this switching on and off of the signal in the output, is a 'bubbling' effect in the sound. The persist parameter ensures that the signal does not bubble in and out in this way. Musical applications This process is intended for cleaning up recordings made in noisy environments. It may be used alongside various editing and filtering processes, to clean a source sound bit by bit." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "persist", min = 0, max = 1000, def = 10, tip = "minimum time (in ms.) for which the clean signal must persist above the threshold level if it is to be retained (Range 0 to 1000 ms.)" },
  arg5 = { name = "noisgain", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}
 
dsp["Specnu Remove 1 - Remove a pitched component from the spectrum of a sound - Removes pitch and its harmonics up to a specified frequency limit"] = {
  cmds = { exe = "specnu", mode = "remove 1", tip = "1 Removes pitch and its harmonics up to a specified frequency limit. SPECNU REMOVE will only work with stable, clearly-pitched components in the spectrum. It allows unwanted signals with steady pitch to be removed from a source. Deciding how many harmonics (and how large a pitch-range) should be eliminated is a matter of judgement based on listening to the results. Removing more is not necessarily better as the resulting hole in the spectrum can appear to emphasize the very pitch(band) you wanted to eliminate. Musical applications; This process is intended for cleaning up recordings made in noisy environments. It is particularly useful for removing pitched components of speech from unwanted speaking voices behind a recording. (Filtering processes can be used to remove unwanted sibillants). It may be used alongside various editing and filtering processes, to clean a source sound bit by bit. These various functions are integrated in the Cleaning Kit option on the Music Testbed of the Sound Loom. Applied to a vocal sound, Mode 1 leaves a modest 'hole' in the sound. Trevor Wishart writes: There are lots of complications in removing a pitch from a sound, which the user will discover. If you remove too many harmonics, the hole itself seems to produce the very pitch you were trying to remove! I have no idea why this is. The high frequency limit should therefore be kept as low as possible if you are seriously attempting to remove an existing pitch element from a sound." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "midimin", min = 1, max = 127, def = 1, tip = "minimum pitch to remove (MIDI pitch values 1-127)" },
  arg4 = { name = "midimax", min = 1, max = 127, def = 1, tip = "maximum pitch to remove (MIDI pitch values 1-127) (all pitches between midimin and midimax are removed)" },
  arg5 = { name = "rangetop", min = 0, max = 22050, def = 1000, tip = "frequency (hz) at which the search for associated harmonics (TO BE ELIMINATED) stops" },
  arg6 = { name = "atten", min = 0, max = 1, def = 0.5, tip = "degree of attenuation of the suppressed components Range: 0 to 1" },
}
 
dsp["Specnu Remove 2 - Remove a pitched component from the spectrum of a sound - Removes everything besides the specified pitched material"] = {
  cmds = { exe = "specnu", mode = "remove 2", tip = "Removes everything besides the specified pitched material. SPECNU REMOVE will only work with stable, clearly-pitched components in the spectrum. It allows unwanted signals with steady pitch to be removed from a source. Deciding how many harmonics (and how large a pitch-range) should be eliminated is a matter of judgement based on listening to the results. Removing more is not necessarily better as the resulting hole in the spectrum can appear to emphasize the very pitch(band) you wanted to eliminate. Musical applications; This process is intended for cleaning up recordings made in noisy environments. It is particularly useful for removing pitched components of speech from unwanted speaking voices behind a recording. (Filtering processes can be used to remove unwanted sibillants). It may be used alongside various editing and filtering processes, to clean a source sound bit by bit. These various functions are integrated in the Cleaning Kit option on the Music Testbed of the Sound Loom. Applied to a vocal sound, Mode 1 leaves a modest 'hole' in the sound. Trevor Wishart writes: There are lots of complications in removing a pitch from a sound, which the user will discover. If you remove too many harmonics, the hole itself seems to produce the very pitch you were trying to remove! I have no idea why this is. The high frequency limit should therefore be kept as low as possible if you are seriously attempting to remove an existing pitch element from a sound." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "midimin", min = 1, max = 127, def = 1, tip = "minimum pitch to remove (MIDI pitch values 1-127)" },
  arg4 = { name = "midimax", min = 1, max = 127, def = 1, tip = "maximum pitch to remove (MIDI pitch values 1-127) (all pitches between midimin and midimax are removed)" },
  arg5 = { name = "rangetop", min = 0, max = 22050, def = 1000, tip = "frequency (hz) at which the search for associated harmonics (TO BE ELIMINATED) stops" },
  arg6 = { name = "atten", min = 0, max = 1, def = 0.5, tip = "degree of attenuation of the suppressed components Range: 0 to 1" },
}
 
dsp["Specnu Subtract - Eliminate from the source file any persisting signal that falls below a threshold (defined by the noisfile)"] = {
  cmds = { exe = "specnu", mode = "subtract", tip = "SPECNU SUBTRACT - Eliminate from the source file any persisting signal that falls below a threshold (defined by the noisfile) AND subtract the amplitude of the noise in the noisfile from any source file signal that is passed. This process is similar to SPECNU CLEAN. SPECNU CLEAN assumes that, once your desired signal has reached an appropriate level it will mask any persisting background noise. With very noisy environments, this may not be sufficient, and the entire noise signal will need to be subtracted from the source. This process is only useful where the noise in the signal is persistent (e.g. the noise of an air-conditioning unit.) Musical applications. This process is intended for cleaning up recordings made in noisy environments. It may be used alongside various editing and filtering processes, to clean a source sound bit by bit. These various functions are integrated in the Cleaning Kit option on the Music Testbed of the Sound Loom." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "persist", min = 0, max = 1000, def = 10, tip = "minimum time (in ms.) for which the clean signal must persist above the threshold level if it is to be retained (Range 0 to 1000 ms.)" },
  arg5 = { name = "noisgain", min = 1, max = 40, def = 2, tip = "multiplies noise levels found in nfile before they are used for comparison with infile signal: (Default 2)" },
}

dsp["[Doesn't work?] Specnu Rand - Randomise the order of spectral windows "] = {
  cmds = { exe = "specnu", mode = "rand", tip = "The degree of randomisation is controlled by the user. By default a set contains only one window. If all the windows are randomised, this means that the maximum randomisation is achieved. The amount of randomisation can be reduced in two ways using the optional parameters. Setting a timescale focuses the randomisation into specific time areas. Thus, as you move forward through the input file, time-specific areas will be jumbled. Giving a value for grouping > 1 keeps this number of consecutive windows the same, but then jumbles the sets of windows. Musical Applications SPECNU RAND provides a way of texturing a sound. Furthermore, as it is operating in the Spectral Domain, the process will create timbral variations. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "timescale", switch = "-t", min = 0, max = 255, def = 0.0, input = "brk", tip = "determines the number of windows locally-randomised. Default: all the windows are randomised" },
  arg4 = { name = "grouping", switch = "-g", min = 0.01, max = 10000, def = 1, tip = "consecutive windows are grouped into sets of size grouping. These sets are randomised. The default grouping is one window per set." },
}

dsp["[Doesn't work?] Specnu Squeeze - Squeeze spectrum in frequency range, around a specified centre frequencyl"] = {
cmds = { exe = "specnu", mode = "squeeze", tip = "Note that the frequency at which the squeezing is to take place can be specified (centrefrq). The result will be some form of intensification (aural density) in that part of the sound. The degree of density will depend on the squeeze factor, higher factors yielding complex 'tones' in the midst of the overall sound. At the time of writing, the program is producing output centred around 341Hz, only. However, combined with suitable transposition(s), e.g. using REPITCH TRANSPOSE, it might still be a useful way to pitch a sound." },
arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
arg3 = { name = "centrefrq", min = 0, max = 22050, def = 5900, input = "brk", tip = "frequency in Hz around which frequency data is squeezed" },
arg4 = { name = "squeeze", min = 0, max = 1, def = 0.5, input = "brk", tip = "the amount the spectrum is squeezed (< 1)" },
}

--[[
dsp["[Doesn't work - multiple outs] Specnu Slice - 1 (moirée slice) Divide an analysis file into individual frequency bands, saving each as a separate analysis file "] = {
cmds = { exe = "specnu", mode = "slice 1", input = "pvoc", tip = "1 - This process slices a signal into a number of spectral components, that can then be reassembled into the original sound, or used in some other way. The output sounds will contain a number of bands (bandcnt) and channel widths (chanwidth), depending on the user's selections, so each sound will be an amalgam of different parts of the original source. It is not possible to predict which sound will appear higher or lower, as each sound is made up of slices taken from the whole range of the source. Thus it all depends on what the source contains. In the various band slices, there could be: nothing at all (don't be surprised if you get a silent outfile), something, but a very low level (i.e., amplitude), something very prominent, something that combines the contents of another band (in the stack in question) to produce a prominent output. In Mode 4, the lower the pitch of the source, the more outputs are produced because there are more potential harmonics before you reach the maximum frequency limit. In fact, there can be a very large number of output files, many of which will be silent. This process does not know if there is any energy at any particular harmonic. It just tracks where the harmonics of the sound would be, and reports what it finds. A pure sine tone for example is going to produce many silent outputs. The number of silent outputs will be reduced if the sound has a very rich or bright spectrum, the program is of more use with this kind of input file. Musical applications; At the moment it appears to be a way to get a number of tonally different versions of the input analysis file rather quickly. The potential for composition lies in possible recombinations of these rather unpredictable slices, especially if they are processed separately before being recombined. To give some idea of what might be done, consider that interesting IRCAM experiment where different rates of vibrato are applied to the odd and to the even harmonics of a vocal sound. You start with the odd harmonics in the left channel and the even harmonics in the right channel. With no vibrato, and you hear a (mono) voice at stage centre. But when you apply a differentiated vibrato you hear a 'clarinet' in one channel and a 'cello' in the other. This suggests the idea of generalising this approach, applying different processes to the different harmonics, and recombining the results, to see what happens. Do you get an elaborate variation of the original sound or does the single original image split into several different images, and if so might this be musically useful? The potentially large number of outputs suggests that it would be practical to approach using this program by means of batch processing, perhaps controlled by scripting with a batch file or higher level scripting language. Scripting multi-sound playback, for example, would enable all the outputs to be heard in sequence, and this could then be extended to eliminating silent files or processing e.g., alternate files in different ways." },
arg1 = { name = "bandcnt", min = 2, max = 64, def = 2, tip = "the number of output files (and the channel separation of the bands in the output sounds)" },
arg2 = { name = "chanwidth", min = 1, max = 256, def = 2, tip = "chanwidth is the number of channels in each band in each output sound" },
}

--]]

--[[

dsp["[Doesn't work - multiple outs] Specnu Slice - 2 (frequency band slice) Divide an analysis file into individual frequency bands, saving each as a separate analysis file "] = {
cmds = { exe = "specnu", mode = "slice 2", input = "pvoc", tip = "2 - This process slices a signal into a number of spectral components, that can then be reassembled into the original sound, or used in some other way. The output sounds will contain a number of bands (bandcnt) and channel widths (chanwidth), depending on the user's selections, so each sound will be an amalgam of different parts of the original source. It is not possible to predict which sound will appear higher or lower, as each sound is made up of slices taken from the whole range of the source. Thus it all depends on what the source contains. In the various band slices, there could be: nothing at all (don't be surprised if you get a silent outfile), something, but a very low level (i.e., amplitude), something very prominent, something that combines the contents of another band (in the stack in question) to produce a prominent output. In Mode 4, the lower the pitch of the source, the more outputs are produced because there are more potential harmonics before you reach the maximum frequency limit. In fact, there can be a very large number of output files, many of which will be silent. This process does not know if there is any energy at any particular harmonic. It just tracks where the harmonics of the sound would be, and reports what it finds. A pure sine tone for example is going to produce many silent outputs. The number of silent outputs will be reduced if the sound has a very rich or bright spectrum, the program is of more use with this kind of input file. Musical applications; At the moment it appears to be a way to get a number of tonally different versions of the input analysis file rather quickly. The potential for composition lies in possible recombinations of these rather unpredictable slices, especially if they are processed separately before being recombined. To give some idea of what might be done, consider that interesting IRCAM experiment where different rates of vibrato are applied to the odd and to the even harmonics of a vocal sound. You start with the odd harmonics in the left channel and the even harmonics in the right channel. With no vibrato, and you hear a (mono) voice at stage centre. But when you apply a differentiated vibrato you hear a 'clarinet' in one channel and a 'cello' in the other. This suggests the idea of generalising this approach, applying different processes to the different harmonics, and recombining the results, to see what happens. Do you get an elaborate variation of the original sound or does the single original image split into several different images, and if so might this be musically useful? The potentially large number of outputs suggests that it would be practical to approach using this program by means of batch processing, perhaps controlled by scripting with a batch file or higher level scripting language. Scripting multi-sound playback, for example, would enable all the outputs to be heard in sequence, and this could then be extended to eliminating silent files or processing e.g., alternate files in different ways." },
arg1 = { name = "bandcnt", min = 2, max = 64, def = 2, tip = "the number of output files (and the channel separation of the bands in the output sounds)" },
arg2 = { name = "chanwidth", min = 0, max = 44100, def = 10, tip = "chanwidth is the width in Hz of each band in each output sound" },
}

--]]

--[[

dsp["[Doesn't work - multiple outs] Specnu Slice - 3 (pitch band slice) Divide an analysis file into individual frequency bands, saving each as a separate analysis file "] = {
cmds = { exe = "specnu", mode = "slice 3", input = "pvoc", tip = "3 - This process slices a signal into a number of spectral components, that can then be reassembled into the original sound, or used in some other way. The output sounds will contain a number of bands (bandcnt) and channel widths (chanwidth), depending on the user's selections, so each sound will be an amalgam of different parts of the original source. It is not possible to predict which sound will appear higher or lower, as each sound is made up of slices taken from the whole range of the source. Thus it all depends on what the source contains. In the various band slices, there could be: nothing at all (don't be surprised if you get a silent outfile), something, but a very low level (i.e., amplitude), something very prominent, something that combines the contents of another band (in the stack in question) to produce a prominent output. In Mode 4, the lower the pitch of the source, the more outputs are produced because there are more potential harmonics before you reach the maximum frequency limit. In fact, there can be a very large number of output files, many of which will be silent. This process does not know if there is any energy at any particular harmonic. It just tracks where the harmonics of the sound would be, and reports what it finds. A pure sine tone for example is going to produce many silent outputs. The number of silent outputs will be reduced if the sound has a very rich or bright spectrum, the program is of more use with this kind of input file. Musical applications; At the moment it appears to be a way to get a number of tonally different versions of the input analysis file rather quickly. The potential for composition lies in possible recombinations of these rather unpredictable slices, especially if they are processed separately before being recombined. To give some idea of what might be done, consider that interesting IRCAM experiment where different rates of vibrato are applied to the odd and to the even harmonics of a vocal sound. You start with the odd harmonics in the left channel and the even harmonics in the right channel. With no vibrato, and you hear a (mono) voice at stage centre. But when you apply a differentiated vibrato you hear a 'clarinet' in one channel and a 'cello' in the other. This suggests the idea of generalising this approach, applying different processes to the different harmonics, and recombining the results, to see what happens. Do you get an elaborate variation of the original sound or does the single original image split into several different images, and if so might this be musically useful? The potentially large number of outputs suggests that it would be practical to approach using this program by means of batch processing, perhaps controlled by scripting with a batch file or higher level scripting language. Scripting multi-sound playback, for example, would enable all the outputs to be heard in sequence, and this could then be extended to eliminating silent files or processing e.g., alternate files in different ways." },
arg1 = { name = "bandcnt", min = 2, max = 64, def = 2, tip = "the number of output files (and the channel separation of the bands in the output sounds)" },
arg2 = { name = "chanwidth", min = -24, max = 24, def = 2, tip = "chanwidth is the width in semitones of each band in each output sound" },
}

--]]

--[[

dsp["[Doesn't work - multiple outs] Specnu Slice - 4 (harmonics slice) Divide an analysis file into individual frequency bands, saving each as a separate analysis file "] = {
cmds = { exe = "specnu", mode = "slice 4", input = "pvoc", tip = "4 - This process slices a signal into a number of spectral components, that can then be reassembled into the original sound, or used in some other way. The output sounds will contain a number of bands (bandcnt) and channel widths (chanwidth), depending on the user's selections, so each sound will be an amalgam of different parts of the original source. It is not possible to predict which sound will appear higher or lower, as each sound is made up of slices taken from the whole range of the source. Thus it all depends on what the source contains. In the various band slices, there could be: nothing at all (don't be surprised if you get a silent outfile), something, but a very low level (i.e., amplitude), something very prominent, something that combines the contents of another band (in the stack in question) to produce a prominent output. In Mode 4, the lower the pitch of the source, the more outputs are produced because there are more potential harmonics before you reach the maximum frequency limit. In fact, there can be a very large number of output files, many of which will be silent. This process does not know if there is any energy at any particular harmonic. It just tracks where the harmonics of the sound would be, and reports what it finds. A pure sine tone for example is going to produce many silent outputs. The number of silent outputs will be reduced if the sound has a very rich or bright spectrum, the program is of more use with this kind of input file. Musical applications; At the moment it appears to be a way to get a number of tonally different versions of the input analysis file rather quickly. The potential for composition lies in possible recombinations of these rather unpredictable slices, especially if they are processed separately before being recombined. To give some idea of what might be done, consider that interesting IRCAM experiment where different rates of vibrato are applied to the odd and to the even harmonics of a vocal sound. You start with the odd harmonics in the left channel and the even harmonics in the right channel. With no vibrato, and you hear a (mono) voice at stage centre. But when you apply a differentiated vibrato you hear a 'clarinet' in one channel and a 'cello' in the other. This suggests the idea of generalising this approach, applying different processes to the different harmonics, and recombining the results, to see what happens. Do you get an elaborate variation of the original sound or does the single original image split into several different images, and if so might this be musically useful? The potentially large number of outputs suggests that it would be practical to approach using this program by means of batch processing, perhaps controlled by scripting with a batch file or higher level scripting language. Scripting multi-sound playback, for example, would enable all the outputs to be heard in sequence, and this could then be extended to eliminating silent files or processing e.g., alternate files in different ways." },
arg1 = { name = "pitchdata", min = 0, max = 0, def = 0, input = "brk", tip = "breakpoint text file containing time pitch value pairs " },
}

--]]
 
------------------------------------------------
-- specross
------------------------------------------------
 
dsp["Specross Partials - Interpolate partials of pitched inanalfile1 towards those of pitched inanalfile2 "] = {
  cmds = { exe = "specross", mode = "partials", tip = "The importance of SPECROSS PARTIALS is that it is focusing on pitched sounds. An interpolation (gradual transition) is being made from one sound to another. In this case, the harmonic partials, i.e., those that are integer multiples. These are heard as pitches because the partials fuse into a single (pitched) entity. This is a welcome addition to the CDP transition programs, because the others deal with any partial, whether harmonic or inharmonic. SPECROSS could arguably be placed in the COMBINE or even the MORPH set. Robert Fraser has placed it in the EXTRA menu of Soundshaper. He has found that its results are not unlike GLISTEN, so I've placed it here in the SPEC set. NEWMORPH morphs between spectral peaks, whereas SPECROSS moves between harmonically-related partials. Robert finds that the NEWMORPH spectral peak transitions are very effective, so you are advised to use SPECROSS as an experimental option and see if you can pick up any subtle differences between the two processes. [Ed.] The harmonically-related partials taken from the pitched source in effect form a colouration, not unlike a formant. This colouration is taken from the second sound and imposed on the first, thus re-colouring it. If the thresh and interp parameters are set to high values, the partials-colouration is so strong that there is virtually nothing left audible from the rest of sound-2, only the colouration comes across to sound-1. If the thresh and interp parameters are on the low side, some of the rest of the sound will come across as well. The example command line uses lower values. Other parameters such as tuning provide ways to fine-tune just how the harmonic qualities of sound2 are picked up. A lower value here is more 'forgiving' and is likely to use more partials. Musical Applications; This program provides a tool to work with recolouring pitched sounds. The various parameters enable you to define how intensely this is done, how focused the result is on the harmonically related partials, and how much else of the second sound is used, making its presence more audible in the output sound. Although designed for use with two pitched sounds, I tried recolouring the vocal file count.ana with the sound of a horn (horn2.ana, which was horn.ana + horn.ana joined with ANALJOIN JOIN). The result was a highly modulated vocal sound with a significant amount of horn colouration – a rather interesting result." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "tuning", min = 0, max = 6, def = 1, tip = "the range in semitones within which the harmonics are 'in tune'; higher values are more 'forgiving' and use more of the second file" },
  arg5 = { name = "minwin", min = 0, max = 40000, def = 2, tip = "the minimum number of adjacent windows that must be pitched, for a pitch-value to be registered (Default: 2" },
  arg6 = { name = "signois", min = 0, max = 1000, def = 80, tip = "the signal-to-noise ration, in decibels (Default 80dB)." },
  arg7 = { name = "harmcnt", min = 1, max = 8, def = 5, tip = "The number of the 8 loudest peaks in the spectrum which much be harmonics in order to confirm that the sound is pitched (Default 5)" },
  arg8 = { name = "lo", min = 10, max = 2756.25, def = 10, tip = "the lowest acceptable frequency for a pitch (Default: 10Hz)" },
  arg9 = { name = "hi", min = 10, max = 2756.25, tip = "the highest acceptable frequency for a valid pitch" },
  arg10 = { name = "thresh", min = 0, max = 1, tip = "the minimum acceptable level of any partial found" },
  arg11 = { name = "level", min = 0, max = 1, def = 1, tip = "the level of the output (Default: 1.0)." },
  arg12 = { name = "interp", min = 0, max = 1, input = "brk", tip = "the interpolation between inanalfile2 and inanalfile1" },
}

------------------------------------------------
-- specgrids
------------------------------------------------

--[[

dsp["[Doesn't work - multiple outs] Specgrids - Partition a spectrum into parts, over a grid"] = {
cmds = { exe = "specgrids", mode = "specgrids", input = "pvoc", tip = "This program enables you to separate out different parts of a sounds spectrum and place these different parts over a multi-channel rig. First it divides the spectrum of a set of disjunct spectra. This means that the input analysis file needs to have distinctly different spectral features – heard as timbral colours. Each output spectrum will contain some selection of the spectral channels of the source, while setting the other channels to zero amplitude. Between them, the complete set of output spectra contains all the channel info from the source. Now you are in a position to resynthesise the different segment-streams separately and distribute the resulting soundfiles over a multichannel output. Thus the spectrum of the original sound is distributed around the multichannel space. Note: This function is identical to SPECNU SLICE, Mode 1, as Trevor Wishart has confirmed." },
arg1 = { name = "outfilecnt", min = 1, max = 64, def = 2, tip = "the number of output spectral files to create" },
arg2 = { name = "changrouping", min = 0, max = 4096, def = 512, tip = "the number of adjacent channels per group in the output spectra" },
}

--]]
 
------------------------------------------------
-- specsphinx
------------------------------------------------
 
dsp["Specsphinx 1 - Impose channel amplitudes of inanalfile2 onto the channel frequencies of inanalfile1."] = {
  cmds = { exe = "specsphinx", mode = "specsphinx 1", tip = "SPECSPHINX – Impose channel amplitudes of inanalfile2 onto the channel frequencies of inanalfile1. SPECSPHINX is one of three experimental programs designed to create spectra that are intermediate between two given spectra. (The other two are NEWMORPH and SPECTWIN. Mode 1 is related to COMBINE CROSS, which replaces the channel amplitudes with those of Infile2. (Here, ampbalance sets the proportion of amplitudes to be retained.)" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "ampbalance", switch = "-a", min = 0, max = 1, def = 0.5, tip = "proportion of inanalfile1's amplitudes that are retained. Default = 0" },
  arg5 = { name = "frqbalance", switch = "-f", min = 0, max = 1, def = 0.5, tip = "proportion of inanalfile2's frequencies injected into the output spectrum. Default = 0" },
}
 
dsp["Specsphinx 2 - Multiply the spectra "] = {
  cmds = { exe = "specsphinx", mode = "specsphinx 2", tip = "Multiply the spectra - SPECSPHINX – Impose channel amplitudes of inanalfile2 onto the channel frequencies of inanalfile1. SPECSPHINX is one of three experimental programs designed to create spectra that are intermediate between two given spectra. (The other two are NEWMORPH and SPECTWIN." },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "bias", switch = "-b", min = -1, max = 1, def = 0, tip = "When non-zero, it adds a proportion of the original files to the output" },
  arg5 = { name = "gain", switch = "-g", min = 0.01, max = 100, def = 1, tip = "overall gain applied to the output " },
}
 
------------------------------------------------
-- spectwin
------------------------------------------------
 
dsp["Spectwin 1 - Combine the formant and/or total spectral envelopes of 2 spectra - Formant envelope of inanalfile1 with the formant envelope of inanalfile2"] = {
  cmds = { exe = "spectwin", mode = "spectwin 1", tip = "Formant envelope of inanalfile1 with the formant envelope of inanalfile2 - SPECTWIN creates hybrid sounds by combining the formant envelopes and/or total spectral envelope of two files. frqint and envint specify the dominance of inanalfile2's spectral frequencies and spectral envelope, respectively. Various hybrids can be formed by reducing the effect of one of these. Roughly speaking, if envint is at maximum and frqint is at minimum, the result is a transformation of inanalfile1 (heavily influenced by inanalfile2); if the reverse, the effect is a transformation of inanalfile2. [R Fraser]" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "frqint", switch = "-f", min = 0, max = 1, def = 1, tip = "dominance of the spectral frequencies of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg5 = { name = "envint", switch = "-e", min = 0, max = 1, def = 1, tip = "dominance of the spectral envelope of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg6 = { name = "dupl", switch = "-d", min = 0, max = 8, def = 2, tip = "duplicate the sound of inanalfile1 dupl times, at higher pitches" },
  arg7 = { name = "step", switch = "-s", min = 0, max = 48, tip = "the transposition step in (fractions of) semitones at each duplication" },
  arg8 = { name = "dec", switch = "-r", min = 0, max = 1, tip = "the amplitude level multiplier from one transposition to the next when creating the transposed duplications" },
}
 
dsp["Spectwin 2 - Combine the formant and/or total spectral envelopes of 2 spectra - Formant envelope of inanalfile1 with the total envelope of inanalfile2 "] = {
  cmds = { exe = "spectwin", mode = "spectwin 2", tip = "2 Formant envelope of inanalfile1 with the total envelope of inanalfile2 - SPECTWIN creates hybrid sounds by combining the formant envelopes and/or total spectral envelope of two files. frqint and envint specify the dominance of inanalfile2's spectral frequencies and spectral envelope, respectively. Various hybrids can be formed by reducing the effect of one of these. Roughly speaking, if envint is at maximum and frqint is at minimum, the result is a transformation of inanalfile1 (heavily influenced by inanalfile2); if the reverse, the effect is a transformation of inanalfile2. [R Fraser]" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "frqint", switch = "-f", min = 0, max = 1, def = 1, tip = "dominance of the spectral frequencies of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg5 = { name = "envint", switch = "-e", min = 0, max = 1, def = 1, tip = "dominance of the spectral envelope of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg6 = { name = "dupl", switch = "-d", min = 0, max = 8, def = 2, tip = "duplicate the sound of inanalfile1 dupl times, at higher pitches" },
  arg7 = { name = "step", switch = "-s", min = 0, max = 48, tip = "the transposition step in (fractions of) semitones at each duplication" },
  arg8 = { name = "dec", switch = "-r", min = 0, max = 1, tip = "the amplitude level multiplier from one transposition to the next when creating the transposed duplications" },
}
 
dsp["Spectwin 3 - Combine the formant and/or total spectral envelopes of 2 spectra - Total envelope of inanalfile1 with the formant envelope of inanalfile2"] = {
  cmds = { exe = "spectwin", mode = "spectwin 3", tip = "3 Total envelope of inanalfile1 with the formant envelope of inanalfile2 - SPECTWIN creates hybrid sounds by combining the formant envelopes and/or total spectral envelope of two files. frqint and envint specify the dominance of inanalfile2's spectral frequencies and spectral envelope, respectively. Various hybrids can be formed by reducing the effect of one of these. Roughly speaking, if envint is at maximum and frqint is at minimum, the result is a transformation of inanalfile1 (heavily influenced by inanalfile2); if the reverse, the effect is a transformation of inanalfile2. [R Fraser]" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "frqint", switch = "-f", min = 0, max = 1, def = 1, tip = "dominance of the spectral frequencies of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg5 = { name = "envint", switch = "-e", min = 0, max = 1, def = 1, tip = "dominance of the spectral envelope of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg6 = { name = "dupl", switch = "-d", min = 0, max = 8, def = 2, tip = "duplicate the sound of inanalfile1 dupl times, at higher pitches" },
  arg7 = { name = "step", switch = "-s", min = 0, max = 48, tip = "the transposition step in (fractions of) semitones at each duplication" },
  arg8 = { name = "dec", switch = "-r", min = 0, max = 1, tip = "the amplitude level multiplier from one transposition to the next when creating the transposed duplications" },
}
 
dsp["Spectwin 4 - Combine the formant and/or total spectral envelopes of 2 spectra - Total envelope of inanalfile1 with the total envelope of inanalfile2"] = {
  cmds = { exe = "spectwin", mode = "spectwin 4", tip = "4 Total envelope of inanalfile1 with the total envelope of inanalfile2 - SPECTWIN creates hybrid sounds by combining the formant envelopes and/or total spectral envelope of two files. frqint and envint specify the dominance of inanalfile2's spectral frequencies and spectral envelope, respectively. Various hybrids can be formed by reducing the effect of one of these. Roughly speaking, if envint is at maximum and frqint is at minimum, the result is a transformation of inanalfile1 (heavily influenced by inanalfile2); if the reverse, the effect is a transformation of inanalfile2. [R Fraser]" },
  arg1 = { name = "Input 1", input = "ana", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "ana", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg4 = { name = "frqint", switch = "-f", min = 0, max = 1, def = 1, tip = "dominance of the spectral frequencies of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg5 = { name = "envint", switch = "-e", min = 0, max = 1, def = 1, tip = "dominance of the spectral envelope of inanalfile2. Range: 0 to 1. Default: 1.0" },
  arg6 = { name = "dupl", switch = "-d", min = 0, max = 8, def = 2, tip = "duplicate the sound of inanalfile1 dupl times, at higher pitches" },
  arg7 = { name = "step", switch = "-s", min = 0, max = 48, tip = "the transposition step in (fractions of) semitones at each duplication" },
  arg8 = { name = "dec", switch = "-r", min = 0, max = 1, tip = "the amplitude level multiplier from one transposition to the next when creating the transposed duplications" },
}
 
------------------------------------------------
-- strange
------------------------------------------------
 
dsp["Strange Glis - 1 shepard tones - create glissandi inside the (changing) spectral envelope of the original sound  "] = {
  cmds = { exe = "strange", mode = "glis 1", tip = "shepard tones - For this process to work, you need to enable one of the N toggles first. This program extracts the time-changing spectral contour (the spectral trajectory) of the sound. It therefore retains any spectral articulation, such as patterns of speech, but replaces the signal by an endlessly rising (positive values for glissrate) or falling (negative values for glissrate) shepard tone or inharmonic glissando. It uses linear interpolation in determining the spectral contour. In Mode 1 (shepard tones) the key parameter appears to be glissrate. A high positive or negative value (e.g., + or -50) introduces quite a bit of noise into the sound, whereas a low value such as 0.3 results in a twangy vibrating sound.  Musical applications; In effect, the original sound is replaced by the glisandoing tone, but the articulations of the original are retained, i.e., the rhythm and attack transients of the spectral envelope are notably the same. However, the tone of the sound is greatly altered, acquiring in Mode 2 as noted above a mushy twang with low values for spacing and a rather hollow vibrating sound with higher values for spacing. One set of trials on a low drum sound transformed it into something not unlike a didgerdoo. Because formants are involved, input sounds with significant resonance features work well with this function, e.g., vocal and some drum sounds, especially those lower in the frequency range. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 0, max = 256, def = 256, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 12, def = 12, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "glisrate", min = -100, max = 100, def = 2, tip = "rate of glissing in semitones per second (negative value for downward glissando)" },
  arg7 = { name = "topfrq", switch = "-t", min = 0, max = 22050, def = 1000, tip = "top of spectrum: must be > 2*channel width of the analysis. Default: Nyquist (i.e., ½ the sample rate)" },
}
 
dsp["Strange Glis - 2 inharmonic glide - create glissandi inside the (changing) spectral envelope of the original sound  "] = {
  cmds = { exe = "strange", mode = "glis 2", tip = "2 inharmonic glide - For this process to work, you need to toggle one of the N options first! This program extracts the time-changing spectral contour (the spectral trajectory) of the sound. It therefore retains any spectral articulation, such as patterns of speech, but replaces the signal by an endlessly rising (positive values for glissrate) or falling (negative values for glissrate) shepard tone or inharmonic glissando. It uses linear interpolation in determining the spectral contour.  In Mode 2 (inharmonic glissando) the two most salient parameters are glissrate and spacing. Higher postive or lower negative values for glissrate, e.g., +24 or -24, +100 or -100, gives a very rapidly cycling glissando (a pretty mushy twang). Low values for spacing (e.g., 45) gives a fairly deep twang, whereas high values, e.g. 1101, results in a rather hollow sound. The number of channels (frequencywise formant extraction) or bands per octave (pitchwise) affects the resolution of the analysis without changing the resulting sound effect very much, and adjusting the top frequency can offer a certain degree of filtering control. Musical applications; In effect, the original sound is replaced by the glisandoing tone, but the articulations of the original are retained, i.e., the rhythm and attack transients of the spectral envelope are notably the same. However, the tone of the sound is greatly altered, acquiring in Mode 2 as noted above a mushy twang with low values for spacing and a rather hollow vibrating sound with higher values for spacing. One set of trials on a low drum sound transformed it into something not unlike a didgerdoo. Because formants are involved, input sounds with significant resonance features work well with this function, e.g., vocal and some drum sounds, especially those lower in the frequency range." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 0, max = 256, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 12, def = 12, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "glisrate", min = -100, max = 100, def = 2, tip = "rate of glissing in semitones per second (negative value for downward glissando)" },
  arg7 = { name = "hzstep", min = 43.5, max = 11025, def = 50, tip = "partials spacing in inharmonic glide. Range: FROM channel-frq-width (usually 43Hz) TO Nyquist/2 (i.e., ¼ the sample rate)" },
  arg8 = { name = "topfrq", switch = "-t", min = 0, max = 22050, def = 1000, tip = "top of spectrum: must be > 2*channel width of the analysis. Default: Nyquist (i.e., ½ the sample rate)" },
}
 
dsp["Strange Glis - 3 self-glissando - create glissandi inside the (changing) spectral envelope of the original sound  "] = {
  cmds = { exe = "strange", mode = "glis 3", tip = "3 self-glissando - For this process to work, you need to enable one of the N toggles first! This program extracts the time-changing spectral contour (the spectral trajectory) of the sound. It therefore retains any spectral articulation, such as patterns of speech, but replaces the signal by an endlessly rising (positive values for glissrate) or falling (negative values for glissrate) shepard tone or inharmonic glissando. It uses linear interpolation in determining the spectral contour.  In Mode 3 (self-glissando); the original tonal quality of the sound is largely preserved, especially with low values for glissrate. With higher values of glissrate, a higher, somewhat hollow glissando is added to the sound, otherwise very much like the original input. Musical applications; In effect, the original sound is replaced by the glisandoing tone, but the articulations of the original are retained, i.e., the rhythm and attack transients of the spectral envelope are notably the same. However, the tone of the sound is greatly altered, acquiring in Mode 2 as noted above a mushy twang with low values for spacing and a rather hollow vibrating sound with higher values for spacing. One set of trials on a low drum sound transformed it into something not unlike a didgerdoo. Because formants are involved, input sounds with significant resonance features work well with this function, e.g., vocal and some drum sounds, especially those lower in the frequency range. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "N", switch = "-f", min = 0, max = 256, tip = "extract formant envelope linear frequency-wise, using 1 point for every N equally-spaced frequency-channels" },
  arg4 = { name = "N", switch = "-p", min = 0, max = 12, def = 12, tip = "extract formant envelope linear pitchwise, using N equally-spaced pitch-bands per octave" },
  arg5 = { name = "-i", switch = "-i", tip = "quicksearch for formants (less accurate)" },
  arg6 = { name = "glisrate", min = -100, max = 100, def = 2, tip = "rate of glissing in semitones per second (negative value for downward glissando)" },
  arg7 = { name = "topfrq", switch = "-t", min = 0, max = 22050, def = 1000, tip = "top of spectrum: must be > 2*channel width of the analysis. Default: Nyquist (i.e., ½ the sample rate)" },
}
 
dsp["Strange Invert - 1 Normal inversion - invert the spectrum  "] = {
  cmds = { exe = "strange", mode = "invert 1", tip = "1 Normal inversion - NB: At the time of last update, it has been found that this function is not working properly. This function was formerly SPEC INV, which required that its infile be generated by SPEC BARE (zeros channels which don't contain harmonics). The SPEC BARE function is now built into STRANGE INVERT, meaning that the result will have more of a 'harmonic' quality than it might have. This will be related to the degree that the original sound was distinctly pitched. The main reason for including the BARE function, however, was simply to make the spectrum 'easier to manipulate', so it doesn't go so far as to harmonise the sound as such (see the PITCH functions for this). What STRANGE INVERT does is to invert the spectral envelope of the sound, relative to the overall spectral envelope. It is derived from an idea developed by Allen Strange. Musical applications; The spectral envelope is shaped by the amplitude level of the various frequencies, changing through time. Inverting this shape should reduce the level of the same frequencies, with the effect of making others more prominent. So what should be most noticeable would be a timbral change in the sound, perhaps deeper and richer in character. But it will be important to explore the possibilities by submitting different types of source to it, ranging from strong, focussed pitches, to sounds of a more amorphous nature. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}
 
dsp["Strange Invert - 2 Output sound retains the amplitude envelope of the source sound"] = {
  cmds = { exe = "strange", mode = "invert 2", tip = "2 Output sound retains the amplitude envelope of the source sound - NB: At the time of last update, it has been found that this function is not working properly. This function was formerly SPEC INV, which required that its infile be generated by SPEC BARE (zeros channels which don't contain harmonics). The SPEC BARE function is now built into STRANGE INVERT, meaning that the result will have more of a 'harmonic' quality than it might have. This will be related to the degree that the original sound was distinctly pitched. The main reason for including the BARE function, however, was simply to make the spectrum 'easier to manipulate', so it doesn't go so far as to harmonise the sound as such (see the PITCH functions for this). What STRANGE INVERT does is to invert the spectral envelope of the sound, relative to the overall spectral envelope. It is derived from an idea developed by Allen Strange. Musical applications; The spectral envelope is shaped by the amplitude level of the various frequencies, changing through time. Inverting this shape should reduce the level of the same frequencies, with the effect of making others more prominent. So what should be most noticeable would be a timbral change in the sound, perhaps deeper and richer in character. But it will be important to explore the possibilities by submitting different types of source to it, ranging from strong, focussed pitches, to sounds of a more amorphous nature. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
}
 
dsp["Strange Shift - 1 Shift the whole spectrum - Linear frequency shift of (part of) the spectrum"] = {
  cmds = { exe = "strange", mode = "shift 1", tip = "1 Shift the whole spectrum - The fact that this shift is linear means that it will produce inharmonic results: it will alter harmonic relationships between partials. The linear (i.e., additive) change distorts harmonic relationships, which are based on multiplication. Musical applications; This function can be used to create major alterations in the timbral character of a sound. A shift upwards will emphasise high frequencies in a way which makes a dense, inharmonic sound. A shift downwards will produce a deeper, more gong-like sound.  " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frqshift", min = -22050, max = 22050, input = "brk", tip = "linear shift in Hz of spectral frequencies (same for all); for downward shifts, use -frqshift, i.e., a negative value (Range: -22050 to 22050)" },
  arg4 = { name = "-l", switch = "-l", tip = "logarithmic interpolation between varying frequency values (Default: linear)." },
}
 
dsp["Strange Shift - 2 Shift the spectrum above frq_divide - Linear frequency shift of (part of) the spectrum"] = {
  cmds = { exe = "strange", mode = "shift 2", tip = "2 Shift the spectrum above frq_divide - The fact that this shift is linear means that it will produce inharmonic results: it will alter harmonic relationships between partials. The linear (i.e., additive) change distorts harmonic relationships, which are based on multiplication. Musical applications; This function can be used to create major alterations in the timbral character of a sound. A shift upwards will emphasise high frequencies in a way which makes a dense, inharmonic sound. A shift downwards will produce a deeper, more gong-like sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frqshift", min = -22050, max = 22050, input = "brk", tip = "linear shift in Hz of spectral frequencies (same for all); for downward shifts, use -frqshift, i.e., a negative value (Range: -22050 to 22050)" },
  arg4 = { name = "frqdivide", min = 0, max = 22050, input = "brk", tip = "frequency at which the shifting starts or stops" },
  arg5 = { name = "-l", switch = "-l", tip = "logarithmic interpolation between varying frequency values (Default: linear)." },
}
 
dsp["Strange Shift - 3 Shift the spectrum below frq_divide - Linear frequency shift of (part of) the spectrum"] = {
  cmds = { exe = "strange", mode = "shift 3", tip = "3 Shift the spectrum below frq_divide - The fact that this shift is linear means that it will produce inharmonic results: it will alter harmonic relationships between partials. The linear (i.e., additive) change distorts harmonic relationships, which are based on multiplication. Musical applications; This function can be used to create major alterations in the timbral character of a sound. A shift upwards will emphasise high frequencies in a way which makes a dense, inharmonic sound. A shift downwards will produce a deeper, more gong-like sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frqshift", min = -22050, max = 22050, input = "brk", tip = "linear shift in Hz of spectral frequencies (same for all); for downward shifts, use -frqshift, i.e., a negative value (Range: -22050 to 22050)" },
  arg4 = { name = "frqdivide", min = 0, max = 22050, input = "brk", tip = "frequency at which the shifting starts or stops" },
  arg5 = { name = "-l", switch = "-l", tip = "logarithmic interpolation between varying frequency values (Default: linear)." },
}
 
dsp["Strange Shift - 4 Shift the spectrum only inside the range frqlo ­ frqhi - Linear frequency shift of (part of) the spectrum"] = {
  cmds = { exe = "strange", mode = "shift 4", tip = "4 Shift the spectrum only inside the range frqlo ­ frqhi - The fact that this shift is linear means that it will produce inharmonic results: it will alter harmonic relationships between partials. The linear (i.e., additive) change distorts harmonic relationships, which are based on multiplication. Musical applications; This function can be used to create major alterations in the timbral character of a sound. A shift upwards will emphasise high frequencies in a way which makes a dense, inharmonic sound. A shift downwards will produce a deeper, more gong-like sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frqshift", min = -22050, max = 22050, input = "brk", tip = "linear shift in Hz of spectral frequencies (same for all); for downward shifts, use -frqshift, i.e., a negative value (Range: -22050 to 22050)" },
  arg4 = { name = "frqlo", min = 0, max = 22050, input = "brk", tip = "define a range inside or outside of which shifting takes place" },
  arg5 = { name = "frqhi", min = 0, max = 22050, input = "brk", tip = "define a range inside or outside of which shifting takes place" },
  arg6 = { name = "-l", switch = "-l", tip = "logarithmic interpolation between varying frequency values (Default: linear)." },
}
 
dsp["Strange Shift - 5 Shift the spectrum only outside the range frqlo ­ frqhi - Linear frequency shift of (part of) the spectrum"] = {
  cmds = { exe = "strange", mode = "shift 5", tip = "5 Shift the spectrum only outside the range frqlo ­ frqhi - The fact that this shift is linear means that it will produce inharmonic results: it will alter harmonic relationships between partials. The linear (i.e., additive) change distorts harmonic relationships, which are based on multiplication. Musical applications; This function can be used to create major alterations in the timbral character of a sound. A shift upwards will emphasise high frequencies in a way which makes a dense, inharmonic sound. A shift downwards will produce a deeper, more gong-like sound. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frqshift", min = -22050, max = 22050, input = "brk", tip = "linear shift in Hz of spectral frequencies (same for all); for downward shifts, use -frqshift, i.e., a negative value (Range: -22050 to 22050)" },
  arg4 = { name = "frqlo", min = 0, max = 22050, input = "brk", tip = "define a range inside or outside of which shifting takes place" },
  arg5 = { name = "frqhi", min = 0, max = 22050, input = "brk", tip = "define a range inside or outside of which shifting takes place" },
  arg6 = { name = "-l", switch = "-l", tip = "logarithmic interpolation between varying frequency values (Default: linear)." },
}
 
dsp["Strange Waver - 1 Standard spectral stretching for inharmonic state - Oscillate between harmonic and inharmonic state "] = {
  cmds = { exe = "strange", mode = "waver 1", tip = "1 Standard spectral stretching for inharmonic state - Formerly SPECIHV, STRANGE WAVER introduces an oscillation towards and away from 'inharmonicness' in the spectrum of a sound. The program moves towards inharmonicness by means of the stretch factor. The rate of oscillation, vibfrq, can be fixed or time-varying, using a time vibfrq breakpoint file. The effect can be very much like a frequency vibrato – it depends on the relationship between vibfrq and stretch. The minimum value for vibfrq is related to the duration of the infile: 1/infile_duration. Thus, durations less than 1 result in a minimum vibfrq which is > 1, and durations greater than 1 result in a minimum vibfrq which is less than 1. Vibfrq will always be greater than 0. The 0.32 minimum value recommended in SoundShaper is for an input file of about 3 sec duration. Longer input files will result in a lower a lower acceptable minimum value. For example, a 10 second file will have a minimum value of 0.1 sec. Whether you would want the effect to operate at this rate is another matter. Vibfrq values greater than 24 start to produce a 'flutter' effect. Higher stretch values will raise the perceived pitch of the output. The stretch factor affects the spectral character of the sound, but does not alter the length – it is not a time-stretch. Take care with the stretch parameter: this function can produce 'wispy' artefacts which higher values will tend to increase. The vibfrq and stretch factors interact in a very interesting way, which is well worth exploring. Musical applications; This progam is a useful way to create vibrato and flutter effects, especially via the vibfrq parameter. However, a very low vibfrq combined with a rather high stretch produces something quite different. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "vibfrq", min = 1.0416666666667e-05, max = 172.265628, input = "brk", tip = "the frequency of oscillation, i.e., number of oscillations per second. (Range: 1/infile_duration to 172.265628)" },
  arg4 = { name = "stretch", min = 1, max = 2205, input = "brk", tip = "the maximum spectral stretch in the inharmonic state (Range: 1 to 2205)" },
  arg5 = { name = "botfrq", min = 0, max = 22050, tip = "the frequency above which spectral stretching happens" },
}
 
dsp["Strange Waver - 2 Specify spectral stretching for inharmonic state - Oscillate between harmonic and inharmonic state "] = {
  cmds = { exe = "strange", mode = "waver 2", tip = "2 Specify spectral stretching for inharmonic state - Formerly SPECIHV, STRANGE WAVER introduces an oscillation towards and away from 'inharmonicness' in the spectrum of a sound. The program moves towards inharmonicness by means of the stretch factor. The rate of oscillation, vibfrq, can be fixed or time-varying, using a time vibfrq breakpoint file. The effect can be very much like a frequency vibrato – it depends on the relationship between vibfrq and stretch. The minimum value for vibfrq is related to the duration of the infile: 1/infile_duration. Thus, durations less than 1 result in a minimum vibfrq which is > 1, and durations greater than 1 result in a minimum vibfrq which is less than 1. Vibfrq will always be greater than 0. The 0.32 minimum value recommended in SoundShaper is for an input file of about 3 sec duration. Longer input files will result in a lower a lower acceptable minimum value. For example, a 10 second file will have a minimum value of 0.1 sec. Whether you would want the effect to operate at this rate is another matter. Vibfrq values greater than 24 start to produce a 'flutter' effect. Higher stretch values will raise the perceived pitch of the output. The stretch factor affects the spectral character of the sound, but does not alter the length – it is not a time-stretch. Take care with the stretch parameter: this function can produce 'wispy' artefacts which higher values will tend to increase. The vibfrq and stretch factors interact in a very interesting way, which is well worth exploring. Musical applications; This progam is a useful way to create vibrato and flutter effects, especially via the vibfrq parameter. However, a very low vibfrq combined with a rather high stretch produces something quite different. " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "vibfrq", min = 1.0416666666667e-05, max = 172.265628, input = "brk", tip = "the frequency of oscillation, i.e., number of oscillations per second. (Range: 1/infile_duration to 172.265628)" },
  arg4 = { name = "stretch", min = 1, max = 2205, input = "brk", tip = "the maximum spectral stretch in the inharmonic state (Range: 1 to 2205)" },
  arg5 = { name = "botfrq", min = 0, max = 22050, tip = "the frequency above which spectral stretching happens" },
  arg6 = { name = "expon", min = 0, max = 10, def = 1, tip = "defines the type of stretch (must be > 0)" },
}
 
------------------------------------------------
-- strans
------------------------------------------------

dsp["Strans Multi - Change the speed or pitch of a stereo sound using a multiplier "] = { 
cmds = { exe = "strans", mode = "multi 1", tip = "Change the pitch (& speed) of a multi-channel input sound (in a possibly time-varying way) by using a speed-multiplier. (doesn't work in windows vista, yet?) Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch. Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.) The graphic program Brkedit (PC Win95 systems) can create exponential or logarithmic breakpoint data, so glissandi in STRANS MULTI can increase or decrease in speed as well as move in a steady (linear) manner. Alternatively, this can be done with Mode 5. Vocal material is very sensitive to pitch changes, so upwards transposition of this type will produce fast, squeaky voices, like the mice trio in Babe, and downwards transposition will produce slow, drawn-out ponderous voices. The vibrato created in Mode 6 is a frequency modulation. Given the very wide ranges allowed, this function is immensely powerful. A slow vibfrq with a large vibdepth will swing the original sound wildly – increase vibrate and it really 'flaps in the breeze' (like a flag in the wind). A fast vibfrq with a reasonably tight vibdepth, e.g., a minor 3rd, will produce a fluttering effect. Altogether, a great program to explore and use to push beyond accepted conventions." }, 
arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
arg3 = { name = "speed", min = 0.003830, max = 256, input = "brk", def = 2, tip = "transposition value (ratio) expressed as a floating point multiplier. " },
}

dsp["Strans Multi 2 - Change the speed or pitch of a stereo sound using a shift in semitones"] = {
cmds = { exe = "strans", mode = "multi 2", tip = "Change the pitch (& speed) of a multi-channel input sound (in a possibly time-varying way) by using a shift in a (possibly fractional) number of semitones. (doesn't work in windows vista, yet?) Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch. Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.) The graphic program Brkedit (PC Win95 systems) can create exponential or logarithmic breakpoint data, so glissandi in STRANS MULTI can increase or decrease in speed as well as move in a steady (linear) manner. Alternatively, this can be done with Mode 5. Vocal material is very sensitive to pitch changes, so upwards transposition of this type will produce fast, squeaky voices, like the mice trio in Babe, and downwards transposition will produce slow, drawn-out ponderous voices. The vibrato created in Mode 6 is a frequency modulation. Given the very wide ranges allowed, this function is immensely powerful. A slow vibfrq with a large vibdepth will swing the original sound wildly – increase vibrate and it really 'flaps in the breeze' (like a flag in the wind). A fast vibfrq with a reasonably tight vibdepth, e.g., a minor 3rd, will produce a fluttering effect. Altogether, a great program to explore and use to push beyond accepted conventions." }, 
arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
arg3 = { name = "semitones", min = -96, max = 96, input = "brk", def = 2, tip = "transposition value in positive or negative number of semitones." },
}
 
dsp["Strans Multi 3 - Accelerate (or decelerate) the speed of a multi-channel source file "] = {
  cmds = { exe = "strans", mode = "multi 3", channels = "any", tip = "Accelerate (or decelerate) the speed of a multi-channel source file. Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch. Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.) The graphic program Brkedit (PC Win95 systems) can create exponential or logarithmic breakpoint data, so glissandi in STRANS MULTI can increase or decrease in speed as well as move in a steady (linear) manner. Alternatively, this can be done with Mode 5. Vocal material is very sensitive to pitch changes, so upwards transposition of this type will produce fast, squeaky voices, like the mice trio in Babe, and downwards transposition will produce slow, drawn-out ponderous voices. The vibrato created in Mode 6 is a frequency modulation. Given the very wide ranges allowed, this function is immensely powerful. A slow vibfrq with a large vibdepth will swing the original sound wildly – increase vibrate and it really 'flaps in the breeze' (like a flag in the wind). A fast vibfrq with a reasonably tight vibdepth, e.g., a minor 3rd, will produce a fluttering effect. Altogether, a great program to explore and use to push beyond accepted conventions." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "accel", min = 0, max = 100, def = 1, tip = "multiplication of speed to be reached by goaltime – i.e., a transposition ratio" },
  arg4 = { name = "goaltime", min = 0.010000, max = length/1000, def = 0.010000, tip = "time in outfile at which the accelerated speed specified by accel is to be reached. " },
  arg5 = { name = "starttime", switch = "-s", min = 0, max = length/1000, def = 0, tip = "time in infile / outfile at which the acceleration begins (Default 0.0)" },
}
 
dsp["Strans Multi 4 - Produce vibrato on a stereo sound "] = {
  cmds = { exe = "strans", mode = "multi 4", channels = "any", tip = "Produce vibrato on a multi-channel sound. Speed modification processes change the duration and the pitch of the sound together. Thus a faster speed causes a higher pitch, a slower speed a lower pitch. Transposition which also changes the speed, and therefore the pitch, of the soundfile greatly alters the character of the sound. It is often very interesting to hear what a sound will be like 1, 2 or even 3 octaves below its original pitch. Deep, rich tones can be achieved in this way. These tones can slowly rise or descend if created with a time-varying breakpoint file e.g., moving an octave up or down over the time of the whole sound (airplane takeoff sounds, etc.) The graphic program Brkedit (PC Win95 systems) can create exponential or logarithmic breakpoint data, so glissandi in STRANS MULTI can increase or decrease in speed as well as move in a steady (linear) manner. Alternatively, this can be done with Mode 5. Vocal material is very sensitive to pitch changes, so upwards transposition of this type will produce fast, squeaky voices, like the mice trio in Babe, and downwards transposition will produce slow, drawn-out ponderous voices. The vibrato created in Mode 6 is a frequency modulation. Given the very wide ranges allowed, this function is immensely powerful. A slow vibfrq with a large vibdepth will swing the original sound wildly – increase vibrate and it really 'flaps in the breeze' (like a flag in the wind). A fast vibfrq with a reasonably tight vibdepth, e.g., a minor 3rd, will produce a fluttering effect. Altogether, a great program to explore and use to push beyond accepted conventions." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "vibfrq", min = 0, max = 120, input = "brk", def = 0, tip = "the vibrato frequency in Hz (cyles-per-second) (Range: 0.0 to 120.0)" },
  arg4 = { name = "vibdepth", min = 0, max = 96, input = "brk", def = 47, tip = "vibrato depth (pitch shift from centre) in [possibly fractional] semitones (Range: 0.0 to 96.0)" },
}
 
------------------------------------------------
-- stretch
------------------------------------------------
 
dsp["Stretch Time - 1 stretch/compress duration of infile without changing the pitch"] = {
  cmds = { exe = "stretch", mode = "time 1", tip = "The key here is to time stretch the sound without altering the pitch. This can be used either to lengthen or compress the duration. Formerly SPECSTR, meaning 'spectrum time-stretching', the internal prolongation of spectral data is itself subject to temporal control according to the amount of stretch specified at different times." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "timestretch", min = 0, max = 10, input = "brk", tip = "Timestretch is used as a multiple." },
}
 
dsp["Stretch Spectrum - 1 Stretch above the frq_divide - stretch/compress frequency components of infile "] = {
  cmds = { exe = "stretch", mode = "spectrum 1", tip = "Stretch above the frq_divide. The STRETCH SPECTRUM process is a means to warp the spectrum of a sound. For example, a harmonic sound may become inharmonic, in a controllable way. (It is derived from CDP's SPECE.) The STRETCH SPECTRUM process gives us control over the way in which we stretch the spectrum of a sound. It should be used for stretching as opposed to shifting the spectrum of a sound. The stretching operation begins at the frq_divide and proceeds upwards. The main thing to appreciate regarding STRETCH SPECTRUM is that the stretch factor is scaled across the frequencies, beginning at 1 at the frq_divide (no stretch) and gradually increasing or decreasing until reaching maxstretch of the uppermost channel in which a frequency is found (not > Nyquist). Overall, it is rather like stretching or compressing a rubber sheet – the whole sheet is warped by the stretching process and the partials are thereby moved. The scaling of maxstretch is linear (even increments) when exponent is an integer. If exponent is < 1.0, the stretch curve will rise quickly and then tend to even out. If exponent is > 1.0, the stretch curve will start to rise slowly and then gradually rise more quickly. The stretching operation expands or contracts the frequency gap between partials. The Technical Discussion offers some supplementary observations about how to think about what this means. Under this operation the frequency ratios between the partials of the original sound are not preserved. Thus, for example, a sound with harmonically related partials, e.g., 100, 200, 300, 400 Hz (common factor, 100) will end up with partials which are not harmonically related, e.g., 110, 213, 316, 419 Hz (no common factor). Such an inharmonic spectrum will normally be perceived as containing several pitches or (with a small stretch) strong spectral colouration. So the stretching operation usually increases inharmonicity and the timbral richness of a sound. Use of maxstretch; Maxstretch specifies the amount of stretch to be applied to the partial found in the uppermost channel. The partial in the channel corresponding to frq_divide does not move (maxstretch = 1). Inbetween components will be moved by an amount corresponding to their relative position. Special Note - components stretched beyond the Nyquist frequency (sr/2) are made to disappear so that the maximum component we will 'hear' will be the one marked 'X' in the above diagram. Hence a maxstretch value > 1 may be something of a fiction in practice because partials stretched beyond Nyquist actually disappear. Musical applications; The image of frequencies on a rubber sheet, referred to above, can help point us in the direction of the possible applications of this process. We can picture the frequencies drawn on the sheet with the lower frequencies at the bottom and the higher frequencies at the top. When we stretch or compress the bottom of the sheet, the frequencies get further apart (i.e., pushing the bottom frequencies lower) or closer together (higher). This will deepen or tighten the tone, as well as make it become more inharmonic at the same time. Similarly, when the top of the sheet is stretched or compressed, the frequencies become further apart (higher: top ones pushed up) or closer together (this will lower and bunch up the high frequencies), with a similar increase in inharmonicity. As ever, the results are greatly affected by the nature of the infile. Experiments with a vocal infile should reveal the way it acts on a sound in an easily perceptible manner. Probably the single most important parameter is frq-divide: where the split point is placed. The quickest way to determine the frequency range of a sound is to use a tuning fork. Trial and error (i.e., guessing) can also be a fairly quick way to get results. Charts of the frequency ranges occupied by various types of sound can also be instructive." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_divide", min = 0, max = 22050, def = 1000, tip = "is the frequency above or below which spectral stretching takes place" },
  arg4 = { name = "maxstretch", min = 0, max = 100, def = 2, tip = "is the transposition ratio of the topmost spectral components" },
  arg5 = { name = "exponent", min = 0.020000, max = 50.000000, def = 3, tip = "specifies the type of stretching required (> 0)" },
  arg6 = { name = "depth", switch = "-d", min = 0, max = 1, input = "brk", def = 0.5, tip = "stretch effect on source (from 0 (no effect) to 1 (full effect)" },
}
 
dsp["Stretch Spectrum - 2 Stretch below the frq_divide - stretch/compress frequency components of infile "] = {
  cmds = { exe = "stretch", mode = "spectrum 2", tip = "Stretch below the frq_divide. The STRETCH SPECTRUM process is a means to warp the spectrum of a sound. For example, a harmonic sound may become inharmonic, in a controllable way. (It is derived from CDP's SPECE.) The STRETCH SPECTRUM process gives us control over the way in which we stretch the spectrum of a sound. It should be used for stretching as opposed to shifting the spectrum of a sound. The stretching operation begins at the frq_divide and proceeds upwards. The main thing to appreciate regarding STRETCH SPECTRUM is that the stretch factor is scaled across the frequencies, beginning at 1 at the frq_divide (no stretch) and gradually increasing or decreasing until reaching maxstretch of the uppermost channel in which a frequency is found (not > Nyquist). Overall, it is rather like stretching or compressing a rubber sheet – the whole sheet is warped by the stretching process and the partials are thereby moved. The scaling of maxstretch is linear (even increments) when exponent is an integer. If exponent is < 1.0, the stretch curve will rise quickly and then tend to even out. If exponent is > 1.0, the stretch curve will start to rise slowly and then gradually rise more quickly. The stretching operation expands or contracts the frequency gap between partials. The Technical Discussion offers some supplementary observations about how to think about what this means. Under this operation the frequency ratios between the partials of the original sound are not preserved. Thus, for example, a sound with harmonically related partials, e.g., 100, 200, 300, 400 Hz (common factor, 100) will end up with partials which are not harmonically related, e.g., 110, 213, 316, 419 Hz (no common factor). Such an inharmonic spectrum will normally be perceived as containing several pitches or (with a small stretch) strong spectral colouration. So the stretching operation usually increases inharmonicity and the timbral richness of a sound. Use of maxstretch; Maxstretch specifies the amount of stretch to be applied to the partial found in the uppermost channel. The partial in the channel corresponding to frq_divide does not move (maxstretch = 1). Inbetween components will be moved by an amount corresponding to their relative position. Special Note - components stretched beyond the Nyquist frequency (sr/2) are made to disappear so that the maximum component we will 'hear' will be the one marked 'X' in the above diagram. Hence a maxstretch value > 1 may be something of a fiction in practice because partials stretched beyond Nyquist actually disappear. Musical applications; The image of frequencies on a rubber sheet, referred to above, can help point us in the direction of the possible applications of this process. We can picture the frequencies drawn on the sheet with the lower frequencies at the bottom and the higher frequencies at the top. When we stretch or compress the bottom of the sheet, the frequencies get further apart (i.e., pushing the bottom frequencies lower) or closer together (higher). This will deepen or tighten the tone, as well as make it become more inharmonic at the same time. Similarly, when the top of the sheet is stretched or compressed, the frequencies become further apart (higher: top ones pushed up) or closer together (this will lower and bunch up the high frequencies), with a similar increase in inharmonicity. As ever, the results are greatly affected by the nature of the infile. Experiments with a vocal infile should reveal the way it acts on a sound in an easily perceptible manner. Probably the single most important parameter is frq-divide: where the split point is placed. The quickest way to determine the frequency range of a sound is to use a tuning fork. Trial and error (i.e., guessing) can also be a fairly quick way to get results. Charts of the frequency ranges occupied by various types of sound can also be instructive." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "frq_divide", min = 0, max = 22050, def = 1000, tip = "is the frequency above or below which spectral stretching takes place" },
  arg4 = { name = "maxstretch", min = 0, max = 100, def = 2, tip = "is the transposition ratio of the topmost spectral components" },
  arg5 = { name = "exponent", min = 0.020000, max = 50, def = 3, tip = "specifies the type of stretching required (> 0)" },
  arg6 = { name = "depth", switch = "-d", min = 0, max = 1, input = "brk", def = 0.5, tip = "stretch effect on source (from 0 (no effect) to 1 (full effect)" },
}

dsp["Stretch Time - 2 output length of stretch in terminal"] = {
  cmds = { exe = "stretch", mode = "time 2", input = "pvoc", tip = "In Mode 2, the program calculates the length of the output only, and does not produce an outfile. The key here is to time stretch the sound without altering the pitch. This can be used either to lengthen or compress the duration. Formerly SPECSTR, meaning 'spectrum time-stretching', the internal prolongation of spectral data is itself subject to temporal control according to the amount of stretch specified at different times. The fact that the time can be set in the breakpoint file provides an important tool. For example, it can be used to retain the ability to recognise the source of the sound. We often can identify a sound according to the way it begins, its 'attack transient'. This can be kept unchanged by giving a timestretch of 1 for this part of the sound. Then the stretching can begin, so we retain the recognisability of the sound while stretching it in time. Retaining recognisability is often a very important compositional issue." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "timestretch", min = 0, max = 100, input = "brk", def = 2, tip = " single value or the name of a time timestretch breakpoint file of values. Timestretch is used as a multiple." },
}

------------------------------------------------
-- stretcha
------------------------------------------------

--[[
dsp["[Doesn't work - possibble audio input, terminal output] Stretcha - 1 (three input options ⇒ a specified tempo). Utility to Calculate timestretch factor relating to beats and tempo for use with STRETCH TIME "] = { 
  cmds = { exe = "stretcha", mode = "-c", tip = "Mode 1: (three input options ⇒ a specified tempo) takes as inputs: a duration OR a soundfile OR (a number of (possibly fractional) beats and a tempo) and also an output tempo. STRETCHA is a utility program for use with STRETCH TIME. I (AE) asked Trevor to write this program several years ago in order to help create passages of music using regular beat patterns. The source code has been lost for all these years, but an enquiry from Arthur Green, who found the program in his directory of CDP programs and discovered that it would not work on a 64-bit PC, led Richard Dobson to search out the source and recompile it for us. It takes a bit of calculating to work out how to make a soundfile fit specific numbers of possibly fractional beats. STRETCHA does the work for you. When a soundfile is named as in input, its duration is automatically determined. It can also handle durations and beats as inputs or as outputs to add extra flexibility. For the time being you will have to run this program on the command line because it is not yet included in either of the GUIs. We will add it to Release 7. Note that, if you do not have the environment variable CDP_SOUND_EXT SET, you will have to include the .wav extension with the soundfile name. The commandline environment also enables you to write the output to a file rather than just see it displayed on the screen. This is done with the redirection symbol > as in this example command line: stretcha -s -fbdt.wav -D8 >strfac.txt. The stretch factor is then used with the CDP program STRETCH TIME. While most of the options are fairly straightforward, note that some possibilities are not covered directly but require two steps. The three -c options of Mode 1, for example, which convert to a specified tempo, display only 'number of beats at new tempo' – it does not actually give a stretchfactor. It gives the number of beats that will result. You can then use this information to fill in the 'beats' parameter value in Mode 2 options to get the stretchfactor to convert the input beats and tempo to a specified duration (option 5) OR tempo (option 6). To illustrate this two step process, if you input a duration of 4 seconds and an outtempo of MM = 90: Mode 1 option 1 (stretcha -c -d4 -T90), the program tells you that 6 crotchets will result. Now you can use that information to calculate the required stretchfactor to fill a specified duration: Mode 2, option 5 (stretcha -s -b6 -t90 -D10), yielding a stretch factor of 2.5. Thus 4 seconds changes to 6 crotchets at MM = 90, and this will fill 10 seconds with a stretch factor of 2.5. You have thus gone from duration to duration while specifying a specific number of beats at a given tempo. What you need to do depends on the nature of the material with which you start. It may also be useful to know the actual duration that results from a given number of beats at a given tempo. For example, if you want 4 beats at MM = 72 to be changed to 7.5 beats at MM = 108, you will have to know what duration the later produces (to use Mode 2 option 5). The program doesn't provide this facility, but it is easily calculated as numbeats * 60.0/tempo, which in this case is 7.5 * (60.0/108), giving 4.1666 seconds. Then you can use: stretcha -s -b4 -t72 -D4.1666, giving a stretch factor of 1.249980. You can also calculate the duration of the input portion of beats at a tempo and then use the input and output duration option (Mode 2 option 1). Thus 4 * (60.0/72) = 3.3333: stretcha -s -d3.3333 -D4.1666 gives a stretch factor of 1.249980." },
  arg1 = { name = "dur", switch = "-d", min = 0, max = 60, def = 1, def = 0, tip = "an input length of time in seconds" },
  arg2 = { name = "sndfile", switch = "-f", input = "wav", tip = "an input soundfile" },
  arg3 = { name = "beats", switch = "-b", min = 0, max = 1000, def = 15, tip = "an input number of (possibly fractional) beats" },
  arg4 = { name = "tempo", switch = "-t", min = 0, max = 1000, def = 100, tip = "an input tempo at which the beats occur" },
  arg5 = { name = "outtempo", switch = "-T", min = 0, max = 1000, def = 166, tip = "an output tempo "},
}
--]]

--[[
dsp["[Doesn't work - possibble audio input, terminal output] Stretcha - 2 (three input options ⇒ to a specified duration OR to a number of beats at a given tempo). Utility to Calculate timestretch factor relating to beats and tempo for use with STRETCH TIME "] = { 
  cmds = { exe = "stretcha", mode = "-s", tip = "Mode 2:  (three input options ⇒ to a specified duration OR to a number of beats at a given tempo) takes as inputs: a duration OR a soundfile OR a number of (possibly fractional) beats and a tempo and also an output duration OR a number of (possibly fractional) output beats and an output tempo . STRETCHA is a utility program for use with STRETCH TIME. I (AE) asked Trevor to write this program several years ago in order to help create passages of music using regular beat patterns. The source code has been lost for all these years, but an enquiry from Arthur Green, who found the program in his directory of CDP programs and discovered that it would not work on a 64-bit PC, led Richard Dobson to search out the source and recompile it for us. It takes a bit of calculating to work out how to make a soundfile fit specific numbers of possibly fractional beats. STRETCHA does the work for you. When a soundfile is named as in input, its duration is automatically determined. It can also handle durations and beats as inputs or as outputs to add extra flexibility. For the time being you will have to run this program on the command line because it is not yet included in either of the GUIs. We will add it to Release 7. Note that, if you do not have the environment variable CDP_SOUND_EXT SET, you will have to include the .wav extension with the soundfile name. The commandline environment also enables you to write the output to a file rather than just see it displayed on the screen. This is done with the redirection symbol > as in this example command line: stretcha -s -fbdt.wav -D8 >strfac.txt. The stretch factor is then used with the CDP program STRETCH TIME. While most of the options are fairly straightforward, note that some possibilities are not covered directly but require two steps. The three -c options of Mode 1, for example, which convert to a specified tempo, display only 'number of beats at new tempo' – it does not actually give a stretchfactor. It gives the number of beats that will result. You can then use this information to fill in the 'beats' parameter value in Mode 2 options to get the stretchfactor to convert the input beats and tempo to a specified duration (option 5) OR tempo (option 6). To illustrate this two step process, if you input a duration of 4 seconds and an outtempo of MM = 90: Mode 1 option 1 (stretcha -c -d4 -T90), the program tells you that 6 crotchets will result. Now you can use that information to calculate the required stretchfactor to fill a specified duration: Mode 2, option 5 (stretcha -s -b6 -t90 -D10), yielding a stretch factor of 2.5. Thus 4 seconds changes to 6 crotchets at MM = 90, and this will fill 10 seconds with a stretch factor of 2.5. You have thus gone from duration to duration while specifying a specific number of beats at a given tempo. What you need to do depends on the nature of the material with which you start. It may also be useful to know the actual duration that results from a given number of beats at a given tempo. For example, if you want 4 beats at MM = 72 to be changed to 7.5 beats at MM = 108, you will have to know what duration the later produces (to use Mode 2 option 5). The program doesn't provide this facility, but it is easily calculated as numbeats * 60.0/tempo, which in this case is 7.5 * (60.0/108), giving 4.1666 seconds. Then you can use: stretcha -s -b4 -t72 -D4.1666, giving a stretch factor of 1.249980. You can also calculate the duration of the input portion of beats at a tempo and then use the input and output duration option (Mode 2 option 1). Thus 4 * (60.0/72) = 3.3333: stretcha -s -d3.3333 -D4.1666 gives a stretch factor of 1.249980." },
  arg1 = { name = "dur", switch = "-d", min = 0, max = 60, def = 1, def = "0", tip = "an input length of time in seconds" },
  arg2 = { name = "sndfile", switch = "-f", tip = "an input soundfile" },
  arg3 = { name = "beats", switch = "-b", min = 0, max = 1000, def = 15, tip = "an input number of (possibly fractional) beats" },
  arg4 = { name = "tempo", switch = "-t", min = 0, max = 1000, def = 100, tip = "an input tempo at which the beats occur" },
  arg5 = { name = "outdur", switch = "-D", min = 0, max = 60, def = 10, tip = "an output length of time in seconds"},
  arg6 = { name = "outbeats", switch = "-B", min = 0, max = 1000, def = 166, tip = "an output number of (possibly fractional) beats"},
  arg7 = { name = "outtempo", switch = "-T", min = 0, max = 1000, def = 166, tip = "an output tempo "},
}
--]]

--[[
dsp["[Doesn't work - possibble audio input, terminal output] Stretcha - 3 (input tempo ⇒ output tempo). Utility to Calculate timestretch factor relating to beats and tempo for use with STRETCH TIME "] = { 
  cmds = { exe = "stretcha", mode = "-s", tip = "Mode 3: (input tempo ⇒ output tempo) takes as inputs: an input tempo and an output tempo; . STRETCHA is a utility program for use with STRETCH TIME. I (AE) asked Trevor to write this program several years ago in order to help create passages of music using regular beat patterns. The source code has been lost for all these years, but an enquiry from Arthur Green, who found the program in his directory of CDP programs and discovered that it would not work on a 64-bit PC, led Richard Dobson to search out the source and recompile it for us. It takes a bit of calculating to work out how to make a soundfile fit specific numbers of possibly fractional beats. STRETCHA does the work for you. When a soundfile is named as in input, its duration is automatically determined. It can also handle durations and beats as inputs or as outputs to add extra flexibility. For the time being you will have to run this program on the command line because it is not yet included in either of the GUIs. We will add it to Release 7. Note that, if you do not have the environment variable CDP_SOUND_EXT SET, you will have to include the .wav extension with the soundfile name. The commandline environment also enables you to write the output to a file rather than just see it displayed on the screen. This is done with the redirection symbol > as in this example command line: stretcha -s -fbdt.wav -D8 >strfac.txt. The stretch factor is then used with the CDP program STRETCH TIME. While most of the options are fairly straightforward, note that some possibilities are not covered directly but require two steps. The three -c options of Mode 1, for example, which convert to a specified tempo, display only 'number of beats at new tempo' – it does not actually give a stretchfactor. It gives the number of beats that will result. You can then use this information to fill in the 'beats' parameter value in Mode 2 options to get the stretchfactor to convert the input beats and tempo to a specified duration (option 5) OR tempo (option 6). To illustrate this two step process, if you input a duration of 4 seconds and an outtempo of MM = 90: Mode 1 option 1 (stretcha -c -d4 -T90), the program tells you that 6 crotchets will result. Now you can use that information to calculate the required stretchfactor to fill a specified duration: Mode 2, option 5 (stretcha -s -b6 -t90 -D10), yielding a stretch factor of 2.5. Thus 4 seconds changes to 6 crotchets at MM = 90, and this will fill 10 seconds with a stretch factor of 2.5. You have thus gone from duration to duration while specifying a specific number of beats at a given tempo. What you need to do depends on the nature of the material with which you start. It may also be useful to know the actual duration that results from a given number of beats at a given tempo. For example, if you want 4 beats at MM = 72 to be changed to 7.5 beats at MM = 108, you will have to know what duration the later produces (to use Mode 2 option 5). The program doesn't provide this facility, but it is easily calculated as numbeats * 60.0/tempo, which in this case is 7.5 * (60.0/108), giving 4.1666 seconds. Then you can use: stretcha -s -b4 -t72 -D4.1666, giving a stretch factor of 1.249980. You can also calculate the duration of the input portion of beats at a tempo and then use the input and output duration option (Mode 2 option 1). Thus 4 * (60.0/72) = 3.3333: stretcha -s -d3.3333 -D4.1666 gives a stretch factor of 1.249980." },
  arg1 = { name = "intempo", switch = "-t", min = 0, max = 1000, def = 98, def = "generic", tip = "an input tempo" },
  arg2 = { name = "outtempo", switch = "-T", min = 0, max = 1000, def = 166, tip = "an output tempo "},
}
--]]
 
------------------------------------------------
-- submix
------------------------------------------------

dsp["Addtomix - Add soundfiles (at maximum level and time zero) to an existing mixfile"] = {
  cmds = { exe = "submix", mode = "addtomix", tip = "Further sounds can be added to a mixfile simply by listing the original mixfile and the new sounds as inputs to the process. A new mixfile is created with the new sounds added at the foot of the file but all at start-time zero, and with maximum loudness. This new file can then be edited to re-position the start times of the new sounds in the new mix, and at the level you require. The standard extension for a CDP mixfile is .mix, but is not required. Sound Loom uses .txt for all its text files. Musical applications; This is a simple utility designed to facilitate expanding the contents of a mixfile." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile to which to add sounds" },
  arg2 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg3 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg4 = { name = "outmixfile", output = "txt", tip = "resultant output mixfile" },
}

dsp["Atstep - Convert a list of soundfiles to a mixfile (fixed time-step)"] = {
  cmds = { exe = "submix", mode = "atstep", tip = "Similar to the SUBMIX ADDTOMIX, but here you specify a fixed time-interval between the entry of each sound in the mix. Musical applications; SUBMIX ATSTEP is useful if you want to use mixfiles for generating strictly rhythmic sequences of sounds. (You can 'naturalise' the rhythmic feel of the output by using existing CDP processes to slightly randomise the entry times of the sounds). See SUBMIX TIMEWARP." },
  arg1 = { name = "Infile 1", input = "txt", tip = "first soundfile in list" },
  arg2 = { name = "Infile 2", input = "txt", tip = "second soundfile in list" },
  arg3 = { name = "Infile 3", input = "txt", tip = "third soundfile in list" },
  arg4 = { name = "outmixfile", output = "txt", tip = "output mixfile produced by the program" },
  arg5 = { name = "step", min = 0, max = 100, def = 0.1, tip = "time, in seconds, between the start times of each sound" },
}

dsp["Attenuate - Alter the overall level of a mixfile"] = {
  cmds = { exe = "submix", mode = "attenuate", tip = "The gainval parameter is a multiplier and is applied to the levels of all the sounds listed in the mixfile. The levels in the mixfile may be notated in dB (e.g., 0dB) or as floating point values (e.g., 1.0). Values > 1 will increase the level. The gain may be applied to selected sounds in the mixfile by specifying 'start' and 'end' lines in the mixfile. Only one line will be affected if 'start' and 'end' are given the same line number. (The line count starts at 1.) Musical Applications; CDP mixfiles may list a large number of sounds – no limit is encoded in the software. It may sometimes, therefore, be useful to adjust the attenuation of whole groups of sounds in the mixfile. This can be done easily with SUBMIX ATTENUATE." },
  arg1 = { name = "inmixfile", input = "txt", tip = "the input is a mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "the output is the adjusted mixfile" },
  arg3 = { name = "gainval", min = 0, max = 100, def = 1, tip = "gain factor to be applied (Must be > 0.0)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 1000, def = 1, tip = "line at which attenuation begins (Default: 1st line in mixfile" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 1000, def = 1000, tip = "line at which attenuation begins (Default: last line in mixfile)" },
}
 
dsp["Balance - Mix between 2 soundfiles, using a balance function)"] = {
  cmds = { exe = "submix", mode = "balance", tip = " This is a 'quick mix' function like SUBMIX MERGE except that one can easily vary the loudness of the sounds relative to one another by means of the balance parameter. The 'inverse of the balance' means that the second soundfile will become as much softer as the balance multiplier makes the first soundfile louder, and v.vs.. The first file is multiplied by balance, and the second file by 1-balance. For example, if two soundfiles are mixed without any reduction of their levels, their amplitudes will be summed (and probably go over). With SUBMIX BALANCE, their amplitudes relative to one another are adjusted such that they never exceed unity (maximum amplitude). If balance is 0.7 and the amplitude of both files is 10000, the first file will become 10000 * 0.7 = 7000, and the second file will become 10000 * (1-0.7, i.e., 0.3) = 3000. If balance is 0.3 and the amplitude of both files is 10000, the first file will become 10000 * 0.3 = 3000, and the second file will become 10000 * (1-0.3, i.e., 0.7) = 7000. In other words, as the level of sndfile1 goes up, the level of sndfile2 goes down, etc., but the some of both of them is always 1, so they don't overload: the total amplitude remains the same. If either file is less than maximum amplitude, the output will also be less than maximum amplitude. The output will always remain within an acceptable range. Musical applications; For very simple mixing jobs, where two input files need to be mixed, BALANCE offers another easier alternative to MIX. The files may be of any length, and the restriction on having similar sampling rates and numbers of channels is removed.You are assured that overload is avoided by adjusting the relative volumes of the two soundfiles, matching them equally at balance = 0.5, making the first soundfile softer and the second louder when balance is < 0.5, and making the first soundfile louder and the second softer when balance is > 0.5. The relative volumes of the two sounds in the output, however, also depends on their respective levels in the first place. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "balance", switch = "-k", min = 0, max = 1, def = 0.5, input = "brk", tip = "describes the relative level of the two sounds (Range: 0 to 1)" },
  arg5 = { name = "start", switch = "-b", min = 0, max = length/1000, def = 0, tip = "start the mix at the time specified" },
  arg6 = { name = "end", switch = "-e", min = 0, max = length/1000, def = length/1000, tip = "stop the mix at the time specified " },
}
 
dsp["Crossfade 1 - Quick linear crossfade between 2 soundfiles (with same number of channels) "] = {
  cmds = { exe = "submix", mode = "crossfade 1", tip = "1 Linear crossfade.  SUBMIX CROSSFADE is a mixing process applied directly to soundfiles. It does not entail the use of a mixfile. It mixes the two sounds, making a gradual transition from the first to the second. The stagger parameter operates in steps of 100ths of a second. If a more precise gap is required, the author suggests splicing a silence of a specific duration onto the beginning of the second soundfile, and then using a stagger of 0. Alternatively, SUBMIX MIX can be used, in which the start time of the second soundfile can be precisely defined. Musical Applications; Note that only spectral morphing will create a true morph between two sounds. The CROSSFADE transition can, however, be useful to prepare sounds for morphing, as well as to create transitions in which the smooth merging of spectra is not the key issue. Transition is one of the key principles of effective musical composition, as it creates a movement over time. In this case, the movement is between sounds, a process well known in the orchestration of instrumental music, but one uniquely suited to further development in an electroacoustic composing environment. The types of sounds and nature of the transition are determined by the compositional logic. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", switch = "-s", min = 0, max = length/1000, def = 0, tip = "2nd file starts stagger seconds after 1st (Default: 0)" },
  arg5 = { name = "begin", switch = "-b", min = 0, max = length/1000, def = 0, tip = "crossfade starts at begin seconds; must be > stagger (Default: 0)" },
  arg6 = { name = "end", switch = "-e", min = 0, max = length/1000, def = length/1000, tip = "crossfade ends at end seconds; must be > begin (Default: end of shortest file)" },
}
 
dsp["Crossfade 2 - Cosinusoidal crossfade between 2 soundfiles (with same number of channels) "] = {
  cmds = { exe = "submix", mode = "crossfade 2", tip = "2 Cosinusoidal crossfade. SUBMIX CROSSFADE is a mixing process applied directly to soundfiles. It does not entail the use of a mixfile. It mixes the two sounds, making a gradual transition from the first to the second. The stagger parameter operates in steps of 100ths of a second. If a more precise gap is required, the author suggests splicing a silence of a specific duration onto the beginning of the second soundfile, and then using a stagger of 0. Alternatively, SUBMIX MIX can be used, in which the start time of the second soundfile can be precisely defined. Musical Applications; Note that only spectral morphing will create a true morph between two sounds. The CROSSFADE transition can, however, be useful to prepare sounds for morphing, as well as to create transitions in which the smooth merging of spectra is not the key issue. Transition is one of the key principles of effective musical composition, as it creates a movement over time. In this case, the movement is between sounds, a process well known in the orchestration of instrumental music, but one uniquely suited to further development in an electroacoustic composing environment. The types of sounds and nature of the transition are determined by the compositional logic. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", switch = "-s", min = 0, max = length/1000, def = 0, tip = "2nd file starts stagger seconds after 1st (Default: 0)" },
  arg5 = { name = "begin", switch = "-b", min = 0, max = length/1000, def = 0, tip = "crossfade starts at begin seconds; must be > stagger (Default: 0)" },
  arg6 = { name = "end", switch = "-e", min = 0, max = length/1000, def = length/1000, tip = "crossfade ends at end seconds; must be > begin (Default: end of shortest file)" },
  arg7 = { name = "powfac", switch = "-p", min = 0.13, max = 8, def = 1, tip = "Crossfade skew" },
}

dsp["Dummy - 1 all files start at time zero. Convert list of soundfiles into a basic mixfile."] = {
  cmds = { exe = "submix", mode = "dummy 1", tip = "1 - all files start at time zero. The list of soundfile names should not include the extension if the CDP_SOUND_EXT environment variable has been set – it will normally be set! The output is a mixfile with default settings, which can then be edited to suit. SUBMIX DUMMY then turns this list of sounds into a prototype mixfile. It is not as much of a 'dummy' as it makes out: it interrogates the headers of the soundfiles to find out the number of channels and check the sample rates (they all need to be the same in a mixfile). Musical Applications; This is a simple utility which may speed up the process of creating a mixfile. The graphic user interfaces have their own way of doing this." },
  arg1 = { name = "infile 1", input = "txt", tip = "list of soundfile names" },
  arg2 = { name = "infile 2", input = "txt", tip = "list of soundfile names" },
  arg3 = { name = "infile 3", input = "txt", tip = "list of soundfile names" },
  arg4 = { name = "mixfile", output = "txt", tip = "output mixfile" },
}

dsp["Dummy - 2 each file starts where previous file ends. Convert list of soundfiles into a basic mixfile."] = {
  cmds = { exe = "submix", mode = "dummy 2", tip = "2 - each file starts where previous file ends. The list of soundfile names should not include the extension if the CDP_SOUND_EXT environment variable has been set – it will normally be set! The output is a mixfile with default settings, which can then be edited to suit. SUBMIX DUMMY then turns this list of sounds into a prototype mixfile. It is not as much of a 'dummy' as it makes out: it interrogates the headers of the soundfiles to find out the number of channels and check the sample rates (they all need to be the same in a mixfile). Musical Applications; This is a simple utility which may speed up the process of creating a mixfile. The graphic user interfaces have their own way of doing this." },
  arg1 = { name = "infile 1", input = "txt", tip = "list of soundfile names" },
  arg2 = { name = "infile 2", input = "txt", tip = "list of soundfile names" },
  arg3 = { name = "infile 3", input = "txt", tip = "list of soundfile names" },
  arg4 = { name = "mixfile", output = "txt", tip = "output mixfile" },
}

dsp["Faders - Mix several mono or stereo files using a time-changing balance function."] = {
  cmds = { exe = "submix", mode = "faders", channels = "any", tip = "This is the equivalent of mixing several sounds using movable faders, or with imposed envelopes on each sound. One file specifies the relative balance of each input file, from moment to moment. A second value or breakpoint datafile specifies the overall level of the mix, which may be time-varying. Musical applications; This is a remarkably straightforward way to achieve time-varying relative level changes in a mix. Previously, each soundfile in the mix would have to be enveloped separately. It is therefore a controlled way to have level 'automation' in a mix. Also, the mixfile stage is by-passed, going directly to a soundfile output." },
  arg1 = { name = "insndfile1", input = "wav", tip = "first input soundfile" },
  arg2 = { name = "insndfile2", input = "wav", tip = "second input soundfile" },
  arg3 = { name = "outsndfile", output = "wav", tip = "list of soundfile names" },
  arg4 = { name = "balance-data", input = "txt", tip = "text file containing 'value sets' to specify levels. Each value set consists of a time followed by the relative-level of each soundfile in the mix at that time." },
  arg5 = { name = "envelope-data", min = 0, max = 100, def = 1, input = "brk", tip = "the loudness envelope to apply to the overall sound." },
}

dsp["Fileformat - Display format of a mixfile."] = {
  cmds = { exe = "submix", mode = "fileformat", tip = "This function displays for your information the required format options for a mixfile. Musical Applications; The mixfile is submitted as data to SUBMIX MIX." },
}

dsp["Get level - 1 Find the maximum level in the mix. Test maximum level of a mix, defined in a mixfile"] = {
  cmds = { exe = "submix", mode = "getlevel 1", tip = "1 Find the maximum level in the mix. The mix process sums amplitudes, and this can sometimes cause levels to go above the maximum amplitude handled by the system. If these go over the maximum permissible level, they will be 'clipped', producing audible clicks in the output soundfile. SUBMIX GETLEVEL checks if this will happen by performing a pre-run through the mix operation prescribed in the mixfile, calculating what will happen with the amplitudes. Besides actually finding the maximum level of a mix, SUBMIX GETLEVEL also recommends a gain factor which should eliminate the clipping. It can also report on the locations where the clipping occurred, enabling you to tell at just what point(s) in the mixfile there was a problem. The start end parameters enable you to focus on particular time sections of the mix. A useful trick to maintain high signal levels without clipping is to try inverting the phase of one or other of your source files. To do this, use a gain of -1 in MODIFY LOUDNESS. Musical Applications; This function is a utility which can save you some time. It also helps you to pinpoint where the problem(s) occur and advises you on what attenuation is needed to remedy the problem. Furthermore, it avoids creating a possibly very large output soundfile unnecessarily. " },
  arg1 = { name = "mixfile", input = "txt", tip = "mixfile submitted to SUBMIX GETLEVEL for level test" },
  arg2 = { name = "start", switch = "-s", min = 0, max = 100, def = 1, tip = "start position in seconds at which to begin testing the output level of the mix" },
  arg3 = { name = "end", switch = "-e", min = 0, max = 100, def = 100, tip = "end position in seconds at which to finish testing the output level of the mix" },
}

dsp["Get level - 2 Find the time-locations where clipping occurs in the mix."] = {
  cmds = { exe = "submix", mode = "getlevel 2", tip = "2 Find the time-locations where clipping occurs in the mix. The mix process sums amplitudes, and this can sometimes cause levels to go above the maximum amplitude handled by the system. If these go over the maximum permissible level, they will be 'clipped', producing audible clicks in the output soundfile. SUBMIX GETLEVEL checks if this will happen by performing a pre-run through the mix operation prescribed in the mixfile, calculating what will happen with the amplitudes. Besides actually finding the maximum level of a mix, SUBMIX GETLEVEL also recommends a gain factor which should eliminate the clipping. It can also report on the locations where the clipping occurred, enabling you to tell at just what point(s) in the mixfile there was a problem. The start end parameters enable you to focus on particular time sections of the mix. A useful trick to maintain high signal levels without clipping is to try inverting the phase of one or other of your source files. To do this, use a gain of -1 in MODIFY LOUDNESS. Musical Applications; This function is a utility which can save you some time. It also helps you to pinpoint where the problem(s) occur and advises you on what attenuation is needed to remedy the problem. Furthermore, it avoids creating a possibly very large output soundfile unnecessarily. " },
  arg1 = { name = "mixfile", input = "txt", tip = "mixfile submitted to SUBMIX GETLEVEL for level test" },
  arg2 = { name = "outtextfile", output = "txt", tip = "report is written to a text file" },
  arg3 = { name = "start", switch = "-s", min = 0, max = 100, def = 1, tip = "start position in seconds at which to begin testing the output level of the mix" },
  arg4 = { name = "end", switch = "-e", min = 0, max = 100, def = 100, tip = "end position in seconds at which to finish testing the output level of the mix" },
}

dsp["Get level - 3 Find both the maximum level in the mix and the time-locations where clipping occurs."] = {
  cmds = { exe = "submix", mode = "getlevel 3", tip = "3 Find both the maximum level in the mix and the time-locations where clipping occurs. The mix process sums amplitudes, and this can sometimes cause levels to go above the maximum amplitude handled by the system. If these go over the maximum permissible level, they will be 'clipped', producing audible clicks in the output soundfile. SUBMIX GETLEVEL checks if this will happen by performing a pre-run through the mix operation prescribed in the mixfile, calculating what will happen with the amplitudes. Besides actually finding the maximum level of a mix, SUBMIX GETLEVEL also recommends a gain factor which should eliminate the clipping. It can also report on the locations where the clipping occurred, enabling you to tell at just what point(s) in the mixfile there was a problem. The start end parameters enable you to focus on particular time sections of the mix. A useful trick to maintain high signal levels without clipping is to try inverting the phase of one or other of your source files. To do this, use a gain of -1 in MODIFY LOUDNESS. Musical Applications; This function is a utility which can save you some time. It also helps you to pinpoint where the problem(s) occur and advises you on what attenuation is needed to remedy the problem. Furthermore, it avoids creating a possibly very large output soundfile unnecessarily. " },
  arg1 = { name = "mixfile", input = "txt", tip = "mixfile submitted to SUBMIX GETLEVEL for level test" },
  arg2 = { name = "outtextfile", output = "txt", tip = "report is written to a text file" },
  arg3 = { name = "start", switch = "-s", min = 0, max = 100, def = 1, tip = "start position in seconds at which to begin testing the output level of the mix" },
  arg4 = { name = "end", switch = "-e", min = 0, max = 100, def = 100, tip = "end position in seconds at which to finish testing the output level of the mix" },
}

dsp["Interleave (mono)files to make a stereo outfile  "] = {
  cmds = { exe = "submix", mode = "interleave", channels = "1in2out", tip = " INTERLEAVE produces multi-channel files by interleaving the specified (mono) files. It will not accept input files with more than one channel (originally, in Renoise a stereo input will be converted to mono beforehand!). Note that all the input files should have the same sample rate. Musical applications; The usual application is to create a stereo file from two mono files. See also HOUSEKEEP CHANS mode 4 (convert stereo to mono). MODIFY SPACE mode 1 (pan) will also create a stereo output from mono inputs. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
}
 
dsp["Merge - Quick mix of 2 soundfiles (with the same number of channels) "] = {
  cmds = { exe = "submix", mode = "merge", tip = "Simple mixing of two soundfiles with facilities to have the second soundfile enter later (stagger), start some time after its beginning (skip), or have less gain than the first soundfile (skew). Musical applications; For very simple mixing jobs, where two input files need to be mixed, MERGE offers an easier alternative to MIX. The files may be of any length, but are expected to have similar sampling rates and numbers of channels. If the skip argument is given, the stagger argument must also be given, although it can be zero. The files are by default mixed at equal levels with each file scaled to half its original level. For this reason overflows will not take place when using MERGE with soundfiles of sample type SHORTS. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "stagger", switch = "-s", min = 0, max = length/1000, def = 0, tip = "sndfile2 enters stagger seconds after (Default: 0)" },
  arg5 = { name = "skip", switch = "-j", min = 0, max = length/1000, def = 0, tip = "move skip seconds into sndfile2 before starting to mix (Default: 0)" },
  arg6 = { name = "skew", switch = "-k", min = 0, max = 100, def = 1, tip = "sndfile1 has skew times more gain than sndfile2 (Default 1: same level)" },
  arg7 = { name = "start", switch = "-b", min = 0, max = length/1000, def = 0, tip = "start the mix at the time specified" },
  arg8 = { name = "end", switch = "-e", min = 0, max = length/1000, def = length/1000, tip = "stop the mix at the time specified " },
}

dsp["Mergemany - Quick mix of several soundfiles (with the same number of channels)"] = {
  cmds = { exe = "submix", mode = "mergemany", channels = "any", tip = "Makes a quick mix of several sounds (all having the same number of channels) with all sounds at a standard loudness level. SUBMIX MERGE already exists to mix two sounds in this way. SUBMIX MERGE avoids calculating the peak sample (in order to avoid clipping) by simply reducing each source sound to half-level before mixing. SUBMIX MERGEMANY, however, calculates the maximum output sample, in a first pass, and then adjusts the standard level of each input file to avoid any clipping of the output. It is best, therefore, to use SUBMIX MERGEMANY for quick mixes of several sounds, if you want use the output (rather than just hear it). Musical applications; This quick mix of several soundfiles with a normalising function can be used to test combinations of sounds, or to mix when all the sounds are at starttime 0. A mixfile is not produced, so no further adjustments are possible.." },
  arg1 = { name = "insndfile1", input = "wav", tip = "first input soundfile" },
  arg2 = { name = "insndfile2", input = "wav", tip = "second input soundfile (number of channels must be consistent)" },
  arg3 = { name = "outsndfile", output = "wav", tip = "Select the output sound to the process" },
}

dsp["Mix - Mix sounds as instructed in a mixfile"] = {
  cmds = { exe = "submix", mode = "mix", tip = "MIX is used to assemble musical passages by mixing several soundfiles into one new soundfile, arranging the way they overlap by specifying the start time of each soundfile. The amplitude of overlapping soundfiles is summed, so the provision for altering the amplitude in dB enables the user to avoid exceeding the maximum (32767) which would cause distorton. The mixfile contains one line of text for each file in the mix. Note that the level value is used as a simple multiplier by the MIX program. This means that files may also be amplified. It is important to be careful about dynamic levels when mixing soundfiles with high amplitudes. As a rule of thumb, always mix two files as close to 0dB (maximum level) as you can, but run SUBMIX GETLEVEL first to check for overload. If overload is reported, use SUBMIX ATTENUATE to reduce the overall level of the mixfile. On the other hand, it is also possible to reduce the volume of soundfiles in an undesirable way, and experience indicates a fairly high tolerance: so do not reduce the volumes too much unless a previous run has produced distortion. SNDINFO MAXSAMP will display the maximum amplitude in any given soundfile, so it can be used to help determine precise values when this seems necessary. Also remember that every 6dB reduction halves the amplitude. Pan parameters may take on values from -1.0 (left) to +1.0 (right) to place the sound in the stereo space between the loudspeakers. Alternatively, L C and R may be used for Left Centre and Right. Values greater than 1 (or less than -1) will force the sound to full Right (or Left), but unattenuated, to give an illusion of even greater rightward (or leftware) distance. Note that by selecting values inbetween 0 and -1 or +1, soundfiles may be placed at specific locations along the horizontal plane. These Pan parameters do not, therefore, create moving sounds as does the MODIFY SPACE function, Mode 1 (Pan), but they do enable the composer to spread out the musical material in the aural field. This is common practice on mixing desks where the manual pan control is used to maintain the aural clarity of all the sounds being mixed. It is also possible, of course, to locate all the soundfiles of a mix in one area of the soundfield, in preparation for placing the results of a different mix in another area. Entering Pan information as 'L' for all the sounds in the mixfile (or as all 'R') will force the creation of a mono soundfile as output. Musical applications; Because the results of previous mixes can be used (they are simply new soundfiles), very complex layering of musical material can be achieved. MIX can also be used to splice sounds together, as this is merely a special case of mixing (see SPLICE). The number of soundfiles which may be open at one time during a mix is limited to 1000 (!), which shows how useful this program can be. Not only can many soundfiles be mixed at one time, but the previous rigmarole of synchronising, starting and stopping several tape recorders becomes a thing of the past." },
  arg1 = { name = "mixfile", input = "txt", tip = "soundfiles to be mixed, with appropriate information" },
  arg2 = { name = "outsndfile", output = "wav", tip = "resultant mix of soundfiles (Mono or Stereo depending on Pan settings in the mixfile)" },
  arg3 = { name = "start", switch = "-s", min = 0, max = 100, def = 0.1, tip = "begin the mixing process at time start (to start mixing later than time zero)" },
  arg4 = { name = "end", switch = "-e", min = 0, max = 100, def = 100, tip = "stop the mixing process at time end (to stop the mix before its true end" },
  arg5 = { name = "attenuation", switch = "-g", min = 0, max = 1, def = 0, tip = "reduce the level of the entire mix (Range: > 0 to 1)" },
  arg6 = { name = "-a", switch = "-a", tip = "alternative mix algorithm, slighly slower, but it may avoid clipping in special circumstances" },
}

dsp["Model - Replace soundfiles in an existing mixfile"] = {
  cmds = { exe = "submix", mode = "model", tip = "This process allows you to use an existing mixfile as a model for a new mix using different sounds. You submit to the process the original mixfile together with the new soundfiles you now want to use – you need to have the same number of soundfiles as in the original mix and in the same order). The original sounds are then replaced by the new sounds and a new mixfile generated. Note that the new sounds must have the same number of channels (and sample-rate) as those that they replace. Musical applications; As you continue to work, you may build up a repertoire of mixfiles with specific level and pan patterns. This program enables you to reuse them with different soundfiles. Alternatively, you may want to try the mix you're working on with some or all new sounds. (If some, you will need to re-enter the sounds you want to retain so that the same number of soundfiles are specified.)" },
  arg1 = { name = "inmixfile", switch = "txt", tip = "input mixfile to alter" },
  arg2 = { name = "sndfile1", input = "wav", tip = "first new soundfile to replace" },
  arg3 = { name = "sndfile2", input = "wav", tip = "second new soundfile to replace" },
  arg4 = { name = "outmixfile", output = "txt", tip = "mixfile created by the program" },
}

dsp["Ongrid - Convert listed soundfiles to a basic mixfile on timed grid (for editing)"] = {
  cmds = { exe = "submix", mode = "ongrid", tip = "CREATE MIXFILE creates a basic mixfile in which the sounds all start at time zero, or where each sound starts as the previous sound has finished. SUBMIX ONGRID makes this more flexible by allowing you to specify the times at which your set of soundfiles are to start in the output mixfile. NB: If no sound is used at time zero, the mix will skip to the first sound actually used. To avoid this, use a silent soundfile at time zero. Musical applications; SUBMIX ONGRID provides another quick way to create a mixfile. In this case, both the file names and the start times are set. That leaves only the level and pan data to be edited." },
  arg1 = { name = "infile 1", input = "wav", tip = "first input soundfile" },
  arg2 = { name = "infile 2", input = "wav", tip = "second input soundfile" },
  arg3 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg4 = { name = "gridfile", input = "txt", tip = "a list of times - see CDP docs for details" },
}

dsp["Pan - Pan a mixfile)"] = {
  cmds = { exe = "submix", mode = "pan", tip = "This process allows the sounds in a mixfile to be repositioned to a given location in the stereo field, or to different locations at different times. Note that it is the start pan positions of the sounds which are (re)set by this process. Once a sound has begun to play, it remains at its start position. Musical applications; ease of pan redesign, possibility of sophisticated positioning via interpolation, the same panfile could be used with block processing in Sound Loom to alter several mixfiles at once, as long as they had the same number of sounds / start times." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "pan", min = -100, max = 100, def = 1, input = "brk", tip = "locates the sounds in the mix at different positions between the speakers." },
}

dsp["Shuffle - 1 Duplicate each line. Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 1", tip = "1 Duplicate each line. Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 2 Reverse order of filenames. Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 2", tip = "2 Reverse order of filenames. Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 3 Scatter order of filenames. Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 3", tip = "3 Scatter order of filenamess. Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 4 Replace sounds in selected lines with sound in startline. Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 4", tip = "4 Replace sounds in selected lines with sound in startline. Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 5 Omit lines (closing up timegaps appropriately). Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 5", tip = "5 Omit lines (closing up timegaps appropriately). In Modes 5 & 6, the mix must be in the correct time order. (Mixfiles can be time-ordered using SUBMIX TIMEWARP Mode 1). Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 6 Omit alternate lines (closing up timegaps appropriately). Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 6", tip = "6 Omit alternate lines (closing up timegaps appropriately). In Modes 5 & 6, the mix must be in the correct time order. (Mixfiles can be time-ordered using SUBMIX TIMEWARP Mode 1). Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
}

dsp["Shuffle - 7 Duplicate and rename: duplicate each line with a new sound, new name. Shuffle the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "shuffle 7", tip = "7 Duplicate and rename: duplicate each line with a new sound, new namee. Derived frrom the original MIXSHUFL, there are now 3 sets of 'shuffle' functions, all of which deal with mixfile data. SUBMIX SPACEWARP handles the spatial location of the sounds, and SUBMIX TIMEWARP handles their onset times. SUBMIX SHUFFLE itself massages lines in the mixfile and filenames. Musical applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. SUBMIX SHUFFLE can duplicate filenames in a mix, or replace all the names with that of the first sound. Omitting lines thins out the mix, reversing the order of the names reverses the sequence of a whole series of events, but each event still goes forward (i.e., not just a soundfile played backwards). Scattering the order of soundfiles loosens up the mix and can be quite handy when making sonic collages or landscapes. p> Only one option can be entered at once, but this should not be a problem. The operation of the program is very fast, as it is only writing a short textfile. A variety of changes can be carried out in sequence by using two outmixfile names: write the first change to outmix1, then use outmix1 as the input for the next run of SUBMIX SHUFFLE, naming the outmixfile outmix2. On the next run, outmix2 is the inmixfile and outmix1 is the outmixfile, the original version being overwritten by the program, etc." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg4 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" },
  arg5 = { name = "-x", switch = "-x", tip = "-x flag turns off 'newname' checking" },
}

dsp["Spacewarp - 1 Sounds to same position – Q is position (stereo files become mono). Alter the spatial distribution of a mixfile"] = {
  cmds = { exe = "submix", mode = "spacewarp 1", tip = "1 Sounds to same position – Q is position (stereo files become mono). Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 2 Narrow spatial spread – Q is a positive number < 1. Alter the spatial distribution of a mixfile"] = {
  cmds = { exe = "submix", mode = "spacewarp 2", tip = "2 Narrow spatial spread – Q is a positive number < 1. Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 3 Sequence positions leftwards – over range Q1 to Q2 (stereo files become mono)."] = {
  cmds = { exe = "submix", mode = "spacewarp 3", tip = "3 Sequence positions leftwards – over range Q1 to Q2 (stereo files become mono). Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "Q2", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg5 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg6 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 4 Sequence positions rightwards – over range Q1 to Q2 (stereo files become mono)."] = {
  cmds = { exe = "submix", mode = "spacewarp 4", tip = "4 Sequence positions rightwards – over range Q1 to Q2 (stereo files become mono). Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "Q2", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg5 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg6 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 5 Random-scatter positions – within range Q1 to Q2 (stereo files become mono)."] = {
  cmds = { exe = "submix", mode = "spacewarp 5", tip = "5 Random-scatter positions – within range Q1 to Q2 (stereo files become mono). Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "Q2", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg5 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg6 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 6 Random-scatter positions, but alternate to the left and right of the centre of the spatial range specified "] = {
  cmds = { exe = "submix", mode = "spacewarp 6", tip = "6 Random-scatter positions, but alternate to the left and right of the centre of the spatial range specified – range Q1 to Q2 (stereo files become mono). Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "Q2", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg5 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg6 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Spacewarp - 7 Invert stereo in alternate lines of mixfile – use to avoid clipping "] = {
  cmds = { exe = "submix", mode = "spacewarp 7", tip = "7 Invert stereo in alternate lines of mixfile – use to avoid clipping. Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
}

dsp["Spacewarp - 8 Invert stereo in specified line of mixfile – Q is line number"] = {
  cmds = { exe = "submix", mode = "spacewarp 8", tip = "8 Invert stereo in specified line of mixfile – Q is line number. Developed from the earlier CDP program, MIXSHUFL, SUBMIX SPACEWARP massages the spatialisation information in existing mixfiles, presuming that there are several sounds involved. It provides a number of 'preset' operations to focus the sounds, move them gradually left or right, scatter them about the aural field, or switch the stereo channels (left to right, right to left). Modes 7 and 8, to 'invert stereo' can be used to avoid clipping which can occur because too many sounds are in one part of the aural field (the amplitudes can accumulate). Musical Applications; The spatial placement of sounds in a mix is part of the art of 'orchestrating' in this type of medium. If all the sounds are in the same position, they will tend to fuse more than if they are spread out in the aural space. Creating different spatial locations for sounds is therefore a way to clarify the aural image. It can also be used to move sounds along the horizontal trajectory or to have them appear in varying (random) locations. The latter might be especially suitable for complex collages or for sound textures which emulate a 'natural' environment." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
}

dsp["Sync - 1 Synchronise soundfile midtimes. Synchronise soundfiles in a mixfile, or generate such a mixfile from a list of soundfiles"] = {
  cmds = { exe = "submix", mode = "sync 1", tip = "1 Synchronise soundfile midtimes. This function aligns soundfiles either at the mid-point in the duration of each sound, or at their endings (like a 'right justify' in word processing). This is a calculation based on duration only, and does not take any account of audio data. It just adjusts the start-times of all the files in order to achieve the alignment specified by the Mode and writes a mixfile with these timings. It actually looks at the header of each soundfile when preparing the mixfile. Thus it can include the number of channels in the output mixfile when provided only with a list of soundfile names. But note that this mixfile may need some editing of its level and pan parameters, which are given the default settings of '1.0' and 'C' respectively for all soundfiles. Musical Applications; SUBMIX SYNC helps block out a mixfile when the sounds are aligned at their mid-points or their endings. It is also a way to create a rough mixfile quickly, and can therefore be used just to create a template mixfile to edit." },
  arg1 = { name = "intextfile", input = "txt", tip = "either an existing mixfile or the name of textfile containing only the names of soundfiles." },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
}

dsp["Sync - 2 synchronise soundfile endtimes. Synchronise soundfiles in a mixfile, or generate such a mixfile from a list of soundfiles"] = {
  cmds = { exe = "submix", mode = "sync 2", tip = "2 synchronise soundfile endtimes. This function aligns soundfiles either at the mid-point in the duration of each sound, or at their endings (like a 'right justify' in word processing). This is a calculation based on duration only, and does not take any account of audio data. It just adjusts the start-times of all the files in order to achieve the alignment specified by the Mode and writes a mixfile with these timings. It actually looks at the header of each soundfile when preparing the mixfile. Thus it can include the number of channels in the output mixfile when provided only with a list of soundfile names. But note that this mixfile may need some editing of its level and pan parameters, which are given the default settings of '1.0' and 'C' respectively for all soundfiles. Musical Applications; SUBMIX SYNC helps block out a mixfile when the sounds are aligned at their mid-points or their endings. It is also a way to create a rough mixfile quickly, and can therefore be used just to create a template mixfile to edit." },
  arg1 = { name = "intextfile", input = "txt", tip = "either an existing mixfile or the name of textfile containing only the names of soundfiles." },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
}

dsp["Syncattack - Synchronise the attacks of soundfiles in a mixfile, or generate such a mixfile from a list of soundfiles"] = {
  cmds = { exe = "submix", mode = "syncattack", tip = "SYNCATTACK means 'synchronised attacks'. This is synchronisation based on audio content, 'connecting' soundfiles at their respective points of high amplitude. SUBMIX SYNCATTACK by default looks for the highest amplitude in each file, and lines up the sounds accordingly. This point could be near the beginning, later on in the file, or within the times specified. Very often, for example, sounds played on wind instruments 'swell': the amplitude increases as the tone continues. This means that the peak point may occur considerably later on as the sound continues. If this is the case, you will find that SYNCATTACK will bring in another sound when this peak occurs. To get the sounds to synchronise at the beginning, specify times in intextfile which will locate the search range for the peak at/near the beginning of the sound(s) which get louder later: e.g., times 0 and 0.1. The 'peak power' option looks for the largest region of high amplitude rather than just the highest amplitude (which could have a very short duration). This process is well-suited for use with vocal sounds, the default maximum amplitude sync point works best for consonants (short durations), while the peak power option (-p) works best for vowels (longer durations). It automatically creates a mixfile, which is used to mix the sounds into a new, synchronised, soundfile. Focus enables the user to control the size of the window in which the attack is to be found, i.e., to tighten the focus on the precise attack moment. The datafile lists the soundfiles to be synchronised, with optional start and end times for each file search. The mixfile is a text file in mixfile format generated by the program. Run this with SUBMIX MIX to create the final, synchronised soundfile. It is usually a good idea to look at the mixfile before using it in order to check that the levels and pan settings are as you would like them to be for those particular soundfiles. If you hear a click at the entry of one (or more) of the sounds, you will have to reduce the level(s) in the mixfile produced by SUBMIX SYNCATTACK before running SUBMIX MIX to carry out the mix. SUBMIX GETLEVEL will report any possible overloads in the mix, while SUBMIX ATTENUATE can be used to reduce the overall level of the mixfile. Musical applications; The basic default operation of this function is a way to assemble sounds in a very smooth and natural way by aligning them at their respective loudest points. This means that they all peak at the same time – and more often than not, their respective beginnings will occur before this point. SUBMIX SYNCATTACK can also be used to create an aural 'mask': with the loud portion of one file 'covering' the entrance of the next sound. If this doesn't happen through the default operation of SUBMIX SYNCATTACK, it can be achieved by using the times mechanism to locate the search range at the beginning of the sound. For example, two sounds may swell a bit, but you want one of them to come in, to begin, at the high point of the other. You therefore enter times such as 0 0.1 in intextfile after the name of the file you want to sync at its beginning. The result is this: you hear the first sound begin and swell in volume. At its peak, the second sound enters, probably with a masking effect, because its beginning is a bit quieter relative to what occurs later. Thus its entry will be 'covered' by the peak of the first sound. As mentioned above, synchronisation can be forced to the beginning of the sounds by specifying times in intextfile." },
  arg1 = { name = "intextfile", input = "txt", tip = "either an existing mixfile or the name of textfile containing only the names of soundfiles." },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "focus", switch = "-w", min = 1, max = 6, tip = "a fixed integer factor which can be used to shorten the window which scans for the attack. Range:1 (max window size, min focus) – 5 (min window size, max focus)" },
  arg4 = { name = "-p", switch = "-p", tip = "find the peak power segment before locating its maximum sample. Default: only look for the maximum sample." },
}

dsp["Test - Test the syntax of a mixfile"] = {
  cmds = { exe = "submix", mode = "test", tip = "SUBMIX TEST checks both that all the soundfiles listed in it are valid soundfiles and that the syntax of the mixfile is correct.In checking the validity of the soundfiles, it actually looks for, needs to find, and reads the header of the soundfiles: so they must be present in the same directory as the mixfile. This is more than a check for mis-spelling the name of the file. There are many different types of file which might be present, possibly with a .wav extension, which will not be soundfiles: such as analysis files, binary envelope files, binary formant files, and binary pitch data files – not to mention breakpoint text files, which may or may not have an extension. You might forget which is which, so SUBMIX TEST actually looks into the header of the file to double-check its type. We have also added a filetype field to the DIRSF report. It is labelled 'fmt', which stands for 'format'. Under this heading, DIRSF displays 'W' for .wav or 'A' for .aif, because any of the above binary files could be .wav or .aif, which is not indicated by the extension. The syntax checks will pick up errors such as an invalid number of channels, a channel count which doesn't match the soundfile, left and right level or pan data for sounds which are only mono, or only one set of data for a stereo sound, or a missing start time (reports an invalid channel number because it's reading the level data in the channel position). It doesn't check the validity of amplitude levels. Musical Applications; This is a useful tool when mixes are large and complex." },
  arg1 = { name = "mixfile", input = "txt", tip = "full name of mixfile to be tested" },
}

dsp["Timewarp - 1  Sort into time order. Timewarp the data in a mixfile"] = {
  cmds = { exe = "submix", mode = "timewarp 1", tip = "1  Sort into time order. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple." },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mix function file" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output (shuffled) mix function file" },
  arg3 = { name = "Q", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
}

dsp["Timewarp - 2  Reverse timing pattern - e.g., a ritardando of sound entries becomes an accelerando"] = {
  cmds = { exe = "submix", mode = "timewarp 2", tip = "2  Reverse timing pattern – e.g., a ritardando of sound entries becomes an accelerando. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 3  Reverse timing pattern & order of filenames"] = {
  cmds = { exe = "submix", mode = "timewarp 3", tip = "3  Reverse timing pattern & order of filenames. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 4  Freeze timegaps - between sounds at first timegap values"] = {
  cmds = { exe = "submix", mode = "timewarp 4", tip = "4  Freeze timegaps - between sounds at first timegap values. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 5  Freeze timegaps & names"] = {
  cmds = { exe = "submix", mode = "timewarp 5", tip = "5  Freeze timegaps & names – same as Mode 4 and all files take the name of the first soundfile. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q1", min = -1, max = 1, def = 0, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 6  Scatter entry times - around the original time values. Q is scattering (Range: 0 to 1)"] = {
  cmds = { exe = "submix", mode = "timewarp 6", tip = "6  Scatter entry times – around the original time values. Q is scattering (Range: 0 to 1). The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 1, def = 1, tip = "position along horizontal spatial axis (Range: -1 to 1)" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 7  Shuffle up entry times - shuffle times in mixfile forward by time Q seconds"] = {
  cmds = { exe = "submix", mode = "timewarp 7", tip = "7  Shuffle up entry times – shuffle times in mixfile forward by time Q seconds. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 8  Add to timegaps - add fixed duration Q seconds to all timegaps between sounds"] = {
  cmds = { exe = "submix", mode = "timewarp 8", tip = "8  Add to timegaps – add fixed duration Q seconds to all timegaps between sounds. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 9  Create fixed timegaps 1 - same gap between all sounds, timegap = Q seconds"] = {
  cmds = { exe = "submix", mode = "timewarp 9", tip = "9  Create fixed timegaps 1 – same gap between all sounds, timegap = Q seconds. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 10 Create fixed timegaps 2 - expanding: starttime + Q, original time + 2Q, etc."] = {
  cmds = { exe = "submix", mode = "timewarp 10", tip = "10 Create fixed timegaps 2 – expanding: starttime + Q, original time + 2Q, etc.. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 11 Create fixed timegaps 3 - expanding: starttime * (1+Q), original time * (1+2Q), etc.."] = {
  cmds = { exe = "submix", mode = "timewarp 11", tip = "11 Create fixed timegaps 3 – expanding: starttime * (1+Q), original time * (1+2Q), etc.. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 12 Create fixed timegaps 4 - expanding: starttime * Q, original time *Q*Q, etc."] = {
  cmds = { exe = "submix", mode = "timewarp 12", tip = "12 Create fixed timegaps 4 – expanding: starttime * Q, original time *Q*Q, etc. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 13 Enlarge timegaps 1 - multiply original times by Q"] = {
  cmds = { exe = "submix", mode = "timewarp 13", tip = "13 Enlarge timegaps 1 – multiply original times by Q. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 14 Enlarge timegaps 2 - by adding Q, 2Q, etc.to each successive time"] = {
  cmds = { exe = "submix", mode = "timewarp 14", tip = "14 Enlarge timegaps 2 – by adding Q, 2Q, etc.to each successive time. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 15 Enlarge timegaps 3 - multiply original times by (1=Q), (1+2Q), (1+3Q),etc."] = {
  cmds = { exe = "submix", mode = "timewarp 15", tip = "15 Enlarge timegaps 3 – multiply original times by (1=Q), (1+2Q), (1+3Q),etc.. The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

dsp["Timewarp - 16 Enlarge timegaps 4 - multiply original times by Q, Q*Q, Q*Q*Q, etc. (Care!)"] = {
  cmds = { exe = "submix", mode = "timewarp 16", tip = "16 Enlarge timegaps 4 – multiply original times by Q, Q*Q, Q*Q*Q, etc. (Care!). The focus here is on the start times for each of the sounds in the mix. With SUBMIX TIMEWARP the several times in the mixfile can be adjusted in a single operation, according to the 'presets' available. The times can be sorted, all made the same, or all enlarged by the same amount, scattered in time (not far, just some 'jitter'), progressively expanded with reference to the start time, or each time in the mixfile can be progressively expanded. Non-mathematicians: take special note of the results of operations carried out on values less than one. For example, repeated (recursive) multiplications of 2 numbers, both of which are less than one soon produces very tiny values: e.g., .2*.2 = .04, .04*.2 = .008 etc. Musical Applications; The purpose of these functions is to enable the user to adjust a (usually complex) mixfile in a quick and easy manner. For example, you may have found the mix too aurally dense and want to spread it out a little. In this case you could for example enlarge the timegaps a little and perform the mix again. Mode 13 can be seen as a simple way to change 'tempo'. We can illustrate this with the following 3 steps: Suppose we create a mixfile with 3 soundfiles by using SUBMIX DUMMY Mode 1, in which all soundfiles are set to start at time 0. (Remember to include the soundfile extensions when you enter the name of each soundfile – they have to be explicitly included). We want to stagger their entries by a regular timegap. Let's make this 1 second, in the sense that 1 second will equal a tempo of crotchet (quarter note) = 60. We can do this with SUBMIX TIMEWARP Mode 8 which adds a constant value to each start time. So we would enter by GUI or command line: submix timewarp 8 mix3dum.mix mix3dum2.mix 1. Now the first soundfile in the new mixfile starts at time 0.0, the second at time 1.0 and the third at time 2.0 – a regular tempo at crotchet = 60. We want to increase this to crotchet = 76. The formula for the change is original_tempo / new_tempo, in this case 60 / 76 = 0.83. Now we go to SUBMIX TIMEWARP Mode 13 (multiply by a constant value) and use 0.83 as the multiplier: submix timewarp 13 mix3dum2.mix mix3dum3.mix 0.83. Now the first soundfile will begin at time 0.0, the second at time 0.83 and the third at time 1.66, so they enter at a tempo of crotchet = 76.  This illustration, hopefully, gives some clues as to how to make use of the other modes of SUBMIX TIMEWARP. SUBMIX TIMEWARP is, in effect, applying a bit of algorithmic processing to the times in a mixfile. Although limited to what are in effect a set of 'presets', its functions nevertheless comprise a useful and time-saving set of routines to focus, regularise, or varyingly expand the interval between the start of each sound in the mix. It points towards further possibilities of score processing/generation by user-defined algorithmic means. The interesting questions are what processes are important musically, and how does one make the output musically supple" },
  arg1 = { name = "inmixfile", input = "txt", tip = "input mixfile" },
  arg2 = { name = "outmixfile", output = "txt", tip = "output mixfile generated by the program" },
  arg3 = { name = "Q2", min = 0, max = 100, def = 1, tip = "seconds" },
  arg4 = { name = "startline", switch = "-s", min = 1, max = 100, def = 1, tip = "line of inmixfile at which to start shuffling (default is 1st line)" },
  arg5 = { name = "endline", switch = "-e", min = 1, max = 100, def = 100, tip = "line of inmixfile at which to end shuffling (default is last line of file)" }, 
}

------------------------------------------------
-- subtract
------------------------------------------------
 
dsp["Subtract - Subtract one file from another"] = {
  cmds = { exe = "subtract", mode = "subtract", tip = "This program simply subtracts infile2 (which must be mono) from infile1, which may be multi-channel. The chan parameter enables you to specify which channel of an input multi-channel soundfile to use. The output soundfile will be mono or multi-channel depending on the inputs." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "chans", switch = "-c", min = 1, max = 2, def = 1, tip = "which channel of a multi-channel insndfile1 (includes a stereo soundfile) to use." },
}
 
------------------------------------------------
-- superaccu
------------------------------------------------
 
dsp["Superaccu - 1 - Operates like FOCUS ACCU - Sustain each spectral band until louder data appears in that band "] = {
  cmds = { exe = "superaccu", mode = "superaccu 1", tip = "1 – Operates like FOCUS ACCU - Superaccu is an extension of FOCUS ACCU, with additional modes to tune the sustained spectral channels to a resonance template. The tuning has a remarkable effect on speech, especially in Mode 2. For SUPERACCU, you might have a statement saying something like: 'Warning: High values of decay can generate very long soundfiles.' I tried a Decay rate of .88 with a 6 second speech file, and got a 78 second long ringing after the original talking. Quite lovely, but something to warn folks about. (Warren Burt) " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "decay", switch = "-d", min = 0, max = 1, def = 1, tip = "sustained channel data attenuates by the factor decay per second. Possible Range: >0.0 to 0.9 Default: 1.0 (no attenuation)" },
  arg4 = { name = "glis", switch = "-g", min = -11.7, max = 11.7, def = 0, tip = "sustained channel data glissandos at glis octaves per second. Approximate Range: -11.7 to 11.7 Default: 0" },
  arg5 = { name = "-r", switch = "-r", tip = "reassign the glissandoing pitches to appropriate channels " },
}
 
dsp["Superaccu - 2 - Forces the (start of) resonances to the tempered scale - Sustain each spectral band until louder data appears in that band "] = {
  cmds = { exe = "superaccu", mode = "superaccu 2", tip = "2 – Forces the (start of) resonances to the tempered scale - Superaccu is an extension of FOCUS ACCU, with additional modes to tune the sustained spectral channels to a resonance template. The tuning has a remarkable effect on speech, especially in Mode 2. For SUPERACCU, you might have a statement saying something like: 'Warning: High values of decay can generate very long soundfiles.' I tried a Decay rate of .88 with a 6 second speech file, and got a 78 second long ringing after the original talking. Quite lovely, but something to warn folks about. (Warren Burt) " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "decay", switch = "-d", min = 0, max = 1, def = 1, tip = "sustained channel data attenuates by the factor decay per second. Possible Range: >0.0 to 0.9 Default: 1.0 (no attenuation)" },
  arg4 = { name = "glis", switch = "-g", min = -11.7, max = 11.7, def = 0, tip = "sustained channel data glissandos at glis octaves per second. Approximate Range: -11.7 to 11.7 Default: 0" },
  arg5 = { name = "-r", switch = "-r", tip = "reassign the glissandoing pitches to appropriate channels " },
}
 
dsp["Superaccu - 3 - The frequencies are specified in a 'tuning' file (a 'harmonic set') - Sustain each spectral band until louder data appears in that band "] = {
  cmds = { exe = "superaccu", mode = "superaccu 3", tip = "3 - The frequencies are specified in a 'tuning' file (a 'harmonic set')- Superaccu is an extension of FOCUS ACCU, with additional modes to tune the sustained spectral channels to a resonance template. The tuning has a remarkable effect on speech, especially in Mode 2. For SUPERACCU, you might have a statement saying something like: 'Warning: High values of decay can generate very long soundfiles.' I tried a Decay rate of .88 with a 6 second speech file, and got a 78 second long ringing after the original talking. Quite lovely, but something to warn folks about. (Warren Burt) " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "tuning", input = "txt", tip = "text file containing the set of frequencies in (possibly fractional) MIDI values (1-127) for tuning the onsets of the resonance" },
  arg4 = { name = "decay", switch = "-d", min = 0, max = 1, def = 1, tip = "sustained channel data attenuates by the factor decay per second. Possible Range: >0.0 to 0.9 Default: 1.0 (no attenuation)" },
  arg5 = { name = "glis", switch = "-g", min = -11.7, max = 11.7, def = 0, tip = "sustained channel data glissandos at glis octaves per second. Approximate Range: -11.7 to 11.7 Default: 0" },
  arg6 = { name = "-r", switch = "-r", tip = "reassign the glissandoing pitches to appropriate channels " },
}
 
dsp["Superaccu - 4 - The frequencies and their octaves are specified in a 'tuning' file (a harmonic field)  - Sustain each spectral band until louder data appears in that band "] = {
  cmds = { exe = "superaccu", mode = "superaccu 4", tip = "4 – The frequencies and their octaves are specified in a 'tuning' file (a harmonic field)- Superaccu is an extension of FOCUS ACCU, with additional modes to tune the sustained spectral channels to a resonance template. The tuning has a remarkable effect on speech, especially in Mode 2. For SUPERACCU, you might have a statement saying something like: 'Warning: High values of decay can generate very long soundfiles.' I tried a Decay rate of .88 with a 6 second speech file, and got a 78 second long ringing after the original talking. Quite lovely, but something to warn folks about. (Warren Burt) " },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "tuning", input = "txt", tip = "text file containing the set of frequencies in (possibly fractional) MIDI values (1-127) for tuning the onsets of the resonance" },
  arg4 = { name = "decay", switch = "-d", min = 0, max = 1, def = 1, tip = "sustained channel data attenuates by the factor decay per second. Possible Range: >0.0 to 0.9 Default: 1.0 (no attenuation)" },
  arg5 = { name = "glis", switch = "-g", min = -11.7, max = 11.7, def = 0, tip = "sustained channel data glissandos at glis octaves per second. Approximate Range: -11.7 to 11.7 Default: 0" },
  arg6 = { name = "-r", switch = "-r", tip = "reassign the glissandoing pitches to appropriate channels " },
}

------------------------------------------------
-- synth
------------------------------------------------

dsp["Synth Noise - Generate noise "] = { 
  cmds = { exe = "synth", mode = "noise", tip = "This process creates a single type of white noise. Variants on this noise can be achieved with filtering. Musical applications; You are recommended not to just take the noise file as it comes. A bit of pre-processing will enhance results when making use of it for music composition. Here are a few ideas. Noise can be used as a complex sound source. With a rapidly changing amplitude shape, it may be a useful source for various subtractive forms of sound transformation, such as HILITE TRACE or HILITE BLTR. It can add grit, such as by being a component in COMBINE INTERLEAVE (short bursts of sound alternating with other sonic material), or by being mixed at a discrete amplitude level with another sound. COMBINE MAX is an interesting way to mingle noise with another sound. Noise can be very uniform, in spite of its complex waveform. So some work to make it more changeable might be in order before expecting very much to happen when subjecting the material to other transformations. Given amplitude variations, these may be enhanced with ENVEL WARP functions, and any spectral envelope present can be exaggerated with FOCUS EXAG" }, 
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000" },
  arg3 = { name = "channels", min = 1, max = 2, tip = "number of channels in outfile " },
  arg4 = { name = "duration", min = 0, max = 10, def = 1, tip = "duration of outfile in seconds" },
  arg5 = { name = "amplitude", switch = "-a", min = 0, max = 1, def = 1, input = "brk", tip = "amplitude of outfile (0.0 < Range < 1.0 max & default)" },
  arg6 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
}

dsp["Synth Silence - Make a silent soundfile"] = { 
  cmds = { exe = "synth", mode = "silence", tip = " All the samples of a silent soundfile have zero amplitude. Musical applications: Silent soundfiles can be used to enforce pauses before, between, or after other sonic material. For example, a 2 second pause may be wanted between a list of soundfiles. This is done with the splicing function, SFEDIT JOIN, with a silent soundfile of 2 seconds duration spliced between each of the other soundfiles. When specifying the sample rate, remember that it needs to be consistent with your other soundfiles so that it can be used in a mix. Similarly, splicing with stereo soundfiles requires a stereo silent file. " }, 
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000" },
  arg3 = { name = "channels", min = 1, max = 2, tip = "number of channels in outfile " },
  arg4 = { name = "duration", min = 0, max = 10, def = 1, tip = "duration of outfile in seconds" },
  arg5 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
}

dsp["Synth Chord 1 - Generate a chord with a simple waveform based on midi pitch values "] = { 
  cmds = { exe = "synth", mode = "chord 1", tip = "All notes of the chord produced begin and end at the same time. A pure sine tone is used to generate the chord. Musical applications; This program was written to provide a way of generating chordal sounds for the Tonal Harmony Workshop of the Musical Testbed in Sound Loom. It is not expected to be used as sonic source material, but given the versatility of the CDP software, the unexpected could happen. For example, it could provide a way to create synthetic mechanical sounds, further developed with functions such as wavecycle distortion, downwards transposition etc.: e.g., to make a background hum for a generator or spaceship." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "datafile", input = "txt", tip = "insert midi pitch values" },
  arg3 = { name = "sr", min = 0, max = 48000, def = 44100, tip = "Sample Rate: may be 48000, 24000, 44100, 22050, 32000, or 16000" },
  arg4 = { name = "chans", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg5 = { name = "dur", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg6 = { name = "amp", switch = "-a",  min = 0, max = 1, def = 1, tip = "amplitude of the output sound (0.0 to 1.0). 1.0 is maximum amplitude, and the Default value." },
  arg7 = { name = "tabsize", switch = "-t", min = 0, max = 32000, def = 4096, tip = "the size of the table storing the waveform. It defaults to 4096: the input value is always rounded to a multiple of 4." },
} 

dsp["Synth Chord 2 - Generate a chord with a simple waveform based on frequency in hertz "] = { 
  cmds = { exe = "synth", mode = "chord 2", tip = "All notes of the chord produced begin and end at the same time. A pure sine tone is used to generate the chord. Musical applications; This program was written to provide a way of generating chordal sounds for the Tonal Harmony Workshop of the Musical Testbed in Sound Loom. It is not expected to be used as sonic source material, but given the versatility of the CDP software, the unexpected could happen. For example, it could provide a way to create synthetic mechanical sounds, further developed with functions such as wavecycle distortion, downwards transposition etc.: e.g., to make a background hum for a generator or spaceship." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "datafile", input = "txt", tip = "contains a list of frequencies in Hz" },
  arg3 = { name = "sr", min = 0, max = 48000, def = 44100, tip = "Sample Rate: may be 48000, 24000, 44100, 22050, 32000, or 16000" },
  arg4 = { name = "chans", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg5 = { name = "dur", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg6 = { name = "amp", switch = "-a",  min = 0, max = 1, def = 1, tip = "amplitude of the output sound (0.0 to 1.0). 1.0 is maximum amplitude, and the Default value." },
  arg7 = { name = "tabsize", switch = "-t", min = 0, max = 32000, def = 4096, tip = "the size of the table storing the waveform. It defaults to 4096: the input value is always rounded to a multiple of 4." },
} 

dsp["Synth Wave 1 - Sine Wave - Generate synthetic waveforms "] = { 
  cmds = { exe = "synth", mode = "wave 1", tip = "1 Sine wave.  A table of the appropriate size is created to store the waveform. It is then used to generate the soundfile with properties as specified by the parameters. The breakpoint file option gives some additional shaping control, e.g., to create a softer attack and ensure that there will not be clicks. Musical applications Geometric waveforms of this type (square, sine, ramp) do produce upper partials, but are mostly used for test purposes or to illustrate some feature of the software in a clear manner, rather than as source material for musical composition. For comprehensive synthesis facilities, see the public domain synthesis engine Winsound, which we distribute with the CDP System. Information about other versions of Csound, both public domain and commercial, is available on the net. See http://www.bath.ac.uk/ ~masjpf/." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, def = 44100, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000. Rates below 8000 are not supported – a warning is issued for other rates " },
  arg3 = { name = "channels", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg4 = { name = "duration", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg5 = { name = "frequency", min = 0, max = 44100, input = "brk", def = 6000, tip = "frequency of outfile in Hz Frequency may vary over time" }, 
  arg6 = { name = "amplitude", switch = "-a",  min = 0, max = 1, def = 1, input = "brk", tip = "amplitude of outfile (0.0 < Range < 1.0 max & default) Amplitude may vary over tim" },
  arg7 = { name = "tablesize", switch = "-t", min = 0, max = 32000, def = 256, tip = "size of table storing the waveform (Default: 256). The input value is always rounded to a multiple of 4. " },
  arg8 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
} 

dsp["Synth Wave 2 - Square Wave 2 - Generate synthetic waveforms "] = { 
  cmds = { exe = "synth", mode = "wave 2", tip = "2 Square wave.  A table of the appropriate size is created to store the waveform. It is then used to generate the soundfile with properties as specified by the parameters. The breakpoint file option gives some additional shaping control, e.g., to create a softer attack and ensure that there will not be clicks. Musical applications Geometric waveforms of this type (square, sine, ramp) do produce upper partials, but are mostly used for test purposes or to illustrate some feature of the software in a clear manner, rather than as source material for musical composition. For comprehensive synthesis facilities, see the public domain synthesis engine Winsound, which we distribute with the CDP System. Information about other versions of Csound, both public domain and commercial, is available on the net. See http://www.bath.ac.uk/ ~masjpf/." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, def = 44100, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000. Rates below 8000 are not supported – a warning is issued for other rates " },
  arg3 = { name = "channels", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg4 = { name = "duration", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg5 = { name = "frequency", min = 0, max = 44100, input = "brk", def = 6000, tip = "frequency of outfile in Hz Frequency may vary over time" }, 
  arg6 = { name = "amplitude", switch = "-a",  min = 0, max = 1, def = 1, input = "brk", tip = "amplitude of outfile (0.0 < Range < 1.0 max & default) Amplitude may vary over tim" },
  arg7 = { name = "tablesize", switch = "-t", min = 0, max = 32000, def = 256, tip = "size of table storing the waveform (Default: 256). The input value is always rounded to a multiple of 4. " },
  arg8 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
} 

dsp["Synth Wave 3 - Triangle Wave 3 - Generate synthetic waveforms "] = { 
  cmds = { exe = "synth", mode = "wave 3", tip = "3 Triangle wave.  A table of the appropriate size is created to store the waveform. It is then used to generate the soundfile with properties as specified by the parameters. The breakpoint file option gives some additional shaping control, e.g., to create a softer attack and ensure that there will not be clicks. Musical applications Geometric waveforms of this type (square, sine, ramp) do produce upper partials, but are mostly used for test purposes or to illustrate some feature of the software in a clear manner, rather than as source material for musical composition. For comprehensive synthesis facilities, see the public domain synthesis engine Winsound, which we distribute with the CDP System. Information about other versions of Csound, both public domain and commercial, is available on the net. See http://www.bath.ac.uk/ ~masjpf/." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, def = 44100, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000. Rates below 8000 are not supported – a warning is issued for other rates " },
  arg3 = { name = "channels", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg4 = { name = "duration", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg5 = { name = "frequency", min = 0, max = 44100, input = "brk", def = 6000, tip = "frequency of outfile in Hz Frequency may vary over time" }, 
  arg6 = { name = "amplitude", switch = "-a",  min = 0, max = 1, def = 1, input = "brk", tip = "amplitude of outfile (0.0 < Range < 1.0 max & default) Amplitude may vary over tim" },
  arg7 = { name = "tablesize", switch = "-t", min = 0, max = 32000, def = 256, tip = "size of table storing the waveform (Default: 256). The input value is always rounded to a multiple of 4. " },
  arg8 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
} 

dsp["Synth Wave 4 - Ramp (sawtooth) Wave 4 - Generate synthetic waveforms "] = { 
  cmds = { exe = "synth", mode = "wave 4", tip = "4 Ramp (sawtooth) wave.  A table of the appropriate size is created to store the waveform. It is then used to generate the soundfile with properties as specified by the parameters. The breakpoint file option gives some additional shaping control, e.g., to create a softer attack and ensure that there will not be clicks. Musical applications Geometric waveforms of this type (square, sine, ramp) do produce upper partials, but are mostly used for test purposes or to illustrate some feature of the software in a clear manner, rather than as source material for musical composition. For comprehensive synthesis facilities, see the public domain synthesis engine Winsound, which we distribute with the CDP System. Information about other versions of Csound, both public domain and commercial, is available on the net. See http://www.bath.ac.uk/ ~masjpf/." },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "sample_rate", min = 8000, max = 96000, def = 44100, tip = "Standard sample rates supported: 96000, 88200, 48000, 44100, 32000, 24000, 22050, 1600, 11025, or 8000. Rates below 8000 are not supported – a warning is issued for other rates " },
  arg3 = { name = "channels", min = 1, max = 2, def = 1, tip = "number of channels: may be 1, 2" },
  arg4 = { name = "duration", min = 0, max = 20, def = 6, tip = "duration of the output sound, in seconds" },
  arg5 = { name = "frequency", min = 0, max = 44100, input = "brk", def = 6000, tip = "frequency of outfile in Hz Frequency may vary over time" }, 
  arg6 = { name = "amplitude", switch = "-a",  min = 0, max = 1, def = 1, input = "brk", tip = "amplitude of outfile (0.0 < Range < 1.0 max & default) Amplitude may vary over tim" },
  arg7 = { name = "tablesize", switch = "-t", min = 0, max = 32000, def = 256, tip = "size of table storing the waveform (Default: 256). The input value is always rounded to a multiple of 4. " },
  arg8 = { name = "-f", switch = "-f", tip = "write soundfile as 32-bit floats (Default: 16-bit shorts)" },
} 

dsp["Synth Clicks - 1 Start and end are times - Create a click track from tempo, meter and barring data "] = { 
  cmds = { exe = "synth", mode = "clicks 1", tip = "1 Start and end are times. SYNTH CLICKS makes a click track from start to end, the music starting at dataline 1, unless 'start at dataline 0' is selected. Provision is made for jumps to time points, General Pauses, tempo, acceleration and deceleration (i.e., gradual change from one tempo to another), barring (i.e., meters) and accent patterns. Musical applications; When used? Loud enough? Relationship to frame-related click tracks for film & video> " },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "clickfile", input = "txt", tip = "contains a numbered sequence of data lines followed by 2, 3 or 4 data items, separated by spaces." },
  arg3 = { name = "start", switch = "-s", min = 0, max = 20, def = 0, tip = "(optional) start time of click track " }, 
  arg4 = { name = "end", switch = "-e",  min = 0, max = 120, def = 10, tip = "(optional) end time of click track" },
  arg5 = { name = "zero", switch = "-z", tip = "option to start music at line zero" },
} 


dsp["Synth Clicks - 2 Start and end are data line numbers - Create a click track from tempo, meter and barring data "] = { 
  cmds = { exe = "synth", mode = "clicks 2", tip = "2 Start and end are data line numbers. SYNTH CLICKS makes a click track from start to end, the music starting at dataline 1, unless 'start at dataline 0' is selected. Provision is made for jumps to time points, General Pauses, tempo, acceleration and deceleration (i.e., gradual change from one tempo to another), barring (i.e., meters) and accent patterns. Musical applications; When used? Loud enough? Relationship to frame-related click tracks for film & video> " },
  arg1 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg2 = { name = "clickfile", input = "txt", tip = "contains a numbered sequence of data lines followed by 2, 3 or 4 data items, separated by spaces." },
  arg3 = { name = "start", switch = "-s", min = 0, max = 20, def = 0, tip = "(optional) start time of click track " }, 
  arg4 = { name = "end", switch = "-e",  min = 0, max = 120, def = 10, tip = "(optional) end time of click track" },
  arg5 = { name = "zero", switch = "-z", tip = "option to start music at line zero" },
} 

--[[
dsp["[Doesn't work - multiple outputs] Synth Spectra - Generate both channels of a stereo spectral band "] = { 
  cmds = { exe = "synth", mode = "spectra", tip = " SYNTH SPECTRA generates tones which can be spread across a bandwidth and be focused to different degrees on a specified pitch. This pitch focus may be made to wander around randomly within the band by the timevary parameter. Focus (min- and max-) is how tightly the energy is focused around the frequency you specify. If focus is very high, you can get almost a pure tone (with a narrow bandwidth – spread, and timevar at 0). With a low focus, the band is broader, less focused. With different values, the focus wobbles, so the sound is somewhat coloured. The timevar parameter can be used to move the point of focus in the frequency band. When timevar is set at 1, e.g., with a highly focused band, the point of focus will move randomly around the bandwidth. Thus the pitch will be highly focused (very little noise component) but wobble randomly within the band. When timevar is set to 0, the focus will adopt some position in the band and stay there without moving. Intermediate values for timevar produce results inbetween these two extremes. Musical applications; If you want to get as close as possible to a pure tone, with no warble, set min-foc and max-foc to 1000." },
  arg1 = { name = "dur", min = 0, max = 30, def = 6, tip = "duration of output sound, in seconds" },
  arg2 = { name = "frq", min = 0, max = 44100, def = 1500, tip = "centre frequency of the band" },
  arg3 = { name = "spread", min = 0, max = 44100, def = 10, tip = "width of band in Hz (default) or as transposition ratio" },
  arg4 = { name = "max-foc", min = 0.001, max = 1000, def = 1000, tip = "maximum degree to which the band energy is focused on (concentrated at) the centre frequency (Range: 0.001 to 1000)" },
  arg5 = { name = "min-foc", min = 0.001, max = 1000, def = 0.001, tip = "minimum degree to which the band energy is focused on (concentrated at) the centre frequency (Range: 0.001 to 1000)" },
  arg6 = { name = "timevar", min = 0, max = 1, def = 0.5, tip = "degree to which the band varies over time (Range: 0 to 1)" },
  arg7 = { name = "-p", switch = "-p", tip = "if this flag is set, spread is a transposition ratio" },
}
--]]
 
------------------------------------------------
-- tapdelay
------------------------------------------------
 
dsp["Tapdelay - stereo multi-echo generator with feedback"] = {
  cmds = { exe = "tapdelay", mode = "", tip = "The principle behind a tapped delay line is that whereas in a standard delay line (e.g., 1 second long) the ouput is taken at the end, in a tapped delay line extra outputs are also taken at intermediate points, such as 0.1 secs, 0.3 secs and 0.75 secs. " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "-f", switch = "-f", tip = "write floating-point outfile (Default: outfile has same format as infile)" },
  arg4 = { name = "tapgain", min = 0.0001, max = 1, tip = "gain factor applied to output from delay line (Range: > 0.0; typical value: 0.25)" },
  arg5 = { name = "feedback", min = -1, max = 1, tip = "delayed signal fed back into the input (Range: -1.0 to 1.0)" },
  arg6 = { name = "mix", min = 0, max = 1, tip = "proportion of source mixed with delay output (Range: 0.0 to < 1.0)" },
  arg7 = { name = "taps.txt", input = "txt", tip = "text file consisting of a list of taps in the format: time amp [pan]" },
  arg8 = { name = "trailtime", min = 0, max = length/1000, tip = "extra time in seconds (beyond infile duration) for delays to play out" },
}
 
------------------------------------------------
-- texture
------------------------------------------------
 
dsp["Texture Decorated 1 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "decorated 1", channels = "1in2out", tip = "1 On a given harmonic field. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Decorated 2 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "decorated 2", channels = "1in2out", tip = "2 On changing harmonic fields. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Decorated 3 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "decorated 3", channels = "1in2out", tip = "3 On a given harmonic set. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Decorated 4 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "decorated 4", channels = "1in2out", tip = "4 On changing harmonic sets. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Decorated 5 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; uses textfile"] = {
  cmds = { exe = "texture", mode = "decorated 5", channels = "1in2out", tip = "5 None . The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Grouped 1 - A texture of separate event groups is shaped by random selections from parameter ranges, with one or more input sounds; On a given harmonic field specified in text file"] = {
  cmds = { exe = "texture", mode = "grouped 1", channels = "1in2out", tip = "1 On a given harmonic field. The idea behind TEXTURE GROUPED is easily grasped by imagining several quick bursts of note events separated by silence. A 'group' is a 'burst of notes', and the program handles creating silences inbetween bursts. The program is, however, more flexible than that because: the note groups can in fact overlap, the timing of the note onsets within a group doesn't have to be rapid, the pitch location where each group begins can vary. You can easily understand, therefore, how the program makes it possible to create textures which are more varied than those of TEXTURE SIMPLE, and, if desired, punctuated by note groups which form gestural shapes. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between group onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg18 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg19 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg20 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg21 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg22 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg23 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg24 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg25 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg26 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg27 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg28 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg29 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg30 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg31 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg32 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg33 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg34 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Grouped 2 - A texture of separate event groups is shaped by random selections from parameter ranges, with one or more input sounds; On changing harmonic fields specified in a text file"] = {
  cmds = { exe = "texture", mode = "grouped 2", channels = "1in2out", tip = "2 On changing harmonic fields. The idea behind TEXTURE GROUPED is easily grasped by imagining several quick bursts of note events separated by silence. A 'group' is a 'burst of notes', and the program handles creating silences inbetween bursts. The program is, however, more flexible than that because: the note groups can in fact overlap, the timing of the note onsets within a group doesn't have to be rapid, the pitch location where each group begins can vary. You can easily understand, therefore, how the program makes it possible to create textures which are more varied than those of TEXTURE SIMPLE, and, if desired, punctuated by note groups which form gestural shapes. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between group onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg18 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg19 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg20 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg21 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg22 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg23 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg24 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg25 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg26 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg27 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg28 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg29 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg30 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg31 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg32 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg33 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg34 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Grouped 3 - A texture of separate event groups is shaped by random selections from parameter ranges, with one or more input sounds; On a given harmonic set specified in a text file"] = {
  cmds = { exe = "texture", mode = "grouped 3", channels = "1in2out", tip = "3 On a given harmonic set. The idea behind TEXTURE GROUPED is easily grasped by imagining several quick bursts of note events separated by silence. A 'group' is a 'burst of notes', and the program handles creating silences inbetween bursts. The program is, however, more flexible than that because: the note groups can in fact overlap, the timing of the note onsets within a group doesn't have to be rapid, the pitch location where each group begins can vary. You can easily understand, therefore, how the program makes it possible to create textures which are more varied than those of TEXTURE SIMPLE, and, if desired, punctuated by note groups which form gestural shapes. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between group onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg18 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg19 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg20 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg21 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg22 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg23 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg24 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg25 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg26 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg27 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg28 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg29 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg30 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg31 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg32 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg33 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg34 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Grouped 4 - A texture of separate event groups is shaped by random selections from parameter ranges, with one or more input sounds; On a given harmonic set specified in a text file"] = {
  cmds = { exe = "texture", mode = "grouped 4", channels = "1in2out", tip = "4 On changing harmonic sets. The idea behind TEXTURE GROUPED is easily grasped by imagining several quick bursts of note events separated by silence. A 'group' is a 'burst of notes', and the program handles creating silences inbetween bursts. The program is, however, more flexible than that because: the note groups can in fact overlap, the timing of the note onsets within a group doesn't have to be rapid, the pitch location where each group begins can vary. You can easily understand, therefore, how the program makes it possible to create textures which are more varied than those of TEXTURE SIMPLE, and, if desired, punctuated by note groups which form gestural shapes. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between group onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = 9600, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg18 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg19 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg20 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg21 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg22 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg23 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg24 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg25 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg26 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg27 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg28 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg29 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg30 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg31 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg32 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg33 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg34 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Grouped 5 - A texture of separate event groups is shaped by random selections from parameter ranges, with one or more input sounds; uses a text file"] = {
  cmds = { exe = "texture", mode = "grouped 5", channels = "1in2out", tip = "5 None . The idea behind TEXTURE GROUPED is easily grasped by imagining several quick bursts of note events separated by silence. A 'group' is a 'burst of notes', and the program handles creating silences inbetween bursts. The program is, however, more flexible than that because: the note groups can in fact overlap, the timing of the note onsets within a group doesn't have to be rapid, the pitch location where each group begins can vary. You can easily understand, therefore, how the program makes it possible to create textures which are more varied than those of TEXTURE SIMPLE, and, if desired, punctuated by note groups which form gestural shapes. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between group onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg18 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg19 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg20 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg21 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg22 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg23 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg24 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg25 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg26 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg27 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg28 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg29 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg30 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg31 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg32 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg33 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg34 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Motifsin 1 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On a given harmonic field,  uses a text file "] = {
  cmds = { exe = "texture", mode = "tmotifsin 1", channels = "1in2out", tip = "1 On a given harmonic field. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Motifsin 2 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On changing harmonic fields, uses a text file "] = {
  cmds = { exe = "texture", mode = "tmotifsin 2", channels = "1in2out", tip = "2 On changing harmonic fields. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Motifsin 3 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On a given harmonic set, uses a text file "] = {
  cmds = { exe = "texture", mode = "tmotifsin 3", channels = "1in2out", tip = "3 On a given harmonic set. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Motifsin 4 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On a given harmonic set, uses a text file "] = {
  cmds = { exe = "texture", mode = "tmotifsin 4", channels = "1in2out", tip = "4 On changing harmonic sets. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Ornate 1 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "ornate 1", channels = "1in2out", tip = "1 On a given harmonic field. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Ornate 2 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On changing harmonic fields from a textfile"] = {
  cmds = { exe = "texture", mode = "ornate 2", channels = "1in2out", tip = "2 On changing harmonic fields. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Ornate 3 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "ornate 3", channels = "1in2out", tip = "3 On a given harmonic set. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Ornate 4 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "ornate 4", channels = "1in2out", tip = "4 On changing harmonic sets. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Ornate 5 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "ornate 5", channels = "1in2out", tip = "5 None. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Postdecor 1 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "postdecor 1", channels = "1in2out", tip = "1 On a given harmonic field. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Postdecor 2 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "postdecor 2", channels = "1in2out", tip = "2 On changing harmonic fields. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Postdecor 3 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "postdecor 3", channels = "1in2out", tip = "3 On a given harmonic set. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Postdecor 4 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "postdecor 4", channels = "1in2out", tip = "4 On changing harmonic sets. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Postdecor 5 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; uses textfile"] = {
  cmds = { exe = "texture", mode = "postdecor 5", channels = "1in2out", tip = "5 None . The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Postornate 1 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "postornate 1", channels = "1in2out", tip = "1 On a given harmonic field. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Postornate 2 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On changing harmonic fields from a textfile"] = {
  cmds = { exe = "texture", mode = "postornate 2", channels = "1in2out", tip = "2 On changing harmonic fields. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Postornate 3 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "postornate 3", channels = "1in2out", tip = "3 On a given harmonic set. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Postornate 4 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "postornate 4", channels = "1in2out", tip = "4 On changing harmonic sets. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Postornate 5 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "postornate 5", channels = "1in2out", tip = "5 None. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Predecor 1 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "predecor 1", channels = "1in2out", tip = "1 On a given harmonic field. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Predecor 2 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "predecor 2", channels = "1in2out", tip = "2 On changing harmonic fields. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Predecor 3 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "predecor 3", channels = "1in2out", tip = "3 On a given harmonic set. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Predecor 4 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; On changing harmonic fields from textfile"] = {
  cmds = { exe = "texture", mode = "predecor 4", channels = "1in2out", tip = "4 On changing harmonic sets. The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Predecor 5 - A texture of 'decoration' events shaped by internal grouping parameters, selected at random from a pitch range or from a Harmonic Field/Set and attached to an underlying 'line'; uses textfile"] = {
  cmds = { exe = "texture", mode = "predecor 5", channels = "1in2out", tip = "5 None . The key to TEXTURE DECORATED is that the (ascending) start times defining the the 'line' of nodes to be decorated are important. The decorations are connected to the line centred (DECORATED), before and ending on (PREDECOR), and starting precisely on (POSTDECOR), these time points. Breakpoint times also have to tie in with these times or nothing will happen. Musically, this means that the decorations on a series of nodes can be varyingly constructed. Remember that a 'decoration' consists of notes chosen randomly from the decoration field, which can be more or less harmonic. This randomness is what differentiates DECOR and ORNATE. In ORNATE, there can be figures with internal rhythm and velocity fully defined and also attached to nodes. In MOTIFS, fully defined figures are used, but not a defined nodal substructure." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg19 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg20 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg21 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg22 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg23 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg24 = { name = "centring", min = 0, max = 7, def = 0, tip = "how the decoration pitches centre on line pitches (Range: 0 to 7; Default: 0)" },
  arg25 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg26 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg27 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg28 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg29 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg30 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg31 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg32 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg33 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
  arg34 = { name = "-k", switch = "-k", tip = "discard original line, after decoration" },
}
 
dsp["Texture Preornate 1 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic field from textfile"] = {
  cmds = { exe = "texture", mode = "preornate 1", channels = "1in2out", tip = "1 On a given harmonic field. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Preornate 2 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On changing harmonic fields from a textfile"] = {
  cmds = { exe = "texture", mode = "preornate 2", channels = "1in2out", tip = "2 On changing harmonic fields. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Preornate 3 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "preornate 3", channels = "1in2out", tip = "3 On a given harmonic set. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Preornate 4 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "preornate 4", channels = "1in2out", tip = "4 On changing harmonic sets. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Preornate 5 - A texture of events with fully user-specified ornaments placed on a 'line', optionally with pitches restricted to a Harmonic Field/Set; On a given harmonic set from a textfile"] = {
  cmds = { exe = "texture", mode = "preornate 5", channels = "1in2out", tip = "5 None. The salient feature in TEXTURE ORNATE is that an ornament is attached to an underlying substructure node. The ornament is a precisely defined musical figure: the start time, amplitude and duration fields in the ornament definition part of the note data file are all active. Individual ornament note durations may be longer than the time until the next ornament note start time, which makes fades and legato effects possible. Amplitude values will be important to shape the ornament rhythmically in an audible way." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile, for syntax see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = 10, def = 1, tip = "time between repeats of motif-to-decorate in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 100, def = 2, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 100, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
  arg27 = { name = "-h", switch = "-h", tip = "decorate topnote of chord (Default: decorate first note listed)" },
  arg28 = { name = "-e", switch = "-e", tip = "decorate all the notes of chords" },
}
 
dsp["Texture Simple 1 - A texture of events shaped by random selections from parameter ranges, with one or more input sounds; On a given harmonic field specified in text file"] = {
  cmds = { exe = "texture", mode = "simple 1", channels = "1in2out", tip = "1 On a given harmonic field - Musical applications; We can already see that this is an extremely powerful set of programs, which can undoubtedly be used in endlessly imaginative ways. For the moment, let us simply outline what we have learned so far. With the TEXTURE program group, we can create: a melodic line, with gaps and/or overlaps. METHOD: use Mode 4, specify specific start times for each note event in the ndata file, and set the packing to be about the same as the shortest time between notes. Note durations in the note data file can be zero (they are disregarded with harmonic fields or sets); note overlaps result from making the successive start times shorter than at least some of the note durations set with the parameter duration range. With the -w flag set, there may be even more overlapping if the sound's duration is longer than the packing setting, with possibilities for resonances and harmonies emerging from the melody. A melodic line, with a grainy, textured surface. METHOD: use Mode 4 and specify specific start times in the ndata file and set packing to be very short, certainly much shorter than the time between note events. A very close packing (e.g., 0.01) may cause amplitude overload. You can apply the gain reduction recommended by the program, or perhaps reduce the maxdur value to lessen the amount of note overlap. A texture of randomly selected notes drawn from a defined list of notes – i.e., these can, for example, form a sonorous chord when the packing is tight. METHOD: use Mode 3, set all start times in the ndata file to 0 and specify pitches as desired. A mindur – maxdur range will produce note overlaps (unless maxdur is less than the time gap between events). A texture which moves between two harmonic fields (Mode 2 all octaves) or sets (Mode 4 only the pitches specified). METHOD: in the ndata file, start all note events of the first set at one time, and all note events of the second set at another, later, time. A texture with envelope shapes, with pitches drawn from a defined harmonic set; time-varying packing can help make the shapes audible by changing the note density. METHOD: define harmonic sets, which may differ at different times. Then create time-varying group pitch range files to define the envelope shapes." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between event onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg18 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg19 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg20 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg21 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Simple 2 - A texture of events shaped by random selections from parameter ranges, with one or more input sounds; On a changing harmonic field specified in text file"] = {
  cmds = { exe = "texture", mode = "simple 2", channels = "1in2out", tip = "2 On changing harmonic fields - Musical applications; We can already see that this is an extremely powerful set of programs, which can undoubtedly be used in endlessly imaginative ways. For the moment, let us simply outline what we have learned so far. With the TEXTURE program group, we can create: a melodic line, with gaps and/or overlaps. METHOD: use Mode 4, specify specific start times for each note event in the ndata file, and set the packing to be about the same as the shortest time between notes. Note durations in the note data file can be zero (they are disregarded with harmonic fields or sets); note overlaps result from making the successive start times shorter than at least some of the note durations set with the parameter duration range. With the -w flag set, there may be even more overlapping if the sound's duration is longer than the packing setting, with possibilities for resonances and harmonies emerging from the melody. A melodic line, with a grainy, textured surface. METHOD: use Mode 4 and specify specific start times in the ndata file and set packing to be very short, certainly much shorter than the time between note events. A very close packing (e.g., 0.01) may cause amplitude overload. You can apply the gain reduction recommended by the program, or perhaps reduce the maxdur value to lessen the amount of note overlap. A texture of randomly selected notes drawn from a defined list of notes – i.e., these can, for example, form a sonorous chord when the packing is tight. METHOD: use Mode 3, set all start times in the ndata file to 0 and specify pitches as desired. A mindur – maxdur range will produce note overlaps (unless maxdur is less than the time gap between events). A texture which moves between two harmonic fields (Mode 2 all octaves) or sets (Mode 4 only the pitches specified). METHOD: in the ndata file, start all note events of the first set at one time, and all note events of the second set at another, later, time. A texture with envelope shapes, with pitches drawn from a defined harmonic set; time-varying packing can help make the shapes audible by changing the note density. METHOD: define harmonic sets, which may differ at different times. Then create time-varying group pitch range files to define the envelope shapes." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between event onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg18 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg19 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg20 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg21 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Simple 3 - A texture of events shaped by random selections from parameter ranges, with one or more input sounds; On a given harmonic set specified in a text file"] = {
  cmds = { exe = "texture", mode = "simple 3", channels = "1in2out", tip = "3 On a given harmonic set - Musical applications; We can already see that this is an extremely powerful set of programs, which can undoubtedly be used in endlessly imaginative ways. For the moment, let us simply outline what we have learned so far. With the TEXTURE program group, we can create: a melodic line, with gaps and/or overlaps. METHOD: use Mode 4, specify specific start times for each note event in the ndata file, and set the packing to be about the same as the shortest time between notes. Note durations in the note data file can be zero (they are disregarded with harmonic fields or sets); note overlaps result from making the successive start times shorter than at least some of the note durations set with the parameter duration range. With the -w flag set, there may be even more overlapping if the sound's duration is longer than the packing setting, with possibilities for resonances and harmonies emerging from the melody. A melodic line, with a grainy, textured surface. METHOD: use Mode 4 and specify specific start times in the ndata file and set packing to be very short, certainly much shorter than the time between note events. A very close packing (e.g., 0.01) may cause amplitude overload. You can apply the gain reduction recommended by the program, or perhaps reduce the maxdur value to lessen the amount of note overlap. A texture of randomly selected notes drawn from a defined list of notes – i.e., these can, for example, form a sonorous chord when the packing is tight. METHOD: use Mode 3, set all start times in the ndata file to 0 and specify pitches as desired. A mindur – maxdur range will produce note overlaps (unless maxdur is less than the time gap between events). A texture which moves between two harmonic fields (Mode 2 all octaves) or sets (Mode 4 only the pitches specified). METHOD: in the ndata file, start all note events of the first set at one time, and all note events of the second set at another, later, time. A texture with envelope shapes, with pitches drawn from a defined harmonic set; time-varying packing can help make the shapes audible by changing the note density. METHOD: define harmonic sets, which may differ at different times. Then create time-varying group pitch range files to define the envelope shapes." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between event onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg18 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg19 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg20 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg21 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Simple 4 - A texture of events shaped by random selections from parameter ranges, with one or more input sounds; On changing harmonic sets specified in a text file"] = {
  cmds = { exe = "texture", mode = "simple 4", channels = "1in2out", tip = "4 On changing harmonic sets - Musical applications; We can already see that this is an extremely powerful set of programs, which can undoubtedly be used in endlessly imaginative ways. For the moment, let us simply outline what we have learned so far. With the TEXTURE program group, we can create: a melodic line, with gaps and/or overlaps. METHOD: use Mode 4, specify specific start times for each note event in the ndata file, and set the packing to be about the same as the shortest time between notes. Note durations in the note data file can be zero (they are disregarded with harmonic fields or sets); note overlaps result from making the successive start times shorter than at least some of the note durations set with the parameter duration range. With the -w flag set, there may be even more overlapping if the sound's duration is longer than the packing setting, with possibilities for resonances and harmonies emerging from the melody. A melodic line, with a grainy, textured surface. METHOD: use Mode 4 and specify specific start times in the ndata file and set packing to be very short, certainly much shorter than the time between note events. A very close packing (e.g., 0.01) may cause amplitude overload. You can apply the gain reduction recommended by the program, or perhaps reduce the maxdur value to lessen the amount of note overlap. A texture of randomly selected notes drawn from a defined list of notes – i.e., these can, for example, form a sonorous chord when the packing is tight. METHOD: use Mode 3, set all start times in the ndata file to 0 and specify pitches as desired. A mindur – maxdur range will produce note overlaps (unless maxdur is less than the time gap between events). A texture which moves between two harmonic fields (Mode 2 all octaves) or sets (Mode 4 only the pitches specified). METHOD: in the ndata file, start all note events of the first set at one time, and all note events of the second set at another, later, time. A texture with envelope shapes, with pitches drawn from a defined harmonic set; time-varying packing can help make the shapes audible by changing the note density. METHOD: define harmonic sets, which may differ at different times. Then create time-varying group pitch range files to define the envelope shapes." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between event onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg18 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg19 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg20 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg21 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Simple 5 - A texture of events shaped by random selections from parameter ranges, with one or more input sounds; uses a text file"] = {
  cmds = { exe = "texture", mode = "simple 5", channels = "1in2out", tip = "5 None (Neutral) - Musical applications; We can already see that this is an extremely powerful set of programs, which can undoubtedly be used in endlessly imaginative ways. For the moment, let us simply outline what we have learned so far. With the TEXTURE program group, we can create: a melodic line, with gaps and/or overlaps. METHOD: use Mode 4, specify specific start times for each note event in the ndata file, and set the packing to be about the same as the shortest time between notes. Note durations in the note data file can be zero (they are disregarded with harmonic fields or sets); note overlaps result from making the successive start times shorter than at least some of the note durations set with the parameter duration range. With the -w flag set, there may be even more overlapping if the sound's duration is longer than the packing setting, with possibilities for resonances and harmonies emerging from the melody. A melodic line, with a grainy, textured surface. METHOD: use Mode 4 and specify specific start times in the ndata file and set packing to be very short, certainly much shorter than the time between note events. A very close packing (e.g., 0.01) may cause amplitude overload. You can apply the gain reduction recommended by the program, or perhaps reduce the maxdur value to lessen the amount of note overlap. A texture of randomly selected notes drawn from a defined list of notes – i.e., these can, for example, form a sonorous chord when the packing is tight. METHOD: use Mode 3, set all start times in the ndata file to 0 and specify pitches as desired. A mindur – maxdur range will produce note overlaps (unless maxdur is less than the time gap between events). A texture which moves between two harmonic fields (Mode 2 all octaves) or sets (Mode 4 only the pitches specified). METHOD: in the ndata file, start all note events of the first set at one time, and all note events of the second set at another, later, time. A texture with envelope shapes, with pitches drawn from a defined harmonic set; time-varying packing can help make the shapes audible by changing the note density. METHOD: define harmonic sets, which may differ at different times. Then create time-varying group pitch range files to define the envelope shapes." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "packing", min = 2.3e-05, max = length/1000, tip = "(average) time in seconds between event onsets" },
  arg7 = { name = "scatter", min = 0, max = 10, def = 1, tip = "randomisation in seconds of event onsets (Range: 0 to 10)" },
  arg8 = { name = "tgrid", min = 0, max = 10000, def = 0, tip = "minimum step in milliseconds or quantised timegrid (for group start times) (Range: 0 to 10000; Default: 0)" },
  arg9 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg10 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg11 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg12 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg13 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg14 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg15 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg16 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg17 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg18 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg19 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg20 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg21 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Tgrouped 1 - A texture with the onsets of separate internally shaped event groups constrained to a rhythmic template, with pitches drawn from a pitch range or a Harmonic Field/Set; On a given harmonic field specified in text file"] = {
  cmds = { exe = "texture", mode = "tgrouped 1", channels = "1in2out", tip = "1 On a given harmonic field. TEXTURE TGROUPS is a natural (and wonderful) development of the rhythmic template idea in TEXTURE TIMED. In this case, the template times replace the packing parameter as the time-onsets of each group. Thus the groups can not only follow one another at regular time onsets, but these onsets can be have a defined rhythm. The groups themselves are shaped just as they are in TEXTURE GROUPED. Once the idea is clear that the time-onsets of each group follow the rhythmic template defined in the first part of the note data file, understanding TGROUPED follows easily. We shall illustrate this by using the parameters in Mode 5 to make the rhythmic template audible, and then 'populate' it with groups. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg16 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg17 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg18 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg19 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg20 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg21 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg22 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg23 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg24 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg25 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg26 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg27 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg28 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg29 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg30 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg31 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg32 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tgrouped 2 - A texture with the onsets of separate internally shaped event groups constrained to a rhythmic template, with pitches drawn from a pitch range or a Harmonic Field/Set; On changing harmonic fields specified in text file"] = {
  cmds = { exe = "texture", mode = "tgrouped 2", channels = "1in2out", tip = "2 On changing harmonic fields. TEXTURE TGROUPS is a natural (and wonderful) development of the rhythmic template idea in TEXTURE TIMED. In this case, the template times replace the packing parameter as the time-onsets of each group. Thus the groups can not only follow one another at regular time onsets, but these onsets can be have a defined rhythm. The groups themselves are shaped just as they are in TEXTURE GROUPED. Once the idea is clear that the time-onsets of each group follow the rhythmic template defined in the first part of the note data file, understanding TGROUPED follows easily. We shall illustrate this by using the parameters in Mode 5 to make the rhythmic template audible, and then 'populate' it with groups. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg16 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg17 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg18 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg19 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg20 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg21 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg22 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg23 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg24 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg25 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg26 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg27 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg28 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg29 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg30 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg31 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg32 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tgrouped 3 - A texture with the onsets of separate internally shaped event groups constrained to a rhythmic template, with pitches drawn from a pitch range or a Harmonic Field/Set; On a given harmonic set specified in text file"] = {
  cmds = { exe = "texture", mode = "tgrouped 3", channels = "1in2out", tip = "3 On a given harmonic set. TEXTURE TGROUPS is a natural (and wonderful) development of the rhythmic template idea in TEXTURE TIMED. In this case, the template times replace the packing parameter as the time-onsets of each group. Thus the groups can not only follow one another at regular time onsets, but these onsets can be have a defined rhythm. The groups themselves are shaped just as they are in TEXTURE GROUPED. Once the idea is clear that the time-onsets of each group follow the rhythmic template defined in the first part of the note data file, understanding TGROUPED follows easily. We shall illustrate this by using the parameters in Mode 5 to make the rhythmic template audible, and then 'populate' it with groups. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg16 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg17 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg18 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg19 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg20 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg21 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg22 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg23 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg24 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg25 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg26 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg27 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg28 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg29 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg30 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg31 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg32 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tgrouped 4 - A texture with the onsets of separate internally shaped event groups constrained to a rhythmic template, with pitches drawn from a pitch range or a Harmonic Field/Set; On changing harmonic sets specified in text file"] = {
  cmds = { exe = "texture", mode = "tgrouped 4", channels = "1in2out", tip = "4 On changing harmonic sets. TEXTURE TGROUPS is a natural (and wonderful) development of the rhythmic template idea in TEXTURE TIMED. In this case, the template times replace the packing parameter as the time-onsets of each group. Thus the groups can not only follow one another at regular time onsets, but these onsets can be have a defined rhythm. The groups themselves are shaped just as they are in TEXTURE GROUPED. Once the idea is clear that the time-onsets of each group follow the rhythmic template defined in the first part of the note data file, understanding TGROUPED follows easily. We shall illustrate this by using the parameters in Mode 5 to make the rhythmic template audible, and then 'populate' it with groups. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg16 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg17 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg18 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg19 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg20 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg21 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg22 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg23 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg24 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg25 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg26 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg27 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg28 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg29 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg30 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg31 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg32 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tgrouped 5 - A texture with the onsets of separate internally shaped event groups constrained to a rhythmic template, with pitches drawn from a pitch range or a Harmonic Field/Set; uses a text file"] = {
  cmds = { exe = "texture", mode = "tgrouped 5", channels = "1in2out", tip = "5 None. TEXTURE TGROUPS is a natural (and wonderful) development of the rhythmic template idea in TEXTURE TIMED. In this case, the template times replace the packing parameter as the time-onsets of each group. Thus the groups can not only follow one another at regular time onsets, but these onsets can be have a defined rhythm. The groups themselves are shaped just as they are in TEXTURE GROUPED. Once the idea is clear that the time-onsets of each group follow the rhythmic template defined in the first part of the note data file, understanding TGROUPED follows easily. We shall illustrate this by using the parameters in Mode 5 to make the rhythmic template audible, and then 'populate' it with groups. " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing assumed MIDI 'pitch' of each input sound, specified on the 1st line & NOTELIST to define a Harmonic Field/Set." },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg16 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg17 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg18 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg19 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg20 = { name = "gpsizelo", min = 0, max = 1000, def = 1, tip = "smallest number of events in any given group" },
  arg21 = { name = "gpsizehi", min = 0, max = 1000, def = 10, tip = "largest number of events in any given group" },
  arg22 = { name = "gppaklo", min = 0.023, max = 60000, def = 0.023, tip = "shortest time between event-onsets within a group" },
  arg23 = { name = "gppakhi", min = 0.023, max = 60000, def = 1000, tip = "longest time between event-onsets within a group" },
  arg24 = { name = "gpranglo", min = 1, max = 127, def = 1, tip = "minimum pitch range for the note events in any given group" },
  arg25 = { name = "gpranghi", min = 1, max = 127, def = 127, tip = "maximum pitch range for the note events in any given group" },
  arg26 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg27 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg28 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg29 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg30 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg31 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg32 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Timed 1 - A texture with events constrained to a rhythmic template and pitches selected at random from a pitch range or a Harmonic Field/Set; On a given harmonic field specified in a text file "] = {
  cmds = { exe = "texture", mode = "timed 1", channels = "1in2out", tip = "1 On a given harmonic field. TEXTURE TIMED repeats a series of notes in random order but locked into a defined rhythmic template. The key to this process lies in realising that the first part of the note data file consists of note events in which only the onset timing is relevant (the other three fields need something in them, however, 1 for the instr_no and usually zeros for the other fields). You are therefore specifying a rhythmic template which can be filled with any sequence of pitches. Musical applications; TEXTURE TIMED requires a very specific set of note event timings, and yet makes its note selections randomly. It is therefore it is well suited to to slower and very distinctive rhythmic ideas. The random reworking of the note selections can be a bit too obvious, however, so, to mitigate this, you might consider using longer note durations, a fairly large and sonorous harmonic field or set, and possibly several input soundfiles. Alternatively, a vigorous texture of fast, repeated rhythms can be obtained with very closely placed timings and a skiptime equal to the length you want the note of the last rhythmic motif note event to have. Then the next motif will begin with a gap. If skiptime is near zero (min skiptime is 0.000002), there is no time allowance for the last note event of the motif, and the last note of one motif and the first of the next motif will be virtually simultaneous." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing stuff, see CDP docs!" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = length/1000, tip = "time between repetitions of timing motif in the note data file (from the end of one motif to the beginning of the next)" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg16 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg17 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg18 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg19 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Timed 2 - A texture with events constrained to a rhythmic template and pitches selected at random from a pitch range or a Harmonic Field/Set; On changing harmonic fields specified in a text file "] = {
  cmds = { exe = "texture", mode = "timed 2", channels = "1in2out", tip = "2 On changing harmonic fields. TEXTURE TIMED repeats a series of notes in random order but locked into a defined rhythmic template. The key to this process lies in realising that the first part of the note data file consists of note events in which only the onset timing is relevant (the other three fields need something in them, however, 1 for the instr_no and usually zeros for the other fields). You are therefore specifying a rhythmic template which can be filled with any sequence of pitches. Musical applications; TEXTURE TIMED requires a very specific set of note event timings, and yet makes its note selections randomly. It is therefore it is well suited to to slower and very distinctive rhythmic ideas. The random reworking of the note selections can be a bit too obvious, however, so, to mitigate this, you might consider using longer note durations, a fairly large and sonorous harmonic field or set, and possibly several input soundfiles. Alternatively, a vigorous texture of fast, repeated rhythms can be obtained with very closely placed timings and a skiptime equal to the length you want the note of the last rhythmic motif note event to have. Then the next motif will begin with a gap. If skiptime is near zero (min skiptime is 0.000002), there is no time allowance for the last note event of the motif, and the last note of one motif and the first of the next motif will be virtually simultaneous." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing stuff, see CDP docs!" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = length/1000, tip = "time between repetitions of timing motif in the note data file (from the end of one motif to the beginning of the next)" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg16 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg17 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg18 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg19 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Timed 3 - A texture with events constrained to a rhythmic template and pitches selected at random from a pitch range or a Harmonic Field/Set; On a given harmonic set specified in a text file "] = {
  cmds = { exe = "texture", mode = "timed 3", channels = "1in2out", tip = "3 On a given harmonic set.TEXTURE TIMED repeats a series of notes in random order but locked into a defined rhythmic template. The key to this process lies in realising that the first part of the note data file consists of note events in which only the onset timing is relevant (the other three fields need something in them, however, 1 for the instr_no and usually zeros for the other fields). You are therefore specifying a rhythmic template which can be filled with any sequence of pitches. Musical applications; TEXTURE TIMED requires a very specific set of note event timings, and yet makes its note selections randomly. It is therefore it is well suited to to slower and very distinctive rhythmic ideas. The random reworking of the note selections can be a bit too obvious, however, so, to mitigate this, you might consider using longer note durations, a fairly large and sonorous harmonic field or set, and possibly several input soundfiles. Alternatively, a vigorous texture of fast, repeated rhythms can be obtained with very closely placed timings and a skiptime equal to the length you want the note of the last rhythmic motif note event to have. Then the next motif will begin with a gap. If skiptime is near zero (min skiptime is 0.000002), there is no time allowance for the last note event of the motif, and the last note of one motif and the first of the next motif will be virtually simultaneous." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing stuff, see CDP docs!" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = length/1000, tip = "time between repetitions of timing motif in the note data file (from the end of one motif to the beginning of the next)" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg16 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg17 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg18 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg19 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Timed 4 - A texture with events constrained to a rhythmic template and pitches selected at random from a pitch range or a Harmonic Field/Set; On changing harmonic sets specified in a text file "] = {
  cmds = { exe = "texture", mode = "timed 4", channels = "1in2out", tip = "4 On changing harmonic sets.TEXTURE TIMED repeats a series of notes in random order but locked into a defined rhythmic template. The key to this process lies in realising that the first part of the note data file consists of note events in which only the onset timing is relevant (the other three fields need something in them, however, 1 for the instr_no and usually zeros for the other fields). You are therefore specifying a rhythmic template which can be filled with any sequence of pitches. Musical applications; TEXTURE TIMED requires a very specific set of note event timings, and yet makes its note selections randomly. It is therefore it is well suited to to slower and very distinctive rhythmic ideas. The random reworking of the note selections can be a bit too obvious, however, so, to mitigate this, you might consider using longer note durations, a fairly large and sonorous harmonic field or set, and possibly several input soundfiles. Alternatively, a vigorous texture of fast, repeated rhythms can be obtained with very closely placed timings and a skiptime equal to the length you want the note of the last rhythmic motif note event to have. Then the next motif will begin with a gap. If skiptime is near zero (min skiptime is 0.000002), there is no time allowance for the last note event of the motif, and the last note of one motif and the first of the next motif will be virtually simultaneous." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing stuff, see CDP docs!" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = length/1000, tip = "time between repetitions of timing motif in the note data file (from the end of one motif to the beginning of the next)" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg16 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg17 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg18 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg19 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Timed 5 - A texture with events constrained to a rhythmic template and pitches selected at random from a pitch range or a Harmonic Field/Set; uses a text file "] = {
  cmds = { exe = "texture", mode = "timed 5", channels = "1in2out", tip = "5 None .TEXTURE TIMED repeats a series of notes in random order but locked into a defined rhythmic template. The key to this process lies in realising that the first part of the note data file consists of note events in which only the onset timing is relevant (the other three fields need something in them, however, 1 for the instr_no and usually zeros for the other fields). You are therefore specifying a rhythmic template which can be filled with any sequence of pitches. Musical applications; TEXTURE TIMED requires a very specific set of note event timings, and yet makes its note selections randomly. It is therefore it is well suited to to slower and very distinctive rhythmic ideas. The random reworking of the note selections can be a bit too obvious, however, so, to mitigate this, you might consider using longer note durations, a fairly large and sonorous harmonic field or set, and possibly several input soundfiles. Alternatively, a vigorous texture of fast, repeated rhythms can be obtained with very closely placed timings and a skiptime equal to the length you want the note of the last rhythmic motif note event to have. Then the next motif will begin with a gap. If skiptime is near zero (min skiptime is 0.000002), there is no time allowance for the last note event of the motif, and the last note of one motif and the first of the next motif will be virtually simultaneous." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing stuff, see CDP docs!" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skiptime", min = 0, max = length/1000, tip = "time between repetitions of timing motif in the note data file (from the end of one motif to the beginning of the next)" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "mindur", min = 0.016, max = length/1000, def = 0.016, tip = "minimum duration of events in texture" },
  arg12 = { name = "maxdur", min = 0, max = length/1000, def = length/1000, tip = "maximum duration of events in texture" },
  arg13 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg14 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg15 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg16 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg17 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg18 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg19 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
}
 
dsp["Texture Tmotifs 1 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On a given harmonic field from a text file "] = {
  cmds = { exe = "texture", mode = "tmotifs 1", channels = "1in2out", tip = "1 On a given harmonic field. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tmotifs 2 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On changing harmonic fields from a text file "] = {
  cmds = { exe = "texture", mode = "tmotifs 2", channels = "1in2out", tip = "2 On changing harmonic fields. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tmotifs 3 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On a given harmonic set from a text file "] = {
  cmds = { exe = "texture", mode = "tmotifs 3", channels = "1in2out", tip = "3 On a given harmonic set. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tmotifs 4 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; On changing harmonic sets from a text file "] = {
  cmds = { exe = "texture", mode = "tmotifs 4", channels = "1in2out", tip = "4 On changing harmonic sets. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
dsp["Texture Tmotifs 5 - A texture with the onsets of fully user-specified motifs constrained to a rhythmic grid and attached to pitches drawn from a pitch range or from a Harmonic Field/Set; uses a text file "] = {
  cmds = { exe = "texture", mode = "tmotifs 5", channels = "1in2out", tip = "5 none. TEXTURE TMOTIFS enables us to create fully defined motifs which begin to play according to the times in a rhythmic template. These will draw their pitches either from a pitch range (Mode 5) or from a Harmonic Field/Set (Modes 1 - 4). In TGROUPED we found that the rhythmic template repeated without very much definition because we could not set the amplitude at specific time points. TMOTIFS enables us to create fully defined musical figures, thus providing a useful spectrum in the 'timed' group of functions from random to fully specified.  The range of possibilities in the TEXTURE set provides facilities for remarkably varied musical situations. In particular, there is a balance between random selection and fully defined musical figures. Always there are constraints within which random selections are made, such as parameter ranges and harmonic fields or sets, and all of these can vary through time. In addition you are not constrained to use the pitches of the tempered scale – fractional MIDI values will give you any tuning you desire. Thus, while it is possible to duplicate MIDI sequencer type fully defined pitch and rhythmic relationships, it is also possible to use the algorithmic potential of this software to move beyond towards freer forms of texture design, designs often very suitable for sounds of a more 'natural' character, i.e., with limited pitch content. For example, you may move from a dense apparently unpitched texture, (using a very small packing time and a wide pitch range, but with no harmonic field specified – Mode 5) then gradually narrow the pitch range to (close to) a single pitch. In this way the unpitched band becomes gradually focused on a pitch (provided your source material has some pitch content). You can also take any sound of complex quality (e.g. speech) and by creating a very dense texture by using a very small packing time, 'white out' the texture, producing a band of undifferentiated noise. By varying the packing time you can then pass between the noise band and a texture of voices. Thus the DECORATED group defines a 'line' (nodal substructure), but the decorations themselves are formed by (constrained) random selections. Complementing this is the ORNATE group, which enables you to place fully defined figures ('ornaments') on this 'line', with the option to constrain the pitches of the figure(s) to a harmonic grid. Similarly, MOTIFS makes fully defined motifs possible, but the 'line' feature is removed. Thus the motifs will begin either on a pitch selected at random from a pitch range, or selected at random from a harmonic grid. In MOTIFS only the first pitch of the motif is 'forced' onto this grid, and in MOTIFSIN, all the pitches of the motif are forced onto the grid, warping the figure if necessary to do so. TMOTIFS/TMOTIFSIN complements the 'line' feature of DECORATED and ORNATE with a timing grid, so that the motifs can be made to begin at specified times. This opens up possibilities such as controlled overlaps, variations in density, and interval control. The relationship between the timing grid, the intervals in the harmonic grid, the intervals in the motif itself, the time between the onset of the last motif on the grid and the rerun of the grid pattern (skiptime), and the tempo control (the motif duration multiplier multlo/hi) – all affect the overall result and provide a wonderfully flexible environment for designing musical textures. Furthermore, as we are working in the context of sound transformation software, it will be normal to apply the designs to inharmonic input soundfiles with little or no clearly defined pitch content. In this case the timing and pitch contours control 'washes' of sound and timbral colour.   " },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "notedata", input = "txt", tip = "textfile containing bunch of stuff, see CDP docs" },
  arg5 = { name = "outdur", min = 0, max = 60, def = 10, tip = "minimum duration of the outfile" },
  arg6 = { name = "skip", min = 0, max = length/1000, tip = "time between repeats of timing motif in notedata, i.e., between runs of the 'line' substructure notelist" },
  arg7 = { name = "sndfirst", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg8 = { name = "sndlast", min = 1, max = 2, input = "brk", def = 1, tip = "first and last soundfiles to use from a list of soundfiles for input (Range: 1 to the number of sounds)." },
  arg9 = { name = "mingain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg10 = { name = "maxgain", min = 1, max = 127, def = 64, tip = "minimum level of input sounds (Range: 1 to 127; Default: 64 and 64)" },
  arg11 = { name = "minpich", min = 1, max = 127, def = 1, tip = "minimum pitch (MIDI note value)" },
  arg12 = { name = "maxpich", min = 1, max = 127, def = 127, tip = "maximum pitch (MIDI note value)" },
  arg13 = { name = "phgrid", min = 0, max = 1000, def = 10, tip = "a timegrid in milliseconds applying WITHIN the groups (Range: 0.0 to 1000.0)" },
  arg14 = { name = "gpspace", min = 0, max = 5, def = 1, tip = "spatialisation of event-groups" },
  arg15 = { name = "gpsprange", min = 0, max = 1, def = 1, tip = "spatial range of event-groups (Range: 0 to 1; Default: 1)" },
  arg16 = { name = "amprise", min = 0, max = 127, def = 0, tip = "amplitude change within groups (Range: 0 to 127; Default: 0)" },
  arg17 = { name = "contour", min = 0, max = 6, def = 0, tip = "amplitude contour of groups (Range: 0 to 6; Default: 0) " },
  arg18 = { name = "multlo", min = 0, max = 1000, def = 1, tip = "smallest multiplier of total input duration of motif" },
  arg19 = { name = "multhi", min = 0, max = 1000, def = 10, tip = "largest multiplier of total input duration of motif" },
  arg20 = { name = "atten", switch = "-a", min = 0, max = 100, def = 0.9, tip = "overall attenuation of the output" },
  arg21 = { name = "position", switch = "-p", min = 0, max = 1, def = 0.5, tip = "centre of sound output image (Range: 0 to 1, where 0 is Left and 1 is Right; Default: 0.5)" },
  arg22 = { name = "spread", switch = "-s", min = 0, max = 1, def = 0.1, tip = "spatial spread of texture events (Range: 0 to 1, where 1 is full spread)" },
  arg23 = { name = "seed", switch = "-r", min = 0, max = 10, def = 0, tip = "the same seed number will produce the same result on rerun (Default: 0, where 0 is different result each time)" },
  arg24 = { name = "-w", switch = "-w", tip = "always play whole input sound, ignoring duration values" },
  arg25 = { name = "-d", switch = "-d", tip = "fixed timestep between groupnotes" },
  arg26 = { name = "-i", switch = "-i", tip = "each group not confined to a fixed instrument (Default: fixed)" },
}
 
------------------------------------------------
-- topantail2
------------------------------------------------
 
dsp["Topantail2 Topantail - Fade both the beginning and end of a sound"] = {
  cmds = { exe = "topantail2", mode = "topantail", channels = "any", tip = "TOPANTAIL2 provides another approach to extracting part of a soundfile by using gate levels. In this case it trims away signal before and after the extracted portion. The 'backtracking' function moves the initial cut point earlier in the sound, thus retaining some of the sound which may be below the startgate's level." },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "startgate", min = 0, max = 1, tip = "level at start before which sound is to be removed. (Range: 0 to 1, Default: 0.001)" },
  arg4 = { name = "endgate", min = 0, max = 1, tip = "level at end after which sound is to be removed. (Range: 0 to 1, Default: 0.001)" },
  arg5 = { name = "splicelen", switch = "-s", min = 0, max = 1000, tip = "splice length in milliseconds" },
  arg6 = { name = "backtrack", switch = "-b", min = 0, max = 1000, tip = "backtrack in time from the initial gate point(found by the program) to an earlier splice point, in milliseconds" },
}
 
------------------------------------------------
-- tremolo
------------------------------------------------
 
dsp["Tremolo Tremolo - Apply width-controlled tremolo to a soundfile linearly"] = {
  cmds = { exe = "tremolo", mode = "tremolo 1", channels = "any", tip = "This applies tremolo to a soundfile with the added ability to narrow ('squeeze') the width of the tremolo segments " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 0, max = 500, input = "brk", tip = "frequency of the tremolo (a low frequency oscillation: Range: 0 to 500)" },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", tip = "frequency displacement of the oscillation (Range: 0 to 1; the Default is 0.250)" },
  arg5 = { name = "gain", min = 0, max = 1, input = "brk", tip = "the overall signal gain, or envelope (Range 0 to 1; the Default is 1)" },
  arg6 = { name = "fineness", min = 1, max = 10, tip = "squeeze the width of the tremolo: an integer value >= 1" },
}
 
dsp["Tremolo Tremolo - Apply width-controlled tremolo to a soundfile logaritmically"] = {
  cmds = { exe = "tremolo", mode = "tremolo 2", channels = "any", tip = "This applies tremolo to a soundfile with the added ability to narrow ('squeeze') the width of the tremolo segments " },
  arg1 = { name = "Input", input = "wav", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg3 = { name = "freq", min = 0, max = 500, input = "brk", def = 50, tip = "frequency of the tremolo (a low frequency oscillation: Range: 0 to 500)" },
  arg4 = { name = "depth", min = 0, max = 1, input = "brk", tip = "frequency displacement of the oscillation (Range: 0 to 1; the Default is 0.250)" },
  arg5 = { name = "gain", min = 0, max = 1, input = "brk", def = 1, tip = "the overall signal gain, or envelope (Range 0 to 1; the Default is 1)" },
  arg6 = { name = "fineness", min = 1, max = 10, tip = "squeeze the width of the tremolo: an integer value >= 1" },
}
 
------------------------------------------------
-- tunevary
------------------------------------------------
 
dsp["Tunevary - Replace spectral frequencies with the harmonics of specified pitch(es), uses pitch template text file"] = {
  cmds = { exe = "tunevary", mode = "tunevary", tip = "This complementary spectral program builds on and combines aspects of PITCH TUNE and FILTER VARIBANK. Thus it can tune a spectrum to the specified pitches (given that those pitches are in the analysis file, otherwise, it skips or approximates them), AND it can do so in a time-varying way. Thus the pitch template file also contains times. TUNEVARY can therefore be seen as an extension of PITCH TUNE." },
  arg1 = { name = "Input", input = "ana", tip = "Select the input sound to the process" },
  arg2 = { name = "Output", output = "ana", tip = "Select the output sound to the process" },
  arg3 = { name = "pitch_template", input = "txt", tip = "textfile containing pitch data as a list of lines, each line containing a time followed by a (possibly fractional) MIDI pitch value(s)." },
  arg4 = { name = "focus", switch = "-f", min = 0, max = 1, input = "brk", def = 1, tip = "determines the degree to which partial pitches are focused onto the template. Range: 0 to 1." },
  arg5 = { name = "clarity", switch = "-c", min = 0, max = 1, input = "brk", def = 0, tip = "determines the degree to which non-template partials are suppressed. Range: 0 to 1" },
  arg6 = { name = "trace", switch = "-t", min = 1, max = 12000, input = "brk", def = 512, tip = "specifies window-by-window the number of most prominent spectral channels to be replaced by the template pitches. (Range: 1 to number of channels)" },
  arg7 = { name = "bcut", switch = "-b", min = 0, max = 22050, input = "brk", def = 50, tip = "instructs the program to ignore any pitches below bcut, the Bass cutoff frequency given in Hz." },
}
 
------------------------------------------------
-- wrappage
------------------------------------------------

--[[ afai understand it from the docs this processes produces a multi-channel outfile = >=2 files? 
dsp["Wrappage - Granular reconstitution of one or more soundfiles over multi-channel space "] = {
  cmds = { exe = "wrappage", mode = "", channels = "1in2out", tip = "How is a 'moving' multi-channel output is achieved? T Wishart writes: The centre parameter of WRAPPAGE defines spatial position: i.e., the channel number at which the output sound field is centred. This centre can be made to move from one output channel to another by providing a data file of time centre direction value-triples. For example: time centre direction 0    2      1, 2    4     -1, 5    2      1. The centre of the image starts (time 0 on line 1) at channel 2, and moves clockwise: the last '1' on line 1 indicates clockwise motion. At 2 seconds (line 2) it reaches channel 4, and then begins to move anticlockwise (the '-1' on line 2), until, at 5 seconds (line 3) it arrives at channel 2. The spread parameter of WRAPPAGE defines spatial spread: the width of the sound image, i.e., how many adjacent channels, or loudspeakers across which it spreads around this centre. Thus, for example, an image centred at 4 with a spread of 3 (in an 8-channel output space) would be spread across channels 3, 4 and 5. The moving image of 'wrappage' works in exactly the same way as that in MCHANPAN, but is in no way dependent on it. WRAPPAGE produces the spatial motion itself. There is a difference, because MCHANPAN Mode 1 is moving a single (mono) source around the multi-channel space whereas WRAPPAGE is generating a sound-image of textured sounds which can not only move, but can change in width (as it moves or even when its' centre is stationary. The wrappage sound image could fill the entire multi-channel space (centre it anywhere and, in an N-channel space, make the spread N), and this image could even rotate (keep the spread at maximum and move the centre). However, unless the sound elements of the texture were themselves long, this rotation might not be noticed: i.e., with very short sounds, it would not be possible to tell whether the whole texture was rotating or whether the individual elements were simply occuring in different positions, sounding much like a non-rotating texture of the same materials. Musical Applications; This program is essentially the same as MODIFY BRASSAGE in that it produces granulated soundfile output. The difference lies in its multi-channel facilities, particularly the movement patterns that can be achieved with a breakpoint file input for the centre parameter: defining with value triples the time, the centre position of the sound image (the channel), and the direction of movement. A multi-channel output soundfile of outchans number of channels can be created." },
  arg1 = { name = "Input 1", input = "wav", tip = "Select the first input sound to the process" },
  arg2 = { name = "Input 2", input = "wav", tip = "Select the second input sound to the process" },
  arg3 = { name = "Output", output = "wav", tip = "Select the output sound to the process" },
  arg4 = { name = "centre", min = 0, max = 2, input = "brk", def = 1, tip = "central position of the output sound field image in the outfile (Range 0 to outchans)." },
  arg5 = { name = "outchans", min = 2, max = 2, def = 2, tip = "the number of channels in the output soundfile.- only stereo supported in Renoise" },
  arg6 = { name = "spread", min = 0, max = 10000, input = "brk", def = 10, tip = "the width (from far Left to far Right) of spatialisation around centre" },
  arg7 = { name = "depth", min = 0, max = 20000, input = "brk", tip = "the number of channels with sound, behind the leading edges of spread." },
  arg8 = { name = "velocity", min = 0, max = 1000, input = "brk", def = 1, tip = "the speed of advance within the infile(s), relative to outfile. This value must be >= 0." },
  arg9 = { name = "hvelocity", min = 0, max = 1000, input = "brk", tip = "the time delay between identical components" },
  arg10 = { name = "density", min = 0, max = 1, input = "brk", tip = "grain overlap. Values > 0 and < 1 leave intergrain silence. Extremely small values don't perform predictably" },
  arg11 = { name = "hdensity", min = 0, max = 1, input = "brk", tip = "grain overlap. Values > 0 and < 1 leave intergrain silence. Extremely small values don't perform predictably" },
  arg12 = { name = "grainsize", min = 0, max = 20000, input = "brk", def = 50, tip = "grainsize in milliseconds (must be > 2 * splicelen). (Default 50)" },
  arg13 = { name = "hgrainsize", min = 0, max = 20000, input = "brk", tip = "grainsize in milliseconds (must be > 2 * splicelen). (Default 50)" },
  arg14 = { name = "pitchshift", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitchshift in + or - (fractions of) semitones." },
  arg15 = { name = "hpitchshift", min = -1000, max = 1000, input = "brk", def = 0, tip = "pitchshift in + or - (fractions of) semitones." },
  arg16 = { name = "amp", min = 0, max = 1, input = "brk", def = 0.5, tip = "gain on grains (Range 0 to 1) (Default 1.0). Use if amplitude variation is required (over a range &/or in time)." },
  arg17 = { name = "hamp", min = 0, max = 1, input = "brk", tip = "gain on grains (Range 0 to 1) (Default 1.0). Use if amplitude variation is required (over a range &/or in time)." },
  arg18 = { name = "bsplice", min = 0, max = length, input = "brk", def = 5, tip = "grain-startsplice length,in milliseconds (Default 5)" },
  arg19 = { name = "hbsplice", min = 0, max = length, input = "brk", def = 5, tip = "grain-startsplice length,in milliseconds (Default 5)" },
  arg20 = { name = "esplice", min = 0, max = length, input = "brk", def = 5, tip = "grain-endsplice length,in milliseconds (Default 5)" },
  arg21 = { name = "hesplice", min = 0, max = length, input = "brk", def = 5, tip = "grain-endsplice length,in milliseconds (Default 5)" },
  arg22 = { name = "range", min = 0, max = length/1000, input = "brk", def = 0, tip = "the length of time in milliseconds before 'now' in the infile in which the search for the next grain will take place (Default 0: stay at 'now')" },
  arg23 = { name = "jitter", min = 0, max = 1, input = "brk", def = 0.5, tip = "randomisation of grain position (Range 0-1) (Default 0.5)" },
  arg24 = { name = "outlength", min = 0, max = 60, def = 0, tip = "maximum outfile duration in seconds (if end of data is not reached). Set to zero (Default) to ignore." },
  arg25 = { name = "mult", switch = "-b", min = 0, max = 10, def = 0, tip = "enlarges the output buffers mult times. This is to accommodate long silences which may appear in the output." },
  arg26 = { name = "-e", switch = "-e", tip = "use exponential splices (Default: linear)" },
  arg27 = { name = "-o", switch = "-o", tip = "make parameters other than velocity relative to time in the output soundfile" },
}

--]]

dsp["Flutter flutter - Add multi-channel-distributed tremolo to a multi-channel file "] = {
  cmds = { exe = "flutter", mode = "flutter", channels = "2in2out", tip = "FLUTTER is a special multi-channel version of ENVEL TREMOLO. The latter program causes the loudness of the entire soundfile to fluctuate at a particular frequency and depth. FLUTTER, on the other hand, causes these tremulations to pass from one (set of) output channel(s) to another as it proceeds, making the multi-channel file tremble in a specified way. Musical Applications; FLUTTER belongs to the class of programs like tremolo and vibrato, which can produce subtle or dramatic changes to the quality of a sound, or can be gradually introduced to the tail of a sound to add liveliness to the sonic event." },
  arg1 = { name = "inmcfile", input = "wav", tip = "input multi-channel sound file" },
  arg2 = { name = "outmcfile ", output = "wav", tip = "output multi-channel sound file" },
  arg3 = { name = "chanseq", input = "txt", tip = "On each cycle of the loudness fluctuation, a different set of output channels fluctuates in loudness (rising and falling) while the other channels do not change in level." },
  arg4 = { name = "freq", min = 0, max = 120, def = 10, tip = "frequency (in Hz) of the loudness variation" },
  arg5 = { name = "depth", min = 0, max = 16, tip = "depth of the loudness variation (Range: 0 to 16)" },
  arg6 = { name = "gain", min = 0, max = 1, def = 1, tip = "overall gain of the loudness variation (Range: 0 to 1)" },
  arg7 = { name = "-r", switch = "-r", tip = "randomisation of chanset order" },
}

dsp["Fracture - 1 The output is N-channel dispersal in N-channel space. Disperse a mono signal into fragments spread over N-channel space "] = {
  cmds = { exe = "fracture", mode = "fracture 1", channels = "1in2out", tip = "1 - The output is N-channel dispersal in N-channel space. FRACTURE cuts the source into enveloped segments of a given duration and scatters them in space. About the etab text file content; etab – a text5file in which each line is a TIME followed by 7 pairs of envelope-data in the form etime lev. etime is relative to time WITHIN the envelope (Range: 0 to 1), and lev is the level at etime (Range: 0 to 1). In each envelope data set lev must start and end at zero and rise to a maximum value of 1.0." },
  arg1 = { name = "infile", input = "wav", tip = "input soundfile (mono)" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "etab", input = "txt", tip = "a text-file in which each line is a TIME followed by 7 pairs of envelope-data. See CDP docs" },
  arg4 = { name = "chns", min = 2, max = 2, def = 2, tip = "the number of channels in the output soundfile.- only stereo supported in Renoise" },
  arg5 = { name = "strms", min = 0, max = 10000, def = 10, tip = "the number of spatial positions (streams) for resulting fragments (Range: greater than 4)" },
  arg6 = { name = "pulse", min = 0, max = 10, def = 1, input = "brk", tip = "the average gap between one set-of-fragments (each in a different stream) and the next set." },
  arg7 = { name = "edpth", min = 0, max = 100, def = 1, input = "brk", def = 1, tip = "envelope depth" },
  arg8 = { name = "stkint", min = 0, max = 12, def = 0, input = "brk", tip = "the interval of (upward) transposition in the stack, in semitones (Range: 0 to 12; the Default, which is 0, is read as octave (12)." },
  arg9 = { name = "seed", switch = "-h", min = 0, max = 100, def = 0, tip = "If NOT zero, repeating the process with the same seed gives identical output" },
  arg10 = { name = "min", switch = "-m", min = 0, max = 60, def = 1, input = "brk", tip = "minimum duration of fragments. If zero, there is no minimum or maximum." },
  arg11 = { name = "max", switch = "-i", min = 0, max = 60, def = 10, input = "brk", tip = "maximum duration of fragments. If zero, there is no minimum or maximum." },
  arg12 = { name = "rnd", switch = "-r", min = 0, max = 1, input = "brk", tip = "randomisation of the read-time in the source soundfile (Range: 0 to 1)" },
  arg13 = { name = "prnd", switch = "-p", min = 0, max = 1, input = "brk", def = 0, tip = "randomisation of pulse-time in the output (Range: 0 to 1)." },
  arg14 = { name = "disp", switch = "-d", min = 0, max = 10, input = "brk", def = 0, tip = "the dispersal (scatter) of output timings between different streams." },
  arg15 = { name = "lrnd", switch = "-v", min = 0, max = 1, input = "brk", def = 0.5, tip = "randomisation of the levels (i.e., volume) of the fragments (Range: 0 to 1)." },
  arg16 = { name = "ernd", switch = "-e", min = 0, max = 100, def = 0, input = "brk", tip = "randomisation of the envelope used. This is a time range in seconds. An event at now reads etable at a random time between now and now minus ernd." },
  arg17 = { name = "rnd", switch = "-s", min = 0, max = 1, input = "brk", def = 0.5, tip = "randomisation of stack (Range: 0 to 1)." },
  arg18 = { name = "trnd", switch = "-t", min = 0, max = 1200, input = "brk", def = 5, tip = "random transposition of the fragments (Range: 0 to 1200 cents: i.e., up to 1 octave upwards. " },
  arg19 = { name = "-y", switch = "-y", tip = "permit stacking of very short events. The Default is 'forbid', i.e., prevent clipping." },
  arg20 = { name = "-l", switch = "-l", tip = "for more than two output channels, the loudspeakers are assumed to encircle the listeners, with a single loudspeaker at centre-front. Setting the -l flag changes this to a linear array, with leftmost and rightmost speakers." },
}

dsp["Frame Shift - 1 Rotate the entire frame of a multi-channel file. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 1", channels = "2in2out", tip = "The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "snake", input = "txt", tip = "the route the movement through the speakers follows" },
  arg4 = { name = "rotation", min = -500, max = 500, def = 100, tip = "the rotation-rate in cycles per second. These cycles are complete frame-rotations." },
  arg5 = { name = "smear", switch = "-s", min = 0, max = 0.5, def = 0, tip = "the extent to which channel-signals bleed into adjacent channels. (Range: 0 to 0.5. Default: 0)" },
}

dsp["Frame Shift - 2 Rotate the entire frame of a multi-channel file. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 2", channels = "2in2out", tip = "In Mode 2, odd and even channels rotate independently: 1 -> 3 -> 5 -> 7 -> 1 etc. and 2 -> 4 -> 6 -> 8 -> 2 etc. Snake data directs motion around a different route. To illustrate this, the 8 6 5 2 7 3 4 1 route above becomes one snake route around the odd entries: 8 -> 5 -> 7 -> 4 -> 8 etc. and a second snake route around the even entries: 6 -> 2 -> 3 -> 1 -> 6 etc.The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "snake", input = "txt", tip = "the route the movement through the speakers follows" },
  arg4 = { name = "rotation", min = -500, max = 500, def = 100, tip = "the rotation-rate in cycles per second. These cycles are complete frame-rotations." },
  arg5 = { name = "rotation2", min = -500, max = 500, def = 100, tip = "the rotation-rate in cycles per second. These cycles are complete frame-rotations." },
  arg6 = { name = "smear", switch = "-s", min = 0, max = 0.5, def = 0, tip = "the extent to which channel-signals bleed into adjacent channels. (Range: 0 to 0.5. Default: 0)" },
}

dsp["Frame Shift - 3 Change the channel assignment of a multi-channel file. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 3", channels = "2in2out", tip = "3 - Change the channel assignment of a multi-channel file. The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "reorient", input = "txt", tip = "list of all input channels in the new order required. In doing so, the order of the entries refers to channels 1, 2, 3 ... etc. in order." },
}

dsp["Frame Shift - 4 Mirror the channel output around a specified mirrorplane. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 4", channels = "2in2out", tip = "4 Mirror the channel output around a specified mirrorplane. The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "mirrorplane", min = 2, max = 16, def = 2, tip = "the line around which the frame is (symmetrically) mirrored. Values can be any (integer) outchannel number OR any half-way position between outchannels, such as 1.5, 2.5 etc. With N channels, 'N.5' lies between the Nth and the 1st channel." },
}

dsp["Frame Shift - 5 Convert between ring-numbered and bilaterally numbered outchans. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 5", channels = "2in2out", tip = "5 - Convert between ring-numbered and bilaterally numbered outchans. The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "-b", switch = "-b", tip = "convert from bilateral to ring. The Default setting is ring to bilateral." },
}

dsp["Frame Shift - 6 Swap any pair of channels (swapA and SwapB). Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 6", channels = "2in2out", tip = "6 - Swap any pair of channels (swapA and SwapB). The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "swapA", min = 1, max = 16, def = 1, tip = "the channel numbers of the 2 channels to be swapped." },
  arg4 = { name = "swapB", min = 1, max = 16, def = 2, tip = "the channel numbers of the 2 channels to be swapped." },
}

dsp["Frame Shift - 7 Allows any channel, or set of channels, to be enveloped independently of the other channels. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 7", channels = "2in2out", tip = "7 - Allows any channel, or set of channels, to be enveloped independently of the other channels. The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "chaninfo", min = 1, max = 16, def = 1, input = "txt", tip = "a single channel number, or a list of channels in a text file (separated by spaces, tabs or newlines)." },
  arg4 = { name = "gain", min = 0, max = 1, def = 1, input = "brk", tip = "gain in amplitude level to apply to enveloped channels: increase (> 1) or decrease (< 1). Gain may vary over time: define in a time gain-value breakpoint file." },
}

dsp["Frame Shift - 8 Converts between ring-numbered and BEAST bilateral numbering. Create frame patterns for multi-channel speaker setups"] = {
  cmds = { exe = "frame", mode = "shift 8", channels = "2in2out", tip = "8 - Converts between ring-numbered and BEAST bilateral numbering ('BEAST' = 'Birmingham Electro-Acoustic Sound Theatre', a multi-speaker diffusion system developed by Jonty Harrison et al. at Birmingham University, famous throughout Europe for outstanding performances of electro-acoustic music. Ed.). The multi-channel frame is defined by the number of output channels and how these are projected into the space. Usually here we are assuming that each channel goes to a different loudspeaker and that these loudspeakers are positioned in a ring or arc around the listener. However, the channels may be regarded as abstract or conceptual 'positions' in any layout that the user requires. The output from these processes are simply multi-channel wav or aiff files, and the channels from this file may be sent to loudspeakers in any desired positional array, or in fact converted to locations in an Ambisonic format defined by the user (see the multi-channel Toolkit). FRAME SHIFT is concerned with operations which affect the entire multi-channel frame, such as by reorienting it (e.g., by turning it though 90 degrees, before we begin to play the sound), or by rotating it (e.g., by turning it though 90 degrees, as we play the sound). FRAME SHIFT Mode 1 allows us to rotate the entire multi-channel soundfile. Compare this with MCHANPAN Mode 1 which allows us to rotate a mono file around a multi-channel space (see the example under MCHANPAN). In the simplest case we simply enter a speed of rotation in cycles per second, either positive for clockwise rotation, or negative for anticlockwise rotation. The number we enter is the number of times the entire frame rotates in 1 second. To rotate more slowly, such as 1 rotation in 4 seconds, we would enter a value less than 1 (in this example, 0.25 = ¼). However, we can also make the channels of the multi-channel sound snake around the space in any pattern we choose. In a simple rotation, clockwise, channel 1 moves to 2, while 2 moves to 3, 3 moves to 4 and so on, each channel following the next in a 'snake' defined (for 8 channels) by the sequence 1 ⇒ 2 ⇒ 3 ⇒ 4 ⇒ 5 ⇒ 6 ⇒ 7 ⇒ 8 ⇒ 1 etc. We can write this as: 1 2 3 4 5 6 7 8. This is the snake-list part of the time snake-list breakpoint textfile submitted to the snake parameter. However, the channels can snake around the space in any way you might like. For example, you could do this: 1 goes to 3, 3 goes to 5, 5 goes to 7, 7 goes to 8, 8 goes to 2, 2 goes to 4, and 4 goes to 6. We can write this as: 1 3 5 7 8 2 4 6. This sequence – the sequence in which the channels follow each other around the space – is the snake which you can enter as data for this process. In addition, Mode 6 allows a simple swapping of a pair of channels in the multi-channel file. Mode 7 similarly allows the level in a particular channel to be modified, without altering the level in the others. Musical applications; Apart from the obvious musical applications, reorientating the frame or swapping the channels in a multi-channel file may be helpful when mixing together multi-channel files in a situation where the exact spatial location of the outputs is not significant. If in a mix, the output overloads because the sounds in one (or more) of the channels are too loud when combined, reorientating the frame of one of the input files may solve this problem. If this proves impossible, or is musically not desired, altering the level of a particular event in a single channel (mode 7) may be the solution." },
  arg1 = { name = "infile", input = "wav", tip = "input multi-channel soundfile" },
  arg2 = { name = "outfile", output = "wav", tip = "output multi-channel soundfile" },
  arg3 = { name = "-b", switch = "-b", tip = "convert from ring-numbering to BEAST bilateral numbering (Default: ring to BEAST)." },
}

